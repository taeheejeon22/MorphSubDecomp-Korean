{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import unicodedata\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "sys.path.append(\"..\")\n",
    "from scripts._mecab import Mecab\n",
    "from typing import Dict, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import (\n",
    "    # CharTokenizer,\n",
    "    # JamoTokenizer,\n",
    "    MeCabSentencePieceTokenizer_fixed,\n",
    "    MeCabSentencePieceTokenizer,\n",
    "    MeCabWordPieceTokenizer,\n",
    "    # MeCabTokenizer,\n",
    "    MeCabTokenizer_fixed,\n",
    "    MeCabTokenizer_all,\n",
    "    # MeCabSentencePieceTokenizer_kortok,\n",
    "    # MeCabTokenizer_kortok,\n",
    "    SentencePieceTokenizer,\n",
    "    WordPieceTokenizer,\n",
    "    Vocab,\n",
    "    # WordTokenizer,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 토큰화 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_name: str, resource_dir: str, token_type, tokenizer_type: str , decomposition_type: str, space_symbol: str, dummy_letter: str, nfd: bool, grammatical_symbol: list = [\"\", \"\"], skip_special_tokens: bool = False, lexical_grammatical: bool = False):   # for LG\n",
    "    tokenizer_dir = os.path.join(resource_dir, tokenizer_name)\n",
    "\n",
    "    if tokenizer_name.startswith(\"sp-\"):\n",
    "        tokenizer = SentencePieceTokenizer(os.path.join(tokenizer_dir, \"tok.model\"))\n",
    "\n",
    "    elif tokenizer_name.startswith(\"mecab_\"):\n",
    "\n",
    "        sp = SentencePieceTokenizer(os.path.join(tokenizer_dir, \"tok.model\"))\n",
    "\n",
    "        if \"orig\" in tokenizer_name:\n",
    "            mecab = MeCabTokenizer_orig(tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter)\n",
    "            tokenizer = MeCabSentencePieceTokenizer_orig(mecab, sp, use_fixed=False) # mecab_sp_orig.py\n",
    "\n",
    "        elif \"fixed\" in tokenizer_name:\n",
    "            mecab = MeCabTokenizer_fixed(tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter)\n",
    "            tokenizer = MeCabSentencePieceTokenizer_fixed(mecab, sp, use_fixed=True) # mecab_fixed.py\n",
    "\n",
    "\n",
    "    # elif tokenizer_name.startswith(\"eojeol\") or tokenizer_name.startswith(\"morpheme\"):\n",
    "    elif tokenizer_name.startswith(\"eojeol\") or tokenizer_name.startswith(\"morpheme\") or tokenizer_name.startswith(\"LG\"):   # LG도 처리할 수 있도록\n",
    "        wp = WordPieceTokenizer(os.path.join(tokenizer_dir, \"bert_tokenizer.json\"), skip_special_tokens=False)\n",
    "        # mecab = MeCabTokenizer_all(token_type=token_type, tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter, nfd=nfd, grammatical_symbol=grammatical_symbol)\n",
    "        mecab = MeCabTokenizer_all(token_type=token_type, tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter, nfd=nfd, grammatical_symbol=grammatical_symbol, lexical_grammatical=lexical_grammatical)   # for LG\n",
    "        tokenizer = MeCabWordPieceTokenizer(mecab=mecab, wp=wp) # mecab_wp.py\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Wrong tokenizer name.\")\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_result(tokenizer, string, nfd: bool = True):\n",
    "    # if nfd == True:\n",
    "    #     string = str_to_nfd(string)\n",
    "\n",
    "    tokenized = tokenizer.tokenize(string)\n",
    "    # print(\" \".join(tokenized))\n",
    "    \n",
    "    return \" \".join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 64k\n",
    "tokenizer_eojeol_composed_F_64k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_composed_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"../resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_eojeol_decomposed_pure_F_64k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"../resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_composed_F_64k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_composed_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"../resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_lexical_F_64k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_lexical_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"../resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_lexical\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_pure_F_64k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"../resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "\n",
    "# 32k\n",
    "tokenizer_eojeol_composed_F_32k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_composed_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"../resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_eojeol_decomposed_pure_F_32k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"../resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_composed_F_32k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_composed_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"../resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_lexical_F_32k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_lexical_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"../resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_lexical\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_pure_F_32k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"../resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tokenizations(string):\n",
    "    # grammatical symbol F\n",
    "    eojeol_composed_F_64k = string + '\\t' + 'eojeol_composed_F_64k' + '\\t' + get_tokenized_result(tokenizer_eojeol_composed_F_64k, string)\n",
    "    eojeol_pure_F_64k = string + '\\t' + 'eojeol_pure_F_64k' + '\\t' + get_tokenized_result(tokenizer_eojeol_decomposed_pure_F_64k, string)\n",
    "    fixed_composed_F_64k = string + '\\t' + 'fixed_composed_F_64k' + '\\t' + get_tokenized_result(tokenizer_fixed_composed_F_64k, string)\n",
    "    fixed_lexical_F_64k = string + '\\t' + 'fixed_lexical_F_64k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_lexical_F_64k, string)\n",
    "    fixed_pure_F_64k = string + '\\t' + 'fixed_pure_F_64k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_pure_F_64k, string)\n",
    "\n",
    "    eojeol_composed_F_32k = string + '\\t' + 'eojeol_composed_F_32k' + '\\t' + get_tokenized_result(tokenizer_eojeol_composed_F_32k, string)\n",
    "    eojeol_pure_F_32k = string + '\\t' + 'eojeol_pure_F_32k' + '\\t' + get_tokenized_result(tokenizer_eojeol_decomposed_pure_F_32k, string)\n",
    "    fixed_composed_F_32k = string + '\\t' + 'fixed_composed_F_32k' + '\\t' + get_tokenized_result(tokenizer_fixed_composed_F_32k, string)\n",
    "    fixed_lexical_F_32k = string + '\\t' + 'fixed_lexical_F_32k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_lexical_F_32k, string)\n",
    "    fixed_pure_F_32k = string + '\\t' + 'fixed_pure_F_32k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_pure_F_32k, string)\n",
    "    \n",
    "    return \"\\n\".join(['\\n'+eojeol_composed_F_64k , eojeol_pure_F_64k , fixed_composed_F_64k, fixed_lexical_F_64k, fixed_pure_F_64k,\n",
    "    eojeol_composed_F_32k , eojeol_pure_F_32k , fixed_composed_F_32k, fixed_lexical_F_32k, fixed_pure_F_32k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_composed_F_64k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_pure_F_64k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_composed_F_64k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] [UNK] [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_lexical_F_64k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_pure_F_64k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_composed_F_32k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_pure_F_32k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_composed_F_32k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] [UNK] [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_lexical_F_32k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_pure_F_32k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_tokenizations('갑자기 더워彭肽꿿뜛땭뜎حمق')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 저장\n",
    "def analysis(task, corpus_list, corpus_name_list, sent2=True):\n",
    "    for corpus, corpus_name in zip(corpus_list, corpus_name_list):\n",
    "        with open(task+'_'+corpus_name+'.tsv', 'w', encoding='utf-8') as f:\n",
    "            f.write('source'+'\\t'+'tokenizer'+'\\t'+'tokenize_result')\n",
    "            if sent2:\n",
    "                for sent1, sent2 in zip(corpus[0], corpus[1]):\n",
    "                    f.write(show_tokenizations(string=sent1))\n",
    "                    f.write(show_tokenizations(string=sent2))\n",
    "            else:\n",
    "                for sent1 in (corpus):\n",
    "                    f.write(show_tokenizations(string=sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cola\n",
    "def load_data(file_path: str) -> Tuple[List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentences: List[str] = []\n",
    "\n",
    "    # for test set\n",
    "    if file_path == \"../dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_test_with_answer.tsv\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f.readlines()[1:]):\n",
    "                splitted = line.strip().split(\"\\t\")\n",
    "                sentences.append(splitted[2])\n",
    "\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f.readlines()[1:]):\n",
    "                splitted = line.strip().split(\"\\t\")\n",
    "                sentences.append(splitted[3])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "task = 'cola'\n",
    "train = load_data('../dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_train.tsv')\n",
    "dev = load_data('../dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_dev.tsv')\n",
    "test = load_data('../dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_test_with_answer.tsv')\n",
    "\n",
    "corpus_list = [train, dev, test]\n",
    "corpus_name_list = ['train', 'dev', 'test']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsmc\n",
    "\n",
    "def load_data(file_path: str):\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentences: List[str] = []\n",
    "    labels: List[int] = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if len(splitted) != 2:\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            sentences.append(splitted[0])\n",
    "            \n",
    "#             labels.append(label_to_index[splitted[1]])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "task = 'nsmc'\n",
    "train = load_data('../dataset/nlu_tasks/nsmc/ratings_train.tsv')\n",
    "dev = load_data('../dataset/nlu_tasks/nsmc/ratings_dev.tsv')\n",
    "test = load_data('../dataset/nlu_tasks/nsmc/ratings_test.tsv')\n",
    "\n",
    "corpus_list = [train, dev, test]\n",
    "corpus_name_list = ['train', 'dev', 'test']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hsd\n",
    "\n",
    "def load_data(file_path: str):\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence_a\n",
    "    2. sentence_b\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentence = []\n",
    "    # sentence_bs = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if len(splitted) != 4:\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "\n",
    "            sentence.append(splitted[0])\n",
    "                # sentence_bs.append(splitted[1])\n",
    "            # sentence_as.append(splitted[0])\n",
    "            # sentence_bs.append(splitted[1])\n",
    "\n",
    "#             labels.append(label_to_index[splitted[3]])\n",
    "\n",
    "    return sentence\n",
    "\n",
    "task = 'hsd'\n",
    "train = load_data('../dataset/nlu_tasks/hsd/train.tsv')\n",
    "dev = load_data('../dataset/nlu_tasks/hsd/dev.tsv')\n",
    "\n",
    "corpus_list = [train, dev]\n",
    "corpus_name_list = ['train', 'dev']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paws\n",
    "def load_data(file_path: str):\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence_a\n",
    "    2. sentence_b\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentence_as = []\n",
    "    sentence_bs = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if len(splitted) != 4:\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            if splitted[1] == \"\" or splitted[2] == \"\":\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            # 문장이 \"NS\"로만 표기된 라인 제외\n",
    "            if splitted[1] == \"NS\" or splitted[2] == \"NS\":\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            sentence_as.append(splitted[1])\n",
    "            sentence_bs.append(splitted[2])\n",
    "        \n",
    "    return sentence_as, sentence_bs\n",
    "\n",
    "task = 'paws'\n",
    "train = load_data('../dataset/nlu_tasks/paws/translated_train.tsv')\n",
    "dev = load_data('../dataset/nlu_tasks/paws/dev_2k.tsv')\n",
    "test = load_data('../dataset/nlu_tasks/paws/test_2k.tsv')\n",
    "\n",
    "corpus_list = [train, dev, test]\n",
    "corpus_name_list = ['train', 'dev', 'test']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7130/2093742003.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dp_train = pd.read_csv('../KLUE-baseline/data/klue_benchmark/klue-dp-v1.1/klue-dp-v1.1_train.tsv',sep='delimiter', header=None)\n",
      "/tmp/ipykernel_7130/2093742003.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dp_dev = pd.read_csv('../KLUE-baseline/data/klue_benchmark/klue-dp-v1.1/klue-dp-v1.1_dev.tsv', sep='delimiter', header=None)\n",
      "/tmp/ipykernel_7130/2093742003.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_train['text'] = df2_train['text'].apply(lambda x: re.sub('## klue-dp-v1_train_.*\\t', '', x))\n",
      "/tmp/ipykernel_7130/2093742003.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_dev['text'] = df2_dev['text'].apply(lambda x: re.sub('## klue-dp-v1_dev_.*\\t', '', x))\n"
     ]
    }
   ],
   "source": [
    "# KLUE-dp\n",
    "dp_train = pd.read_csv('../KLUE-baseline/data/klue_benchmark/klue-dp-v1.1/klue-dp-v1.1_train.tsv',sep='delimiter', header=None)\n",
    "dp_dev = pd.read_csv('../KLUE-baseline/data/klue_benchmark/klue-dp-v1.1/klue-dp-v1.1_dev.tsv', sep='delimiter', header=None)\n",
    "dp_train.columns = ['text']\n",
    "dp_dev.columns = ['text']\n",
    "df2_train = dp_train['text'].str.contains('## klue-dp')\n",
    "df2_dev = dp_dev['text'].str.contains('## klue-dp')\n",
    "df2_train = dp_train[df2_train]\n",
    "df2_dev = dp_dev[df2_dev]\n",
    "df2_train['text'] = df2_train['text'].apply(lambda x: re.sub('## klue-dp-v1_train_.*\\t', '', x))\n",
    "df2_dev['text'] = df2_dev['text'].apply(lambda x: re.sub('## klue-dp-v1_dev_.*\\t', '', x))\n",
    "df2_train.to_csv('dp_orig_train.tsv', index=False, sep='\\t')\n",
    "df2_dev.to_csv('dp_orig_dev.tsv', index=False, sep='\\t')\n",
    "\n",
    "def load_data(file_path: str) -> Tuple[List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentenced\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentences: List[str] = []\n",
    "    #labels: List[int] = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip()\n",
    "            # if len(splitted) != 2:\n",
    "            #     #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "            #     continue\n",
    "            sentences.append(splitted)\n",
    "     #       labels.append(label_to_index[splitted[1]])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "task = 'dp'\n",
    "train = load_data('dp_orig_train.tsv')\n",
    "dev = load_data('dp_orig_dev.tsv')\n",
    "\n",
    "corpus_list = [train, dev]\n",
    "corpus_name_list = ['train', 'dev']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLUE-nli\n",
    "# json to tsv\n",
    "import json\n",
    "import pandas as pd\n",
    "with open('../KLUE-baseline/data/klue_benchmark/klue-nli-v1.1/klue-nli-v1.1_train.json', 'r') as f:\n",
    "    nli_train = json.load(f)\n",
    "with open('../KLUE-baseline/data/klue_benchmark/klue-nli-v1.1/klue-nli-v1.1_dev.json') as f2:\n",
    "    nli_dev = json.load(f2)\n",
    "\n",
    "\n",
    "nli_train = pd.DataFrame(nli_train)\n",
    "nli_train = nli_train[['premise', 'hypothesis']]\n",
    "nli_train.to_csv('nli_orig_train.tsv', index=False, sep='\\t')\n",
    "nli_dev = pd.DataFrame(nli_dev)\n",
    "nli_dev = nli_dev[['premise', 'hypothesis']]\n",
    "nli_dev.to_csv('nli_orig_dev.tsv', index=False, sep='\\t')\n",
    "\n",
    "def load_data(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence_a\n",
    "    2. sentence_b\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentence_as = []\n",
    "    sentence_bs = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if splitted[0] == \"\" or splitted[1] == \"\":\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            sentence_as.append(splitted[0])\n",
    "            sentence_bs.append(splitted[1])\n",
    "        \n",
    "    return sentence_as, sentence_bs\n",
    "\n",
    "task = 'nli'\n",
    "train = load_data('nli_orig_train.tsv')\n",
    "dev = load_data('nli_orig_dev.tsv')\n",
    "\n",
    "corpus_list = [train, dev]\n",
    "corpus_name_list = ['train', 'dev']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OOV rate, ## rate 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [UNK]의 개수 / 문장의 길이 * 100\n",
    "from typing import List\n",
    "\n",
    "# 문장당 oov rate (OR)\n",
    "def getOOVRatePerSentence(sentence):\n",
    "    sentence = removeCS(sentence)\n",
    "    OOV_rate = sentence.count('[UNK]') / len(sentence.split()) * 100   \n",
    "    \n",
    "    return OOV_rate\n",
    "\n",
    "# count of all OOV tokens (OC)\n",
    "def getCountofAllOOV(sentence):\n",
    "    cnt = 0\n",
    "    cnt += len(re.findall('[UNK]', sentence))\n",
    "    \n",
    "    return cnt\n",
    "\n",
    "# [CLS, SEP] 제거\n",
    "def removeCS(sentence):\n",
    "    sentence = sentence.replace('[CLS]', '')\n",
    "    sentence = sentence.replace('[SEP]', '')\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# [UNK]수 /전체 토큰 수\n",
    "# def getOOVdividedbyAllTokens(corpus):\n",
    "#     corpus['OC'] = corpus['sentence'].apply(lambda x: getCountofAllOOV(x))\n",
    "#     corpus['sentence'].apply(lambda x: removeCS(x))\n",
    "#     corpus['token_count'] = corpus['sentence'].apply(lambda x: len(x.split()))\n",
    "#     OOV_count = corpus['OC'].sum()\n",
    "#     token_count = corpus['token_count'].sum()\n",
    "    \n",
    "#     return OOV_count, token_count, OOV_count/token_count*100\n",
    "\n",
    "\n",
    "# \"##\"\" 세기\n",
    "# 문장당 ## rate (SR)\n",
    "def getShopRatePerSentence(sentence):\n",
    "    sentence = removeCS(sentence)\n",
    "    shop_rate = sentence.count('##') / len(sentence.split()) * 100   \n",
    "    \n",
    "    return shop_rate\n",
    "\n",
    "# count of all ## tokens (SC)\n",
    "def getCountofAllShop(sentence):\n",
    "    cnt = 0\n",
    "    cnt += len(re.findall('##', sentence))\n",
    "    \n",
    "    return cnt\n",
    "\n",
    "# [CLS, SEP] 제거\n",
    "def removeCS(sentence):\n",
    "    sentence = sentence.replace('[CLS]', '')\n",
    "    sentence = sentence.replace('[SEP]', '')\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# ##수 /전체 토큰 수\n",
    "# def getOOVdividedbyAllTokens(corpus):\n",
    "#     corpus['SC'] = corpus['sentence'].apply(lambda x: getCountofAllShop(x))\n",
    "#     corpus['sentence'].apply(lambda x: removeCS(x))\n",
    "#     corpus['token_count'] = corpus['sentence'].apply(lambda x: len(x.split()))\n",
    "#     Shop_count = corpus['SC'].sum()\n",
    "#     token_count = corpus['token_count'].sum()\n",
    "    \n",
    "#     return Shop_count, token_count, Shop_count/token_count*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "cola_train = pd.read_csv('cola_train.tsv', sep='\\t')\n",
    "cola_dev = pd.read_csv('cola_dev.tsv', sep='\\t')\n",
    "cola_test = pd.read_csv('cola_test.tsv', sep='\\t')\n",
    "cola = pd.concat([cola_train, cola_dev, cola_test])\n",
    "cola['tokenize_result'] = cola['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "cola['source_len'] = cola['source'].apply(lambda x: len(x.split()))\n",
    "cola['tokenized_len'] = cola['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "\n",
    "cola['OOV_per_tokenized_sent'] = cola['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "cola['OOV_count'] = cola['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "cola['OOV_per_source_sent'] = cola['OOV_count'] / cola['source_len'] * 100\n",
    "cola['##_per_tokenized_sent'] = cola['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "cola['##_count'] = cola['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "cola['##_per_source_sent'] = cola['##_count'] / cola['source_len'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source                    0\n",
       "tokenizer                 0\n",
       "tokenize_result           0\n",
       "source_len                0\n",
       "tokenized_len             0\n",
       "OOV_per_tokenized_sent    0\n",
       "OOV_count                 0\n",
       "OOV_per_source_sent       0\n",
       "##_per_tokenized_sent     0\n",
       "##_count                  0\n",
       "##_per_source_sent        0\n",
       "task                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, tokenize_result, source_len, tokenized_len, OOV_per_tokenized_sent, OOV_count, OOV_per_source_sent, ##_per_tokenized_sent, ##_count, ##_per_source_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola[cola['tokenized_len'] > 128].groupby('tokenizer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th colspan=\"2\" halign=\"left\">tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>8.460269</td>\n",
       "      <td>152251</td>\n",
       "      <td>0.032876</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>39.408974</td>\n",
       "      <td>62072</td>\n",
       "      <td>72.816467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>7.428540</td>\n",
       "      <td>133684</td>\n",
       "      <td>0.036712</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>31.165140</td>\n",
       "      <td>43505</td>\n",
       "      <td>51.650842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>7.844354</td>\n",
       "      <td>141167</td>\n",
       "      <td>0.035132</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>34.697582</td>\n",
       "      <td>50988</td>\n",
       "      <td>60.109946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>7.228717</td>\n",
       "      <td>130088</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>29.298988</td>\n",
       "      <td>39909</td>\n",
       "      <td>47.571706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>12.380196</td>\n",
       "      <td>222794</td>\n",
       "      <td>0.019880</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>4.353754</td>\n",
       "      <td>10372</td>\n",
       "      <td>12.154478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>11.972772</td>\n",
       "      <td>215462</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>1.323949</td>\n",
       "      <td>3040</td>\n",
       "      <td>3.629573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>12.213770</td>\n",
       "      <td>219799</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>3.129198</td>\n",
       "      <td>7377</td>\n",
       "      <td>8.710197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>11.917037</td>\n",
       "      <td>214459</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>0.884107</td>\n",
       "      <td>2037</td>\n",
       "      <td>2.420483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>12.207157</td>\n",
       "      <td>219680</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>3.083478</td>\n",
       "      <td>7258</td>\n",
       "      <td>8.578487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>11.914259</td>\n",
       "      <td>214409</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>0.861383</td>\n",
       "      <td>1987</td>\n",
       "      <td>2.356606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len         OOV_per_tokenized_sent  \\\n",
       "                            mean          mean     sum                   mean   \n",
       "tokenizer                                                                       \n",
       "eojeol_composed_F_32k   5.011058      8.460269  152251               0.032876   \n",
       "eojeol_composed_F_64k   5.011058      7.428540  133684               0.036712   \n",
       "eojeol_pure_F_32k       5.011058      7.844354  141167               0.035132   \n",
       "eojeol_pure_F_64k       5.011058      7.228717  130088               0.037975   \n",
       "fixed_composed_F_32k    5.011058     12.380196  222794               0.019880   \n",
       "fixed_composed_F_64k    5.011058     11.972772  215462               0.020558   \n",
       "fixed_lexical_F_32k     5.011058     12.213770  219799               0.020423   \n",
       "fixed_lexical_F_64k     5.011058     11.917037  214459               0.020611   \n",
       "fixed_pure_F_32k        5.011058     12.207157  219680               0.020423   \n",
       "fixed_pure_F_64k        5.011058     11.914259  214409               0.020611   \n",
       "\n",
       "                      OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                            sum                mean                  mean   \n",
       "tokenizer                                                                   \n",
       "eojeol_composed_F_32k       189            0.162933             39.408974   \n",
       "eojeol_composed_F_64k       189            0.162933             31.165140   \n",
       "eojeol_pure_F_32k           189            0.162933             34.697582   \n",
       "eojeol_pure_F_64k           189            0.162933             29.298988   \n",
       "fixed_composed_F_32k        216            0.181257              4.353754   \n",
       "fixed_composed_F_64k        216            0.181257              1.323949   \n",
       "fixed_lexical_F_32k         216            0.181257              3.129198   \n",
       "fixed_lexical_F_64k         216            0.181257              0.884107   \n",
       "fixed_pure_F_32k            216            0.181257              3.083478   \n",
       "fixed_pure_F_64k            216            0.181257              0.861383   \n",
       "\n",
       "                      ##_count ##_per_source_sent  \n",
       "                           sum               mean  \n",
       "tokenizer                                          \n",
       "eojeol_composed_F_32k    62072          72.816467  \n",
       "eojeol_composed_F_64k    43505          51.650842  \n",
       "eojeol_pure_F_32k        50988          60.109946  \n",
       "eojeol_pure_F_64k        39909          47.571706  \n",
       "fixed_composed_F_32k     10372          12.154478  \n",
       "fixed_composed_F_64k      3040           3.629573  \n",
       "fixed_lexical_F_32k       7377           8.710197  \n",
       "fixed_lexical_F_64k       2037           2.420483  \n",
       "fixed_pure_F_32k          7258           8.578487  \n",
       "fixed_pure_F_64k          1987           2.356606  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_group = cola.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean', 'sum'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean'],  \n",
    "})\n",
    "cola_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsmc_train = pd.read_csv('nsmc_train.tsv', sep='\\t')\n",
    "nsmc_dev = pd.read_csv('nsmc_dev.tsv', sep='\\t')\n",
    "nsmc_test = pd.read_csv('nsmc_test.tsv', sep='\\t')\n",
    "nsmc = pd.concat([nsmc_train, nsmc_dev, nsmc_test])\n",
    "nsmc['tokenize_result'] = nsmc['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "nsmc['source_len'] = nsmc['source'].apply(lambda x: len(x.split()))\n",
    "nsmc['tokenized_len'] = nsmc['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "\n",
    "nsmc['OOV_per_tokenized_sent'] = nsmc['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "nsmc['OOV_count'] = nsmc['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "nsmc['OOV_per_source_sent'] = nsmc['OOV_count'] / nsmc['source_len'] * 100\n",
    "nsmc['##_per_tokenized_sent'] = nsmc['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "nsmc['##_count'] = nsmc['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "nsmc['##_per_source_sent'] = nsmc['##_count'] / nsmc['source_len'] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source                    0\n",
       "tokenizer                 0\n",
       "tokenize_result           0\n",
       "source_len                0\n",
       "tokenized_len             0\n",
       "OOV_per_tokenized_sent    0\n",
       "OOV_count                 0\n",
       "OOV_per_source_sent       0\n",
       "##_per_tokenized_sent     0\n",
       "##_count                  0\n",
       "##_per_source_sent        0\n",
       "task                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th colspan=\"2\" halign=\"left\">tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>17.727344</td>\n",
       "      <td>3545327</td>\n",
       "      <td>0.346157</td>\n",
       "      <td>8989</td>\n",
       "      <td>1.392632</td>\n",
       "      <td>54.903910</td>\n",
       "      <td>2027121</td>\n",
       "      <td>186.080295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>15.669567</td>\n",
       "      <td>3133788</td>\n",
       "      <td>0.356818</td>\n",
       "      <td>8989</td>\n",
       "      <td>1.392632</td>\n",
       "      <td>48.895810</td>\n",
       "      <td>1615582</td>\n",
       "      <td>154.626038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>16.681637</td>\n",
       "      <td>3336194</td>\n",
       "      <td>0.455792</td>\n",
       "      <td>9813</td>\n",
       "      <td>1.738694</td>\n",
       "      <td>52.051229</td>\n",
       "      <td>1817988</td>\n",
       "      <td>167.590889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>15.262716</td>\n",
       "      <td>3052421</td>\n",
       "      <td>0.464452</td>\n",
       "      <td>9813</td>\n",
       "      <td>1.738694</td>\n",
       "      <td>47.621194</td>\n",
       "      <td>1534215</td>\n",
       "      <td>144.851687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.892581</td>\n",
       "      <td>4378341</td>\n",
       "      <td>0.127833</td>\n",
       "      <td>10730</td>\n",
       "      <td>1.883802</td>\n",
       "      <td>7.515631</td>\n",
       "      <td>317082</td>\n",
       "      <td>32.287948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.090634</td>\n",
       "      <td>4217958</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>10730</td>\n",
       "      <td>1.883802</td>\n",
       "      <td>3.894379</td>\n",
       "      <td>156699</td>\n",
       "      <td>17.575249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.536411</td>\n",
       "      <td>4307110</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>10448</td>\n",
       "      <td>1.797754</td>\n",
       "      <td>5.956904</td>\n",
       "      <td>245851</td>\n",
       "      <td>26.269544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.041962</td>\n",
       "      <td>4208224</td>\n",
       "      <td>0.126059</td>\n",
       "      <td>10448</td>\n",
       "      <td>1.797754</td>\n",
       "      <td>3.669871</td>\n",
       "      <td>146965</td>\n",
       "      <td>17.113886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.511515</td>\n",
       "      <td>4302131</td>\n",
       "      <td>0.122186</td>\n",
       "      <td>10430</td>\n",
       "      <td>1.795306</td>\n",
       "      <td>5.855999</td>\n",
       "      <td>240872</td>\n",
       "      <td>25.793359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.033336</td>\n",
       "      <td>4206499</td>\n",
       "      <td>0.125915</td>\n",
       "      <td>10430</td>\n",
       "      <td>1.795306</td>\n",
       "      <td>3.624766</td>\n",
       "      <td>145240</td>\n",
       "      <td>16.919890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len           \\\n",
       "                            mean          mean      sum   \n",
       "tokenizer                                                 \n",
       "eojeol_composed_F_32k   7.591344     17.727344  3545327   \n",
       "eojeol_composed_F_64k   7.591344     15.669567  3133788   \n",
       "eojeol_pure_F_32k       7.591344     16.681637  3336194   \n",
       "eojeol_pure_F_64k       7.591344     15.262716  3052421   \n",
       "fixed_composed_F_32k    7.591344     21.892581  4378341   \n",
       "fixed_composed_F_64k    7.591344     21.090634  4217958   \n",
       "fixed_lexical_F_32k     7.591344     21.536411  4307110   \n",
       "fixed_lexical_F_64k     7.591344     21.041962  4208224   \n",
       "fixed_pure_F_32k        7.591344     21.511515  4302131   \n",
       "fixed_pure_F_64k        7.591344     21.033336  4206499   \n",
       "\n",
       "                      OOV_per_tokenized_sent OOV_count OOV_per_source_sent  \\\n",
       "                                        mean       sum                mean   \n",
       "tokenizer                                                                    \n",
       "eojeol_composed_F_32k               0.346157      8989            1.392632   \n",
       "eojeol_composed_F_64k               0.356818      8989            1.392632   \n",
       "eojeol_pure_F_32k                   0.455792      9813            1.738694   \n",
       "eojeol_pure_F_64k                   0.464452      9813            1.738694   \n",
       "fixed_composed_F_32k                0.127833     10730            1.883802   \n",
       "fixed_composed_F_64k                0.133200     10730            1.883802   \n",
       "fixed_lexical_F_32k                 0.122378     10448            1.797754   \n",
       "fixed_lexical_F_64k                 0.126059     10448            1.797754   \n",
       "fixed_pure_F_32k                    0.122186     10430            1.795306   \n",
       "fixed_pure_F_64k                    0.125915     10430            1.795306   \n",
       "\n",
       "                      ##_per_tokenized_sent ##_count ##_per_source_sent  \n",
       "                                       mean      sum               mean  \n",
       "tokenizer                                                                \n",
       "eojeol_composed_F_32k             54.903910  2027121         186.080295  \n",
       "eojeol_composed_F_64k             48.895810  1615582         154.626038  \n",
       "eojeol_pure_F_32k                 52.051229  1817988         167.590889  \n",
       "eojeol_pure_F_64k                 47.621194  1534215         144.851687  \n",
       "fixed_composed_F_32k               7.515631   317082          32.287948  \n",
       "fixed_composed_F_64k               3.894379   156699          17.575249  \n",
       "fixed_lexical_F_32k                5.956904   245851          26.269544  \n",
       "fixed_lexical_F_64k                3.669871   146965          17.113886  \n",
       "fixed_pure_F_32k                   5.855999   240872          25.793359  \n",
       "fixed_pure_F_64k                   3.624766   145240          16.919890  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_group = nsmc.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean', 'sum'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean'],  \n",
    "})\n",
    "nsmc_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(nsmc[nsmc['tokenized_len'] > 128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "paws_train = pd.read_csv('paws_train.tsv', sep='\\t')\n",
    "paws_dev = pd.read_csv('paws_dev.tsv', sep='\\t')\n",
    "paws_test = pd.read_csv('paws_test.tsv', sep='\\t')\n",
    "paws = pd.concat([paws_train, paws_dev, paws_test])\n",
    "\n",
    "paws['tokenize_result'] = paws['tokenize_result'].apply(str)\n",
    "paws['tokenize_result'] = paws['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "paws['source_len'] = paws['source'].apply(lambda x: len(x.split()))\n",
    "paws['tokenized_len'] = paws['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "paws['OOV_per_tokenized_sent'] = paws['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "paws['OOV_count'] = paws['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "paws['OOV_per_source_sent'] = paws['OOV_count'] / paws['source_len'] * 100\n",
    "paws['##_per_tokenized_sent'] = paws['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "paws['##_count'] = paws['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "paws['##_per_source_sent'] = paws['##_count'] / paws['source_len'] * 100\n",
    "\n",
    "paws_group = paws.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean', 'sum'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean'],  \n",
    "})\n",
    "paws_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, tokenize_result, source_len, tokenized_len, OOV_per_tokenized_sent, OOV_count, OOV_per_source_sent, ##_per_tokenized_sent, ##_count, ##_per_source_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws[paws['tokenized_len'] > 128].groupby('tokenizer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paws' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m paws\u001b[39m.\u001b[39misna()\u001b[39m.\u001b[39msum()\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(paws[paws[\u001b[39m'\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misna()]))\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(paws_train), \u001b[39mlen\u001b[39m(paws_dev), \u001b[39mlen\u001b[39m(paws_test))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paws' is not defined"
     ]
    }
   ],
   "source": [
    "paws.isna().sum()\n",
    "print(len(paws[paws['tokenizer'].isna()]))\n",
    "print(len(paws_train), len(paws_dev), len(paws_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th colspan=\"2\" halign=\"left\">tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>20.849289</td>\n",
       "      <td>174446</td>\n",
       "      <td>0.270432</td>\n",
       "      <td>803</td>\n",
       "      <td>1.595408</td>\n",
       "      <td>58.494769</td>\n",
       "      <td>104113</td>\n",
       "      <td>169.389726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>18.462890</td>\n",
       "      <td>154479</td>\n",
       "      <td>0.292663</td>\n",
       "      <td>803</td>\n",
       "      <td>1.595408</td>\n",
       "      <td>53.111395</td>\n",
       "      <td>84146</td>\n",
       "      <td>139.137183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>19.770288</td>\n",
       "      <td>165418</td>\n",
       "      <td>0.276332</td>\n",
       "      <td>794</td>\n",
       "      <td>1.589262</td>\n",
       "      <td>56.128817</td>\n",
       "      <td>95085</td>\n",
       "      <td>155.923334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>18.094419</td>\n",
       "      <td>151396</td>\n",
       "      <td>0.296624</td>\n",
       "      <td>794</td>\n",
       "      <td>1.589262</td>\n",
       "      <td>52.066382</td>\n",
       "      <td>81063</td>\n",
       "      <td>134.379660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>24.718179</td>\n",
       "      <td>206817</td>\n",
       "      <td>0.210914</td>\n",
       "      <td>1031</td>\n",
       "      <td>1.951710</td>\n",
       "      <td>9.324234</td>\n",
       "      <td>18142</td>\n",
       "      <td>30.930292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>23.516434</td>\n",
       "      <td>196762</td>\n",
       "      <td>0.222857</td>\n",
       "      <td>1031</td>\n",
       "      <td>1.951710</td>\n",
       "      <td>4.448516</td>\n",
       "      <td>8087</td>\n",
       "      <td>14.188006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>24.185132</td>\n",
       "      <td>202357</td>\n",
       "      <td>0.215083</td>\n",
       "      <td>1028</td>\n",
       "      <td>1.941751</td>\n",
       "      <td>7.246773</td>\n",
       "      <td>13682</td>\n",
       "      <td>23.700267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>23.412095</td>\n",
       "      <td>195889</td>\n",
       "      <td>0.223857</td>\n",
       "      <td>1028</td>\n",
       "      <td>1.941751</td>\n",
       "      <td>4.007282</td>\n",
       "      <td>7214</td>\n",
       "      <td>12.934445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>24.155372</td>\n",
       "      <td>202108</td>\n",
       "      <td>0.214752</td>\n",
       "      <td>1025</td>\n",
       "      <td>1.935775</td>\n",
       "      <td>7.132793</td>\n",
       "      <td>13433</td>\n",
       "      <td>23.295903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>23.398948</td>\n",
       "      <td>195779</td>\n",
       "      <td>0.223262</td>\n",
       "      <td>1025</td>\n",
       "      <td>1.935775</td>\n",
       "      <td>3.939866</td>\n",
       "      <td>7104</td>\n",
       "      <td>12.729737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len         OOV_per_tokenized_sent  \\\n",
       "                            mean          mean     sum                   mean   \n",
       "tokenizer                                                                       \n",
       "eojeol_composed_F_32k      8.406     20.849289  174446               0.270432   \n",
       "eojeol_composed_F_64k      8.406     18.462890  154479               0.292663   \n",
       "eojeol_pure_F_32k          8.406     19.770288  165418               0.276332   \n",
       "eojeol_pure_F_64k          8.406     18.094419  151396               0.296624   \n",
       "fixed_composed_F_32k       8.406     24.718179  206817               0.210914   \n",
       "fixed_composed_F_64k       8.406     23.516434  196762               0.222857   \n",
       "fixed_lexical_F_32k        8.406     24.185132  202357               0.215083   \n",
       "fixed_lexical_F_64k        8.406     23.412095  195889               0.223857   \n",
       "fixed_pure_F_32k           8.406     24.155372  202108               0.214752   \n",
       "fixed_pure_F_64k           8.406     23.398948  195779               0.223262   \n",
       "\n",
       "                      OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                            sum                mean                  mean   \n",
       "tokenizer                                                                   \n",
       "eojeol_composed_F_32k       803            1.595408             58.494769   \n",
       "eojeol_composed_F_64k       803            1.595408             53.111395   \n",
       "eojeol_pure_F_32k           794            1.589262             56.128817   \n",
       "eojeol_pure_F_64k           794            1.589262             52.066382   \n",
       "fixed_composed_F_32k       1031            1.951710              9.324234   \n",
       "fixed_composed_F_64k       1031            1.951710              4.448516   \n",
       "fixed_lexical_F_32k        1028            1.941751              7.246773   \n",
       "fixed_lexical_F_64k        1028            1.941751              4.007282   \n",
       "fixed_pure_F_32k           1025            1.935775              7.132793   \n",
       "fixed_pure_F_64k           1025            1.935775              3.939866   \n",
       "\n",
       "                      ##_count ##_per_source_sent  \n",
       "                           sum               mean  \n",
       "tokenizer                                          \n",
       "eojeol_composed_F_32k   104113         169.389726  \n",
       "eojeol_composed_F_64k    84146         139.137183  \n",
       "eojeol_pure_F_32k        95085         155.923334  \n",
       "eojeol_pure_F_64k        81063         134.379660  \n",
       "fixed_composed_F_32k     18142          30.930292  \n",
       "fixed_composed_F_64k      8087          14.188006  \n",
       "fixed_lexical_F_32k      13682          23.700267  \n",
       "fixed_lexical_F_64k       7214          12.934445  \n",
       "fixed_pure_F_32k         13433          23.295903  \n",
       "fixed_pure_F_64k          7104          12.729737  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsd_train = pd.read_csv('hsd_train.tsv', sep='\\t')\n",
    "hsd_dev = pd.read_csv('hsd_dev.tsv', sep='\\t')\n",
    "hsd = pd.concat([hsd_train, hsd_dev])\n",
    "\n",
    "hsd['tokenize_result'] = hsd['tokenize_result'].apply(str)\n",
    "hsd['tokenize_result'] = hsd['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "hsd['source_len'] = hsd['source'].apply(lambda x: len(x.split()))\n",
    "hsd['tokenized_len'] = hsd['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "hsd['OOV_per_tokenized_sent'] = hsd['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "hsd['OOV_count'] = hsd['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "hsd['OOV_per_source_sent'] = hsd['OOV_count'] / hsd['source_len'] * 100\n",
    "hsd['##_per_tokenized_sent'] = hsd['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "hsd['##_count'] = hsd['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "hsd['##_per_source_sent'] = hsd['##_count'] / hsd['source_len'] * 100\n",
    "\n",
    "hsd_group = hsd.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean', 'sum'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean'],  \n",
    "})\n",
    "hsd_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, tokenize_result, source_len, tokenized_len, OOV_per_tokenized_sent, OOV_count, OOV_per_source_sent, ##_per_tokenized_sent, ##_count, ##_per_source_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsd[hsd['tokenized_len'] > 128].groupby('tokenizer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th colspan=\"2\" halign=\"left\">tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>21.536333</td>\n",
       "      <td>258436</td>\n",
       "      <td>0.240620</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>45.853804</td>\n",
       "      <td>121449</td>\n",
       "      <td>91.453097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>18.498500</td>\n",
       "      <td>221982</td>\n",
       "      <td>0.276842</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>36.989648</td>\n",
       "      <td>84995</td>\n",
       "      <td>64.573298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>19.841750</td>\n",
       "      <td>238101</td>\n",
       "      <td>0.259793</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>41.279413</td>\n",
       "      <td>101114</td>\n",
       "      <td>76.673673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>17.945833</td>\n",
       "      <td>215350</td>\n",
       "      <td>0.285277</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>35.037910</td>\n",
       "      <td>78363</td>\n",
       "      <td>59.605604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>27.813750</td>\n",
       "      <td>333765</td>\n",
       "      <td>0.185928</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>5.710300</td>\n",
       "      <td>21074</td>\n",
       "      <td>14.808530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>26.661667</td>\n",
       "      <td>319940</td>\n",
       "      <td>0.194508</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>2.004738</td>\n",
       "      <td>7249</td>\n",
       "      <td>5.042086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>27.215167</td>\n",
       "      <td>326582</td>\n",
       "      <td>0.190371</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>3.849567</td>\n",
       "      <td>13891</td>\n",
       "      <td>9.798267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>26.539333</td>\n",
       "      <td>318472</td>\n",
       "      <td>0.195732</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>1.630206</td>\n",
       "      <td>5781</td>\n",
       "      <td>4.088706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>27.185583</td>\n",
       "      <td>326227</td>\n",
       "      <td>0.190566</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>3.749132</td>\n",
       "      <td>13536</td>\n",
       "      <td>9.538848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>26.534000</td>\n",
       "      <td>318408</td>\n",
       "      <td>0.195764</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>1.611258</td>\n",
       "      <td>5717</td>\n",
       "      <td>4.038697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len         OOV_per_tokenized_sent  \\\n",
       "                            mean          mean     sum                   mean   \n",
       "tokenizer                                                                       \n",
       "eojeol_composed_F_32k  11.415583     21.536333  258436               0.240620   \n",
       "eojeol_composed_F_64k  11.415583     18.498500  221982               0.276842   \n",
       "eojeol_pure_F_32k      11.415583     19.841750  238101               0.259793   \n",
       "eojeol_pure_F_64k      11.415583     17.945833  215350               0.285277   \n",
       "fixed_composed_F_32k   11.415583     27.813750  333765               0.185928   \n",
       "fixed_composed_F_64k   11.415583     26.661667  319940               0.194508   \n",
       "fixed_lexical_F_32k    11.415583     27.215167  326582               0.190371   \n",
       "fixed_lexical_F_64k    11.415583     26.539333  318472               0.195732   \n",
       "fixed_pure_F_32k       11.415583     27.185583  326227               0.190566   \n",
       "fixed_pure_F_64k       11.415583     26.534000  318408               0.195764   \n",
       "\n",
       "                      OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                            sum                mean                  mean   \n",
       "tokenizer                                                                   \n",
       "eojeol_composed_F_32k      2881            1.451239             45.853804   \n",
       "eojeol_composed_F_64k      2881            1.451239             36.989648   \n",
       "eojeol_pure_F_32k          2881            1.451239             41.279413   \n",
       "eojeol_pure_F_64k          2881            1.451239             35.037910   \n",
       "fixed_composed_F_32k       3346            1.665835              5.710300   \n",
       "fixed_composed_F_64k       3346            1.665835              2.004738   \n",
       "fixed_lexical_F_32k        3346            1.665835              3.849567   \n",
       "fixed_lexical_F_64k        3346            1.665835              1.630206   \n",
       "fixed_pure_F_32k           3346            1.665835              3.749132   \n",
       "fixed_pure_F_64k           3346            1.665835              1.611258   \n",
       "\n",
       "                      ##_count ##_per_source_sent  \n",
       "                           sum               mean  \n",
       "tokenizer                                          \n",
       "eojeol_composed_F_32k   121449          91.453097  \n",
       "eojeol_composed_F_64k    84995          64.573298  \n",
       "eojeol_pure_F_32k       101114          76.673673  \n",
       "eojeol_pure_F_64k        78363          59.605604  \n",
       "fixed_composed_F_32k     21074          14.808530  \n",
       "fixed_composed_F_64k      7249           5.042086  \n",
       "fixed_lexical_F_32k      13891           9.798267  \n",
       "fixed_lexical_F_64k       5781           4.088706  \n",
       "fixed_pure_F_32k         13536           9.538848  \n",
       "fixed_pure_F_64k          5717           4.038697  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_train = pd.read_csv('dp_train.tsv', sep='\\t')\n",
    "dp_dev = pd.read_csv('dp_dev.tsv', sep='\\t')\n",
    "dp = pd.concat([dp_train, dp_dev])\n",
    "\n",
    "dp['tokenize_result'] = dp['tokenize_result'].apply(str)\n",
    "dp['tokenize_result'] = dp['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "dp['source_len'] = dp['source'].apply(lambda x: len(x.split()))\n",
    "dp['tokenized_len'] = dp['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "dp['OOV_per_tokenized_sent'] = dp['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "dp['OOV_count'] = dp['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "dp['OOV_per_source_sent'] = dp['OOV_count'] / dp['source_len'] * 100\n",
    "dp['##_per_tokenized_sent'] = dp['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "dp['##_count'] = dp['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "dp['##_per_source_sent'] = dp['##_count'] / dp['source_len'] * 100\n",
    "\n",
    "dp_group = dp.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean', 'sum'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean'],  \n",
    "})\n",
    "dp_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, tokenize_result, source_len, tokenized_len, OOV_per_tokenized_sent, OOV_count, OOV_per_source_sent, ##_per_tokenized_sent, ##_count, ##_per_source_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp[dp['tokenized_len'] > 128].groupby('tokenizer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>15.332113</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>151</td>\n",
       "      <td>0.02578</td>\n",
       "      <td>44.817167</td>\n",
       "      <td>395046</td>\n",
       "      <td>90.252580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>13.176370</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>151</td>\n",
       "      <td>0.02578</td>\n",
       "      <td>35.874211</td>\n",
       "      <td>274333</td>\n",
       "      <td>63.527354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>14.121044</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>151</td>\n",
       "      <td>0.02578</td>\n",
       "      <td>40.045834</td>\n",
       "      <td>327231</td>\n",
       "      <td>75.100799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>12.774127</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>151</td>\n",
       "      <td>0.02578</td>\n",
       "      <td>33.887470</td>\n",
       "      <td>251809</td>\n",
       "      <td>58.397642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>20.357936</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>5.907860</td>\n",
       "      <td>73156</td>\n",
       "      <td>16.203469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.523573</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>2.234514</td>\n",
       "      <td>26435</td>\n",
       "      <td>5.897201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.931638</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>4.104835</td>\n",
       "      <td>49285</td>\n",
       "      <td>11.089676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.424727</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>1.784534</td>\n",
       "      <td>20900</td>\n",
       "      <td>4.699381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.909172</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>4.003570</td>\n",
       "      <td>48027</td>\n",
       "      <td>10.805589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.422119</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>1.772856</td>\n",
       "      <td>20754</td>\n",
       "      <td>4.670615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_tokenized_sent  \\\n",
       "                            mean          mean                   mean   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k   8.277216     15.332113               0.003902   \n",
       "eojeol_composed_F_64k   8.277216     13.176370               0.004501   \n",
       "eojeol_pure_F_32k       8.277216     14.121044               0.004100   \n",
       "eojeol_pure_F_64k       8.277216     12.774127               0.004600   \n",
       "fixed_composed_F_32k    8.277216     20.357936               0.003230   \n",
       "fixed_composed_F_64k    8.277216     19.523573               0.003463   \n",
       "fixed_lexical_F_32k     8.277216     19.931638               0.003309   \n",
       "fixed_lexical_F_64k     8.277216     19.424727               0.003484   \n",
       "fixed_pure_F_32k        8.277216     19.909172               0.003309   \n",
       "fixed_pure_F_64k        8.277216     19.422119               0.003484   \n",
       "\n",
       "                      OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                            sum                mean                  mean   \n",
       "tokenizer                                                                   \n",
       "eojeol_composed_F_32k       151             0.02578             44.817167   \n",
       "eojeol_composed_F_64k       151             0.02578             35.874211   \n",
       "eojeol_pure_F_32k           151             0.02578             40.045834   \n",
       "eojeol_pure_F_64k           151             0.02578             33.887470   \n",
       "fixed_composed_F_32k        196             0.03105              5.907860   \n",
       "fixed_composed_F_64k        196             0.03105              2.234514   \n",
       "fixed_lexical_F_32k         196             0.03105              4.104835   \n",
       "fixed_lexical_F_64k         196             0.03105              1.784534   \n",
       "fixed_pure_F_32k            196             0.03105              4.003570   \n",
       "fixed_pure_F_64k            196             0.03105              1.772856   \n",
       "\n",
       "                      ##_count ##_per_source_sent  \n",
       "                           sum               mean  \n",
       "tokenizer                                          \n",
       "eojeol_composed_F_32k   395046          90.252580  \n",
       "eojeol_composed_F_64k   274333          63.527354  \n",
       "eojeol_pure_F_32k       327231          75.100799  \n",
       "eojeol_pure_F_64k       251809          58.397642  \n",
       "fixed_composed_F_32k     73156          16.203469  \n",
       "fixed_composed_F_64k     26435           5.897201  \n",
       "fixed_lexical_F_32k      49285          11.089676  \n",
       "fixed_lexical_F_64k      20900           4.699381  \n",
       "fixed_pure_F_32k         48027          10.805589  \n",
       "fixed_pure_F_64k         20754           4.670615  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_train = pd.read_csv('nli_train.tsv', sep='\\t')\n",
    "nli_dev = pd.read_csv('nli_dev.tsv', sep='\\t')\n",
    "nli = pd.concat([nli_train, nli_dev])\n",
    "\n",
    "nli['tokenize_result'] = nli['tokenize_result'].apply(str)\n",
    "nli['tokenize_result'] = nli['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "nli['source_len'] = nli['source'].apply(lambda x: len(x.split()))\n",
    "nli['tokenized_len'] = nli['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "nli['OOV_per_tokenized_sent'] = nli['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "nli['OOV_count'] = nli['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "nli['OOV_per_source_sent'] = nli['OOV_count'] / nli['source_len'] * 100\n",
    "nli['##_per_tokenized_sent'] = nli['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "nli['##_count'] = nli['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "nli['##_per_source_sent'] = nli['##_count'] / nli['source_len'] * 100\n",
    "\n",
    "nli_group = nli.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean'],  \n",
    "})\n",
    "nli_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, tokenize_result, source_len, tokenized_len, OOV_per_tokenized_sent, OOV_count, OOV_per_source_sent, ##_per_tokenized_sent, ##_count, ##_per_source_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli[nli['tokenized_len'] > 128].groupby('tokenizer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17996.0 199992.0 106132.0 8367.0 12000.0 55996.0\n"
     ]
    }
   ],
   "source": [
    "# size\n",
    "print(len(cola)/10,\n",
    "len(nsmc)/10,\n",
    "len(paws)/10,\n",
    "len(hsd)/10,\n",
    "len(dp)/10,\n",
    "len(nli)/10\n",
    ")\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "# save results\n",
    "cola.to_csv('results/cola_result.tsv', sep='\\t', index=False)\n",
    "nsmc.to_csv('results/nsmc_result.tsv', sep='\\t', index=False)\n",
    "paws.to_csv('results/paws_result.tsv', sep='\\t', index=False)\n",
    "hsd.to_csv('results/hsd_result.tsv', sep='\\t', index=False)\n",
    "dp.to_csv('results/dp_result.tsv', sep='\\t', index=False)\n",
    "nli.to_csv('results/nli_result.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola_group['task'] = 'cola'\n",
    "dp_group['task'] = 'dp'\n",
    "hsd_group['task'] = 'hsd'\n",
    "nli_group['task'] = 'nli'\n",
    "nsmc_group['task'] = 'nsmc'\n",
    "paws_group['task'] = 'paws'\n",
    "\n",
    "cola['task'] = 'cola'\n",
    "dp['task'] = 'dp'\n",
    "hsd['task'] = 'hsd'\n",
    "nli['task'] = 'nli'\n",
    "nsmc['task'] = 'nsmc'\n",
    "paws['task'] = 'paws'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result = pd.concat([cola.reset_index(),\n",
    "                          dp.reset_index(),\n",
    "                          hsd.reset_index(),\n",
    "                          nli.reset_index(),\n",
    "                          nsmc.reset_index(),\n",
    "                          paws.reset_index()])\n",
    "total_result\n",
    "total_result.to_csv('results/total_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>source_len</th>\n",
       "      <th colspan=\"2\" halign=\"left\">tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eojeol_composed_F_32k</td>\n",
       "      <td>5.011058</td>\n",
       "      <td>8.460269</td>\n",
       "      <td>152251.0</td>\n",
       "      <td>0.032876</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>39.408974</td>\n",
       "      <td>62072</td>\n",
       "      <td>72.816467</td>\n",
       "      <td>cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eojeol_composed_F_64k</td>\n",
       "      <td>5.011058</td>\n",
       "      <td>7.428540</td>\n",
       "      <td>133684.0</td>\n",
       "      <td>0.036712</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>31.165140</td>\n",
       "      <td>43505</td>\n",
       "      <td>51.650842</td>\n",
       "      <td>cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eojeol_pure_F_32k</td>\n",
       "      <td>5.011058</td>\n",
       "      <td>7.844354</td>\n",
       "      <td>141167.0</td>\n",
       "      <td>0.035132</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>34.697582</td>\n",
       "      <td>50988</td>\n",
       "      <td>60.109946</td>\n",
       "      <td>cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eojeol_pure_F_64k</td>\n",
       "      <td>5.011058</td>\n",
       "      <td>7.228717</td>\n",
       "      <td>130088.0</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>29.298988</td>\n",
       "      <td>39909</td>\n",
       "      <td>47.571706</td>\n",
       "      <td>cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>5.011058</td>\n",
       "      <td>12.380196</td>\n",
       "      <td>222794.0</td>\n",
       "      <td>0.019880</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>4.353754</td>\n",
       "      <td>10372</td>\n",
       "      <td>12.154478</td>\n",
       "      <td>cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fixed_composed_F_64k</td>\n",
       "      <td>5.011058</td>\n",
       "      <td>11.972772</td>\n",
       "      <td>215462.0</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>1.323949</td>\n",
       "      <td>3040</td>\n",
       "      <td>3.629573</td>\n",
       "      <td>cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fixed_lexical_F_32k</td>\n",
       "      <td>5.011058</td>\n",
       "      <td>12.213770</td>\n",
       "      <td>219799.0</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>3.129198</td>\n",
       "      <td>7377</td>\n",
       "      <td>8.710197</td>\n",
       "      <td>cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fixed_lexical_F_64k</td>\n",
       "      <td>5.011058</td>\n",
       "      <td>11.917037</td>\n",
       "      <td>214459.0</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>0.884107</td>\n",
       "      <td>2037</td>\n",
       "      <td>2.420483</td>\n",
       "      <td>cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fixed_pure_F_32k</td>\n",
       "      <td>5.011058</td>\n",
       "      <td>12.207157</td>\n",
       "      <td>219680.0</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>3.083478</td>\n",
       "      <td>7258</td>\n",
       "      <td>8.578487</td>\n",
       "      <td>cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fixed_pure_F_64k</td>\n",
       "      <td>5.011058</td>\n",
       "      <td>11.914259</td>\n",
       "      <td>214409.0</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>0.861383</td>\n",
       "      <td>1987</td>\n",
       "      <td>2.356606</td>\n",
       "      <td>cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eojeol_composed_F_32k</td>\n",
       "      <td>11.415583</td>\n",
       "      <td>21.536333</td>\n",
       "      <td>258436.0</td>\n",
       "      <td>0.240620</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>45.853804</td>\n",
       "      <td>121449</td>\n",
       "      <td>91.453097</td>\n",
       "      <td>dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eojeol_composed_F_64k</td>\n",
       "      <td>11.415583</td>\n",
       "      <td>18.498500</td>\n",
       "      <td>221982.0</td>\n",
       "      <td>0.276842</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>36.989648</td>\n",
       "      <td>84995</td>\n",
       "      <td>64.573298</td>\n",
       "      <td>dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eojeol_pure_F_32k</td>\n",
       "      <td>11.415583</td>\n",
       "      <td>19.841750</td>\n",
       "      <td>238101.0</td>\n",
       "      <td>0.259793</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>41.279413</td>\n",
       "      <td>101114</td>\n",
       "      <td>76.673673</td>\n",
       "      <td>dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eojeol_pure_F_64k</td>\n",
       "      <td>11.415583</td>\n",
       "      <td>17.945833</td>\n",
       "      <td>215350.0</td>\n",
       "      <td>0.285277</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>35.037910</td>\n",
       "      <td>78363</td>\n",
       "      <td>59.605604</td>\n",
       "      <td>dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>11.415583</td>\n",
       "      <td>27.813750</td>\n",
       "      <td>333765.0</td>\n",
       "      <td>0.185928</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>5.710300</td>\n",
       "      <td>21074</td>\n",
       "      <td>14.808530</td>\n",
       "      <td>dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fixed_composed_F_64k</td>\n",
       "      <td>11.415583</td>\n",
       "      <td>26.661667</td>\n",
       "      <td>319940.0</td>\n",
       "      <td>0.194508</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>2.004738</td>\n",
       "      <td>7249</td>\n",
       "      <td>5.042086</td>\n",
       "      <td>dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fixed_lexical_F_32k</td>\n",
       "      <td>11.415583</td>\n",
       "      <td>27.215167</td>\n",
       "      <td>326582.0</td>\n",
       "      <td>0.190371</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>3.849567</td>\n",
       "      <td>13891</td>\n",
       "      <td>9.798267</td>\n",
       "      <td>dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fixed_lexical_F_64k</td>\n",
       "      <td>11.415583</td>\n",
       "      <td>26.539333</td>\n",
       "      <td>318472.0</td>\n",
       "      <td>0.195732</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>1.630206</td>\n",
       "      <td>5781</td>\n",
       "      <td>4.088706</td>\n",
       "      <td>dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fixed_pure_F_32k</td>\n",
       "      <td>11.415583</td>\n",
       "      <td>27.185583</td>\n",
       "      <td>326227.0</td>\n",
       "      <td>0.190566</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>3.749132</td>\n",
       "      <td>13536</td>\n",
       "      <td>9.538848</td>\n",
       "      <td>dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fixed_pure_F_64k</td>\n",
       "      <td>11.415583</td>\n",
       "      <td>26.534000</td>\n",
       "      <td>318408.0</td>\n",
       "      <td>0.195764</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>1.611258</td>\n",
       "      <td>5717</td>\n",
       "      <td>4.038697</td>\n",
       "      <td>dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eojeol_composed_F_32k</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>20.849289</td>\n",
       "      <td>174446.0</td>\n",
       "      <td>0.270432</td>\n",
       "      <td>803</td>\n",
       "      <td>1.595408</td>\n",
       "      <td>58.494769</td>\n",
       "      <td>104113</td>\n",
       "      <td>169.389726</td>\n",
       "      <td>hsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eojeol_composed_F_64k</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>18.462890</td>\n",
       "      <td>154479.0</td>\n",
       "      <td>0.292663</td>\n",
       "      <td>803</td>\n",
       "      <td>1.595408</td>\n",
       "      <td>53.111395</td>\n",
       "      <td>84146</td>\n",
       "      <td>139.137183</td>\n",
       "      <td>hsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eojeol_pure_F_32k</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>19.770288</td>\n",
       "      <td>165418.0</td>\n",
       "      <td>0.276332</td>\n",
       "      <td>794</td>\n",
       "      <td>1.589262</td>\n",
       "      <td>56.128817</td>\n",
       "      <td>95085</td>\n",
       "      <td>155.923334</td>\n",
       "      <td>hsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eojeol_pure_F_64k</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>18.094419</td>\n",
       "      <td>151396.0</td>\n",
       "      <td>0.296624</td>\n",
       "      <td>794</td>\n",
       "      <td>1.589262</td>\n",
       "      <td>52.066382</td>\n",
       "      <td>81063</td>\n",
       "      <td>134.379660</td>\n",
       "      <td>hsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>24.718179</td>\n",
       "      <td>206817.0</td>\n",
       "      <td>0.210914</td>\n",
       "      <td>1031</td>\n",
       "      <td>1.951710</td>\n",
       "      <td>9.324234</td>\n",
       "      <td>18142</td>\n",
       "      <td>30.930292</td>\n",
       "      <td>hsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fixed_composed_F_64k</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>23.516434</td>\n",
       "      <td>196762.0</td>\n",
       "      <td>0.222857</td>\n",
       "      <td>1031</td>\n",
       "      <td>1.951710</td>\n",
       "      <td>4.448516</td>\n",
       "      <td>8087</td>\n",
       "      <td>14.188006</td>\n",
       "      <td>hsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fixed_lexical_F_32k</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>24.185132</td>\n",
       "      <td>202357.0</td>\n",
       "      <td>0.215083</td>\n",
       "      <td>1028</td>\n",
       "      <td>1.941751</td>\n",
       "      <td>7.246773</td>\n",
       "      <td>13682</td>\n",
       "      <td>23.700267</td>\n",
       "      <td>hsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fixed_lexical_F_64k</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>23.412095</td>\n",
       "      <td>195889.0</td>\n",
       "      <td>0.223857</td>\n",
       "      <td>1028</td>\n",
       "      <td>1.941751</td>\n",
       "      <td>4.007282</td>\n",
       "      <td>7214</td>\n",
       "      <td>12.934445</td>\n",
       "      <td>hsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fixed_pure_F_32k</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>24.155372</td>\n",
       "      <td>202108.0</td>\n",
       "      <td>0.214752</td>\n",
       "      <td>1025</td>\n",
       "      <td>1.935775</td>\n",
       "      <td>7.132793</td>\n",
       "      <td>13433</td>\n",
       "      <td>23.295903</td>\n",
       "      <td>hsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fixed_pure_F_64k</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>23.398948</td>\n",
       "      <td>195779.0</td>\n",
       "      <td>0.223262</td>\n",
       "      <td>1025</td>\n",
       "      <td>1.935775</td>\n",
       "      <td>3.939866</td>\n",
       "      <td>7104</td>\n",
       "      <td>12.729737</td>\n",
       "      <td>hsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eojeol_composed_F_32k</td>\n",
       "      <td>8.277216</td>\n",
       "      <td>15.332113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>151</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>44.817167</td>\n",
       "      <td>395046</td>\n",
       "      <td>90.252580</td>\n",
       "      <td>nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eojeol_composed_F_64k</td>\n",
       "      <td>8.277216</td>\n",
       "      <td>13.176370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>151</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>35.874211</td>\n",
       "      <td>274333</td>\n",
       "      <td>63.527354</td>\n",
       "      <td>nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eojeol_pure_F_32k</td>\n",
       "      <td>8.277216</td>\n",
       "      <td>14.121044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>151</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>40.045834</td>\n",
       "      <td>327231</td>\n",
       "      <td>75.100799</td>\n",
       "      <td>nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eojeol_pure_F_64k</td>\n",
       "      <td>8.277216</td>\n",
       "      <td>12.774127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>151</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>33.887470</td>\n",
       "      <td>251809</td>\n",
       "      <td>58.397642</td>\n",
       "      <td>nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>8.277216</td>\n",
       "      <td>20.357936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>196</td>\n",
       "      <td>0.031050</td>\n",
       "      <td>5.907860</td>\n",
       "      <td>73156</td>\n",
       "      <td>16.203469</td>\n",
       "      <td>nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fixed_composed_F_64k</td>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.523573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>196</td>\n",
       "      <td>0.031050</td>\n",
       "      <td>2.234514</td>\n",
       "      <td>26435</td>\n",
       "      <td>5.897201</td>\n",
       "      <td>nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fixed_lexical_F_32k</td>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.931638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>196</td>\n",
       "      <td>0.031050</td>\n",
       "      <td>4.104835</td>\n",
       "      <td>49285</td>\n",
       "      <td>11.089676</td>\n",
       "      <td>nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fixed_lexical_F_64k</td>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.424727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>196</td>\n",
       "      <td>0.031050</td>\n",
       "      <td>1.784534</td>\n",
       "      <td>20900</td>\n",
       "      <td>4.699381</td>\n",
       "      <td>nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fixed_pure_F_32k</td>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.909172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>196</td>\n",
       "      <td>0.031050</td>\n",
       "      <td>4.003570</td>\n",
       "      <td>48027</td>\n",
       "      <td>10.805589</td>\n",
       "      <td>nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fixed_pure_F_64k</td>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.422119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>196</td>\n",
       "      <td>0.031050</td>\n",
       "      <td>1.772856</td>\n",
       "      <td>20754</td>\n",
       "      <td>4.670615</td>\n",
       "      <td>nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eojeol_composed_F_32k</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>17.727344</td>\n",
       "      <td>3545327.0</td>\n",
       "      <td>0.346157</td>\n",
       "      <td>8989</td>\n",
       "      <td>1.392632</td>\n",
       "      <td>54.903910</td>\n",
       "      <td>2027121</td>\n",
       "      <td>186.080295</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eojeol_composed_F_64k</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>15.669567</td>\n",
       "      <td>3133788.0</td>\n",
       "      <td>0.356818</td>\n",
       "      <td>8989</td>\n",
       "      <td>1.392632</td>\n",
       "      <td>48.895810</td>\n",
       "      <td>1615582</td>\n",
       "      <td>154.626038</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eojeol_pure_F_32k</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>16.681637</td>\n",
       "      <td>3336194.0</td>\n",
       "      <td>0.455792</td>\n",
       "      <td>9813</td>\n",
       "      <td>1.738694</td>\n",
       "      <td>52.051229</td>\n",
       "      <td>1817988</td>\n",
       "      <td>167.590889</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eojeol_pure_F_64k</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>15.262716</td>\n",
       "      <td>3052421.0</td>\n",
       "      <td>0.464452</td>\n",
       "      <td>9813</td>\n",
       "      <td>1.738694</td>\n",
       "      <td>47.621194</td>\n",
       "      <td>1534215</td>\n",
       "      <td>144.851687</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.892581</td>\n",
       "      <td>4378341.0</td>\n",
       "      <td>0.127833</td>\n",
       "      <td>10730</td>\n",
       "      <td>1.883802</td>\n",
       "      <td>7.515631</td>\n",
       "      <td>317082</td>\n",
       "      <td>32.287948</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fixed_composed_F_64k</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.090634</td>\n",
       "      <td>4217958.0</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>10730</td>\n",
       "      <td>1.883802</td>\n",
       "      <td>3.894379</td>\n",
       "      <td>156699</td>\n",
       "      <td>17.575249</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fixed_lexical_F_32k</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.536411</td>\n",
       "      <td>4307110.0</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>10448</td>\n",
       "      <td>1.797754</td>\n",
       "      <td>5.956904</td>\n",
       "      <td>245851</td>\n",
       "      <td>26.269544</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fixed_lexical_F_64k</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.041962</td>\n",
       "      <td>4208224.0</td>\n",
       "      <td>0.126059</td>\n",
       "      <td>10448</td>\n",
       "      <td>1.797754</td>\n",
       "      <td>3.669871</td>\n",
       "      <td>146965</td>\n",
       "      <td>17.113886</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fixed_pure_F_32k</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.511515</td>\n",
       "      <td>4302131.0</td>\n",
       "      <td>0.122186</td>\n",
       "      <td>10430</td>\n",
       "      <td>1.795306</td>\n",
       "      <td>5.855999</td>\n",
       "      <td>240872</td>\n",
       "      <td>25.793359</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fixed_pure_F_64k</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.033336</td>\n",
       "      <td>4206499.0</td>\n",
       "      <td>0.125915</td>\n",
       "      <td>10430</td>\n",
       "      <td>1.795306</td>\n",
       "      <td>3.624766</td>\n",
       "      <td>145240</td>\n",
       "      <td>16.919890</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eojeol_composed_F_32k</td>\n",
       "      <td>14.261856</td>\n",
       "      <td>31.760239</td>\n",
       "      <td>3369412.0</td>\n",
       "      <td>0.332959</td>\n",
       "      <td>52211</td>\n",
       "      <td>3.706431</td>\n",
       "      <td>52.528700</td>\n",
       "      <td>1856386</td>\n",
       "      <td>125.130972</td>\n",
       "      <td>paws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eojeol_composed_F_64k</td>\n",
       "      <td>14.261856</td>\n",
       "      <td>27.389041</td>\n",
       "      <td>2905676.0</td>\n",
       "      <td>0.378464</td>\n",
       "      <td>52211</td>\n",
       "      <td>3.706431</td>\n",
       "      <td>45.110259</td>\n",
       "      <td>1392650</td>\n",
       "      <td>94.064866</td>\n",
       "      <td>paws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eojeol_pure_F_32k</td>\n",
       "      <td>14.261856</td>\n",
       "      <td>29.551999</td>\n",
       "      <td>3135142.0</td>\n",
       "      <td>0.063106</td>\n",
       "      <td>30829</td>\n",
       "      <td>2.055070</td>\n",
       "      <td>49.030981</td>\n",
       "      <td>1622116</td>\n",
       "      <td>109.734915</td>\n",
       "      <td>paws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eojeol_pure_F_64k</td>\n",
       "      <td>14.261856</td>\n",
       "      <td>26.715211</td>\n",
       "      <td>2834190.0</td>\n",
       "      <td>0.068572</td>\n",
       "      <td>30829</td>\n",
       "      <td>2.055070</td>\n",
       "      <td>43.813119</td>\n",
       "      <td>1321164</td>\n",
       "      <td>89.547106</td>\n",
       "      <td>paws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>14.261856</td>\n",
       "      <td>35.557918</td>\n",
       "      <td>3772304.0</td>\n",
       "      <td>0.282816</td>\n",
       "      <td>52346</td>\n",
       "      <td>3.715277</td>\n",
       "      <td>20.455463</td>\n",
       "      <td>846546</td>\n",
       "      <td>57.006188</td>\n",
       "      <td>paws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fixed_composed_F_64k</td>\n",
       "      <td>14.261856</td>\n",
       "      <td>32.424436</td>\n",
       "      <td>3439876.0</td>\n",
       "      <td>0.307464</td>\n",
       "      <td>52346</td>\n",
       "      <td>3.715277</td>\n",
       "      <td>13.717539</td>\n",
       "      <td>514118</td>\n",
       "      <td>34.930494</td>\n",
       "      <td>paws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fixed_lexical_F_32k</td>\n",
       "      <td>14.261856</td>\n",
       "      <td>34.030267</td>\n",
       "      <td>3610237.0</td>\n",
       "      <td>0.296267</td>\n",
       "      <td>52346</td>\n",
       "      <td>3.715277</td>\n",
       "      <td>17.292841</td>\n",
       "      <td>684479</td>\n",
       "      <td>46.160720</td>\n",
       "      <td>paws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fixed_lexical_F_64k</td>\n",
       "      <td>14.261856</td>\n",
       "      <td>31.962305</td>\n",
       "      <td>3390849.0</td>\n",
       "      <td>0.311179</td>\n",
       "      <td>52346</td>\n",
       "      <td>3.715277</td>\n",
       "      <td>12.632391</td>\n",
       "      <td>465091</td>\n",
       "      <td>31.684449</td>\n",
       "      <td>paws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fixed_pure_F_32k</td>\n",
       "      <td>14.261856</td>\n",
       "      <td>34.159102</td>\n",
       "      <td>3623905.0</td>\n",
       "      <td>0.049922</td>\n",
       "      <td>30904</td>\n",
       "      <td>2.059366</td>\n",
       "      <td>17.618618</td>\n",
       "      <td>698147</td>\n",
       "      <td>47.287916</td>\n",
       "      <td>paws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fixed_pure_F_64k</td>\n",
       "      <td>14.261856</td>\n",
       "      <td>32.073646</td>\n",
       "      <td>3402661.0</td>\n",
       "      <td>0.052145</td>\n",
       "      <td>30904</td>\n",
       "      <td>2.059366</td>\n",
       "      <td>12.954692</td>\n",
       "      <td>476903</td>\n",
       "      <td>32.651603</td>\n",
       "      <td>paws</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tokenizer source_len tokenized_len             \\\n",
       "                               mean          mean        sum   \n",
       "0  eojeol_composed_F_32k   5.011058      8.460269   152251.0   \n",
       "1  eojeol_composed_F_64k   5.011058      7.428540   133684.0   \n",
       "2      eojeol_pure_F_32k   5.011058      7.844354   141167.0   \n",
       "3      eojeol_pure_F_64k   5.011058      7.228717   130088.0   \n",
       "4   fixed_composed_F_32k   5.011058     12.380196   222794.0   \n",
       "5   fixed_composed_F_64k   5.011058     11.972772   215462.0   \n",
       "6    fixed_lexical_F_32k   5.011058     12.213770   219799.0   \n",
       "7    fixed_lexical_F_64k   5.011058     11.917037   214459.0   \n",
       "8       fixed_pure_F_32k   5.011058     12.207157   219680.0   \n",
       "9       fixed_pure_F_64k   5.011058     11.914259   214409.0   \n",
       "0  eojeol_composed_F_32k  11.415583     21.536333   258436.0   \n",
       "1  eojeol_composed_F_64k  11.415583     18.498500   221982.0   \n",
       "2      eojeol_pure_F_32k  11.415583     19.841750   238101.0   \n",
       "3      eojeol_pure_F_64k  11.415583     17.945833   215350.0   \n",
       "4   fixed_composed_F_32k  11.415583     27.813750   333765.0   \n",
       "5   fixed_composed_F_64k  11.415583     26.661667   319940.0   \n",
       "6    fixed_lexical_F_32k  11.415583     27.215167   326582.0   \n",
       "7    fixed_lexical_F_64k  11.415583     26.539333   318472.0   \n",
       "8       fixed_pure_F_32k  11.415583     27.185583   326227.0   \n",
       "9       fixed_pure_F_64k  11.415583     26.534000   318408.0   \n",
       "0  eojeol_composed_F_32k   8.406000     20.849289   174446.0   \n",
       "1  eojeol_composed_F_64k   8.406000     18.462890   154479.0   \n",
       "2      eojeol_pure_F_32k   8.406000     19.770288   165418.0   \n",
       "3      eojeol_pure_F_64k   8.406000     18.094419   151396.0   \n",
       "4   fixed_composed_F_32k   8.406000     24.718179   206817.0   \n",
       "5   fixed_composed_F_64k   8.406000     23.516434   196762.0   \n",
       "6    fixed_lexical_F_32k   8.406000     24.185132   202357.0   \n",
       "7    fixed_lexical_F_64k   8.406000     23.412095   195889.0   \n",
       "8       fixed_pure_F_32k   8.406000     24.155372   202108.0   \n",
       "9       fixed_pure_F_64k   8.406000     23.398948   195779.0   \n",
       "0  eojeol_composed_F_32k   8.277216     15.332113        NaN   \n",
       "1  eojeol_composed_F_64k   8.277216     13.176370        NaN   \n",
       "2      eojeol_pure_F_32k   8.277216     14.121044        NaN   \n",
       "3      eojeol_pure_F_64k   8.277216     12.774127        NaN   \n",
       "4   fixed_composed_F_32k   8.277216     20.357936        NaN   \n",
       "5   fixed_composed_F_64k   8.277216     19.523573        NaN   \n",
       "6    fixed_lexical_F_32k   8.277216     19.931638        NaN   \n",
       "7    fixed_lexical_F_64k   8.277216     19.424727        NaN   \n",
       "8       fixed_pure_F_32k   8.277216     19.909172        NaN   \n",
       "9       fixed_pure_F_64k   8.277216     19.422119        NaN   \n",
       "0  eojeol_composed_F_32k   7.591344     17.727344  3545327.0   \n",
       "1  eojeol_composed_F_64k   7.591344     15.669567  3133788.0   \n",
       "2      eojeol_pure_F_32k   7.591344     16.681637  3336194.0   \n",
       "3      eojeol_pure_F_64k   7.591344     15.262716  3052421.0   \n",
       "4   fixed_composed_F_32k   7.591344     21.892581  4378341.0   \n",
       "5   fixed_composed_F_64k   7.591344     21.090634  4217958.0   \n",
       "6    fixed_lexical_F_32k   7.591344     21.536411  4307110.0   \n",
       "7    fixed_lexical_F_64k   7.591344     21.041962  4208224.0   \n",
       "8       fixed_pure_F_32k   7.591344     21.511515  4302131.0   \n",
       "9       fixed_pure_F_64k   7.591344     21.033336  4206499.0   \n",
       "0  eojeol_composed_F_32k  14.261856     31.760239  3369412.0   \n",
       "1  eojeol_composed_F_64k  14.261856     27.389041  2905676.0   \n",
       "2      eojeol_pure_F_32k  14.261856     29.551999  3135142.0   \n",
       "3      eojeol_pure_F_64k  14.261856     26.715211  2834190.0   \n",
       "4   fixed_composed_F_32k  14.261856     35.557918  3772304.0   \n",
       "5   fixed_composed_F_64k  14.261856     32.424436  3439876.0   \n",
       "6    fixed_lexical_F_32k  14.261856     34.030267  3610237.0   \n",
       "7    fixed_lexical_F_64k  14.261856     31.962305  3390849.0   \n",
       "8       fixed_pure_F_32k  14.261856     34.159102  3623905.0   \n",
       "9       fixed_pure_F_64k  14.261856     32.073646  3402661.0   \n",
       "\n",
       "  OOV_per_tokenized_sent OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                    mean       sum                mean                  mean   \n",
       "0               0.032876       189            0.162933             39.408974   \n",
       "1               0.036712       189            0.162933             31.165140   \n",
       "2               0.035132       189            0.162933             34.697582   \n",
       "3               0.037975       189            0.162933             29.298988   \n",
       "4               0.019880       216            0.181257              4.353754   \n",
       "5               0.020558       216            0.181257              1.323949   \n",
       "6               0.020423       216            0.181257              3.129198   \n",
       "7               0.020611       216            0.181257              0.884107   \n",
       "8               0.020423       216            0.181257              3.083478   \n",
       "9               0.020611       216            0.181257              0.861383   \n",
       "0               0.240620      2881            1.451239             45.853804   \n",
       "1               0.276842      2881            1.451239             36.989648   \n",
       "2               0.259793      2881            1.451239             41.279413   \n",
       "3               0.285277      2881            1.451239             35.037910   \n",
       "4               0.185928      3346            1.665835              5.710300   \n",
       "5               0.194508      3346            1.665835              2.004738   \n",
       "6               0.190371      3346            1.665835              3.849567   \n",
       "7               0.195732      3346            1.665835              1.630206   \n",
       "8               0.190566      3346            1.665835              3.749132   \n",
       "9               0.195764      3346            1.665835              1.611258   \n",
       "0               0.270432       803            1.595408             58.494769   \n",
       "1               0.292663       803            1.595408             53.111395   \n",
       "2               0.276332       794            1.589262             56.128817   \n",
       "3               0.296624       794            1.589262             52.066382   \n",
       "4               0.210914      1031            1.951710              9.324234   \n",
       "5               0.222857      1031            1.951710              4.448516   \n",
       "6               0.215083      1028            1.941751              7.246773   \n",
       "7               0.223857      1028            1.941751              4.007282   \n",
       "8               0.214752      1025            1.935775              7.132793   \n",
       "9               0.223262      1025            1.935775              3.939866   \n",
       "0               0.003902       151            0.025780             44.817167   \n",
       "1               0.004501       151            0.025780             35.874211   \n",
       "2               0.004100       151            0.025780             40.045834   \n",
       "3               0.004600       151            0.025780             33.887470   \n",
       "4               0.003230       196            0.031050              5.907860   \n",
       "5               0.003463       196            0.031050              2.234514   \n",
       "6               0.003309       196            0.031050              4.104835   \n",
       "7               0.003484       196            0.031050              1.784534   \n",
       "8               0.003309       196            0.031050              4.003570   \n",
       "9               0.003484       196            0.031050              1.772856   \n",
       "0               0.346157      8989            1.392632             54.903910   \n",
       "1               0.356818      8989            1.392632             48.895810   \n",
       "2               0.455792      9813            1.738694             52.051229   \n",
       "3               0.464452      9813            1.738694             47.621194   \n",
       "4               0.127833     10730            1.883802              7.515631   \n",
       "5               0.133200     10730            1.883802              3.894379   \n",
       "6               0.122378     10448            1.797754              5.956904   \n",
       "7               0.126059     10448            1.797754              3.669871   \n",
       "8               0.122186     10430            1.795306              5.855999   \n",
       "9               0.125915     10430            1.795306              3.624766   \n",
       "0               0.332959     52211            3.706431             52.528700   \n",
       "1               0.378464     52211            3.706431             45.110259   \n",
       "2               0.063106     30829            2.055070             49.030981   \n",
       "3               0.068572     30829            2.055070             43.813119   \n",
       "4               0.282816     52346            3.715277             20.455463   \n",
       "5               0.307464     52346            3.715277             13.717539   \n",
       "6               0.296267     52346            3.715277             17.292841   \n",
       "7               0.311179     52346            3.715277             12.632391   \n",
       "8               0.049922     30904            2.059366             17.618618   \n",
       "9               0.052145     30904            2.059366             12.954692   \n",
       "\n",
       "  ##_count ##_per_source_sent  task  \n",
       "       sum               mean        \n",
       "0    62072          72.816467  cola  \n",
       "1    43505          51.650842  cola  \n",
       "2    50988          60.109946  cola  \n",
       "3    39909          47.571706  cola  \n",
       "4    10372          12.154478  cola  \n",
       "5     3040           3.629573  cola  \n",
       "6     7377           8.710197  cola  \n",
       "7     2037           2.420483  cola  \n",
       "8     7258           8.578487  cola  \n",
       "9     1987           2.356606  cola  \n",
       "0   121449          91.453097    dp  \n",
       "1    84995          64.573298    dp  \n",
       "2   101114          76.673673    dp  \n",
       "3    78363          59.605604    dp  \n",
       "4    21074          14.808530    dp  \n",
       "5     7249           5.042086    dp  \n",
       "6    13891           9.798267    dp  \n",
       "7     5781           4.088706    dp  \n",
       "8    13536           9.538848    dp  \n",
       "9     5717           4.038697    dp  \n",
       "0   104113         169.389726   hsd  \n",
       "1    84146         139.137183   hsd  \n",
       "2    95085         155.923334   hsd  \n",
       "3    81063         134.379660   hsd  \n",
       "4    18142          30.930292   hsd  \n",
       "5     8087          14.188006   hsd  \n",
       "6    13682          23.700267   hsd  \n",
       "7     7214          12.934445   hsd  \n",
       "8    13433          23.295903   hsd  \n",
       "9     7104          12.729737   hsd  \n",
       "0   395046          90.252580   nli  \n",
       "1   274333          63.527354   nli  \n",
       "2   327231          75.100799   nli  \n",
       "3   251809          58.397642   nli  \n",
       "4    73156          16.203469   nli  \n",
       "5    26435           5.897201   nli  \n",
       "6    49285          11.089676   nli  \n",
       "7    20900           4.699381   nli  \n",
       "8    48027          10.805589   nli  \n",
       "9    20754           4.670615   nli  \n",
       "0  2027121         186.080295  nsmc  \n",
       "1  1615582         154.626038  nsmc  \n",
       "2  1817988         167.590889  nsmc  \n",
       "3  1534215         144.851687  nsmc  \n",
       "4   317082          32.287948  nsmc  \n",
       "5   156699          17.575249  nsmc  \n",
       "6   245851          26.269544  nsmc  \n",
       "7   146965          17.113886  nsmc  \n",
       "8   240872          25.793359  nsmc  \n",
       "9   145240          16.919890  nsmc  \n",
       "0  1856386         125.130972  paws  \n",
       "1  1392650          94.064866  paws  \n",
       "2  1622116         109.734915  paws  \n",
       "3  1321164          89.547106  paws  \n",
       "4   846546          57.006188  paws  \n",
       "5   514118          34.930494  paws  \n",
       "6   684479          46.160720  paws  \n",
       "7   465091          31.684449  paws  \n",
       "8   698147          47.287916  paws  \n",
       "9   476903          32.651603  paws  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_group_result = pd.concat([cola_group.reset_index(),\n",
    "           dp_group.reset_index(),\n",
    "           hsd_group.reset_index(),\n",
    "           nli_group.reset_index(),\n",
    "           nsmc_group.reset_index(),\n",
    "           paws_group.reset_index()])\n",
    "\n",
    "total_group_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_group_result.to_csv('total_group_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "emnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "453e7ba0431a1f965429b96a03a1d5546248e001661d541f6bea069f7abcbd9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
