{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import unicodedata\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "sys.path.append(\"..\")\n",
    "from scripts._mecab import Mecab\n",
    "from typing import Dict, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import (\n",
    "    # CharTokenizer,\n",
    "    # JamoTokenizer,\n",
    "    MeCabSentencePieceTokenizer_orig,\n",
    "    MeCabSentencePieceTokenizer_fixed,\n",
    "    MeCabSentencePieceTokenizer,\n",
    "    MeCabWordPieceTokenizer,\n",
    "    # MeCabTokenizer,\n",
    "    MeCabTokenizer_orig,\n",
    "    MeCabTokenizer_fixed,\n",
    "    MeCabTokenizer_all,\n",
    "    # MeCabSentencePieceTokenizer_kortok,\n",
    "    # MeCabTokenizer_kortok,\n",
    "    SentencePieceTokenizer,\n",
    "    WordPieceTokenizer,\n",
    "    Vocab,\n",
    "    # WordTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 토큰화 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_name: str, resource_dir: str, token_type, tokenizer_type: str , decomposition_type: str, space_symbol: str, dummy_letter: str, nfd: bool, grammatical_symbol: list = [\"\", \"\"], skip_special_tokens: bool = False, lexical_grammatical: bool = False):   # for LG\n",
    "    tokenizer_dir = os.path.join(resource_dir, tokenizer_name)\n",
    "\n",
    "    if tokenizer_name.startswith(\"sp-\"):\n",
    "        tokenizer = SentencePieceTokenizer(os.path.join(tokenizer_dir, \"tok.model\"))\n",
    "\n",
    "    elif tokenizer_name.startswith(\"mecab_\"):\n",
    "\n",
    "        sp = SentencePieceTokenizer(os.path.join(tokenizer_dir, \"tok.model\"))\n",
    "\n",
    "        if \"orig\" in tokenizer_name:\n",
    "            mecab = MeCabTokenizer_orig(tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter)\n",
    "            tokenizer = MeCabSentencePieceTokenizer_orig(mecab, sp, use_fixed=False) # mecab_sp_orig.py\n",
    "\n",
    "        elif \"fixed\" in tokenizer_name:\n",
    "            mecab = MeCabTokenizer_fixed(tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter)\n",
    "            tokenizer = MeCabSentencePieceTokenizer_fixed(mecab, sp, use_fixed=True) # mecab_fixed.py\n",
    "\n",
    "\n",
    "    # elif tokenizer_name.startswith(\"eojeol\") or tokenizer_name.startswith(\"morpheme\"):\n",
    "    elif tokenizer_name.startswith(\"eojeol\") or tokenizer_name.startswith(\"morpheme\") or tokenizer_name.startswith(\"LG\"):   # LG도 처리할 수 있도록\n",
    "        wp = WordPieceTokenizer(os.path.join(tokenizer_dir, \"bert_tokenizer.json\"), skip_special_tokens=False)\n",
    "        # mecab = MeCabTokenizer_all(token_type=token_type, tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter, nfd=nfd, grammatical_symbol=grammatical_symbol)\n",
    "        mecab = MeCabTokenizer_all(token_type=token_type, tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter, nfd=nfd, grammatical_symbol=grammatical_symbol, lexical_grammatical=lexical_grammatical)   # for LG\n",
    "        tokenizer = MeCabWordPieceTokenizer(mecab=mecab, wp=wp) # mecab_wp.py\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Wrong tokenizer name.\")\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_result(tokenizer, string, nfd: bool = True):\n",
    "    # if nfd == True:\n",
    "    #     string = str_to_nfd(string)\n",
    "\n",
    "    tokenized = tokenizer.tokenize(string)\n",
    "    # print(\" \".join(tokenized))\n",
    "    \n",
    "    return \" \".join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 64k\n",
    "tokenizer_eojeol_composed_F_64k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_composed_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"./resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_eojeol_decomposed_pure_F_64k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"./resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_composed_F_64k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_composed_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"./resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_lexical_F_64k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_lexical_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"./resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_lexical\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_pure_F_64k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"./resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "\n",
    "# 32k\n",
    "tokenizer_eojeol_composed_F_32k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_composed_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"./resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_eojeol_decomposed_pure_F_32k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"./resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_composed_F_32k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_composed_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"./resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_lexical_F_32k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_lexical_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"./resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_lexical\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_pure_F_32k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"./resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tokenizations(string):\n",
    "    # grammatical symbol F\n",
    "    eojeol_composed_F_64k = string + '\\t' + 'eojeol_composed_F_64k' + '\\t' + get_tokenized_result(tokenizer_eojeol_composed_F_64k, string)\n",
    "    eojeol_pure_F_64k = string + '\\t' + 'eojeol_pure_F_64k' + '\\t' + get_tokenized_result(tokenizer_eojeol_decomposed_pure_F_64k, string)\n",
    "    fixed_composed_F_64k = string + '\\t' + 'fixed_composed_F_64k' + '\\t' + get_tokenized_result(tokenizer_fixed_composed_F_64k, string)\n",
    "    fixed_lexical_F_64k = string + '\\t' + 'fixed_lexical_F_64k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_lexical_F_64k, string)\n",
    "    fixed_pure_F_64k = string + '\\t' + 'fixed_pure_F_64k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_pure_F_64k, string)\n",
    "\n",
    "    eojeol_composed_F_32k = string + '\\t' + 'eojeol_composed_F_32k' + '\\t' + get_tokenized_result(tokenizer_eojeol_composed_F_32k, string)\n",
    "    eojeol_pure_F_32k = string + '\\t' + 'eojeol_pure_F_32k' + '\\t' + get_tokenized_result(tokenizer_eojeol_decomposed_pure_F_32k, string)\n",
    "    fixed_composed_F_32k = string + '\\t' + 'fixed_composed_F_32k' + '\\t' + get_tokenized_result(tokenizer_fixed_composed_F_32k, string)\n",
    "    fixed_lexical_F_32k = string + '\\t' + 'fixed_lexical_F_32k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_lexical_F_32k, string)\n",
    "    fixed_pure_F_32k = string + '\\t' + 'fixed_pure_F_32k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_pure_F_32k, string)\n",
    "    \n",
    "    return \"\\n\".join(['\\n'+eojeol_composed_F_64k , eojeol_pure_F_64k , fixed_composed_F_64k, fixed_lexical_F_64k, fixed_pure_F_64k,\n",
    "    eojeol_composed_F_32k , eojeol_pure_F_32k , fixed_composed_F_32k, fixed_lexical_F_32k, fixed_pure_F_32k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_composed_F_64k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_pure_F_64k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_composed_F_64k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] [UNK] [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_lexical_F_64k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_pure_F_64k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_composed_F_32k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_pure_F_32k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_composed_F_32k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] [UNK] [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_lexical_F_32k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_pure_F_32\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_tokenizations('갑자기 더워彭肽꿿뜛땭뜎حمق')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 저장\n",
    "def analysis(task, corpus_list, corpus_name_list, sent2=True):\n",
    "    for corpus, corpus_name in zip(corpus_list, corpus_name_list):\n",
    "        with open('dataset_analysis/' + task+'_'+corpus_name+'.tsv', 'w', encoding='utf-8') as f:\n",
    "            f.write('source'+'\\t'+'tokenizer'+'\\t'+'tokenize_result')\n",
    "            if sent2:\n",
    "                for sent1, sent2 in zip(corpus[0], corpus[1]):\n",
    "                    f.write(show_tokenizations(string=sent1))\n",
    "                    f.write(show_tokenizations(string=sent2))\n",
    "            else:\n",
    "                for sent1 in (corpus):\n",
    "                    f.write(show_tokenizations(string=sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cola\n",
    "def load_data(file_path: str) -> Tuple[List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentences: List[str] = []\n",
    "\n",
    "    # for test set\n",
    "    if file_path == \"./dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_test_with_answer.tsv\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f.readlines()[1:]):\n",
    "                splitted = line.strip().split(\"\\t\")\n",
    "                sentences.append(splitted[2])\n",
    "\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f.readlines()[1:]):\n",
    "                splitted = line.strip().split(\"\\t\")\n",
    "                sentences.append(splitted[3])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "task = 'cola'\n",
    "train = load_data('./dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_train.tsv')\n",
    "dev = load_data('./dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_dev.tsv')\n",
    "test = load_data('./dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_test_with_answer.tsv')\n",
    "\n",
    "corpus_list = [train, dev, test]\n",
    "corpus_name_list = ['train', 'dev', 'test']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsmc\n",
    "\n",
    "def load_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentences: List[str] = []\n",
    "    labels: List[int] = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if len(splitted) != 2:\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            sentences.append(splitted[0])\n",
    "            \n",
    "#             labels.append(label_to_index[splitted[1]])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "task = 'nsmc'\n",
    "train = load_data('./dataset/nlu_tasks/nsmc/ratings_train.tsv')\n",
    "dev = load_data('./dataset/nlu_tasks/nsmc/ratings_dev.tsv')\n",
    "test = load_data('./dataset/nlu_tasks/nsmc/ratings_test.tsv')\n",
    "\n",
    "corpus_list = [train, dev, test]\n",
    "corpus_name_list = ['train', 'dev', 'test']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hsd\n",
    "\n",
    "def load_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence_a\n",
    "    2. sentence_b\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentence = []\n",
    "    # sentence_bs = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if len(splitted) != 4:\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "\n",
    "            sentence.append(splitted[0])\n",
    "                # sentence_bs.append(splitted[1])\n",
    "            # sentence_as.append(splitted[0])\n",
    "            # sentence_bs.append(splitted[1])\n",
    "\n",
    "#             labels.append(label_to_index[splitted[3]])\n",
    "\n",
    "    return sentence\n",
    "\n",
    "task = 'hsd'\n",
    "train = load_data('./dataset/nlu_tasks/hsd/train.tsv')\n",
    "dev = load_data('./dataset/nlu_tasks/hsd/dev.tsv')\n",
    "\n",
    "corpus_list = [train, dev]\n",
    "corpus_name_list = ['train', 'dev']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paws\n",
    "def load_data(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence_a\n",
    "    2. sentence_b\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentence_as = []\n",
    "    sentence_bs = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if len(splitted) != 4:\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            if splitted[1] == \"\" or splitted[2] == \"\":\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            # 문장이 \"NS\"로만 표기된 라인 제외\n",
    "            if splitted[1] == \"NS\" or splitted[2] == \"NS\":\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            sentence_as.append(splitted[1])\n",
    "            sentence_bs.append(splitted[2])\n",
    "        \n",
    "    return sentence_as, sentence_bs\n",
    "\n",
    "task = 'paws'\n",
    "train = load_data('./dataset/nlu_tasks/paws/translated_train.tsv')\n",
    "dev = load_data('./dataset/nlu_tasks/paws/dev_2k.tsv')\n",
    "test = load_data('./dataset/nlu_tasks/paws/test_2k.tsv')\n",
    "\n",
    "corpus_list = [train, dev, test]\n",
    "corpus_name_list = ['train', 'dev', 'test']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bongseok/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n",
      "/tmp/ipykernel_22291/2797297455.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_train['text'] = df2_train['text'].apply(lambda x: re.sub('## klue-dp-v1_train_.*\\t', '', x))\n",
      "/tmp/ipykernel_22291/2797297455.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_dev['text'] = df2_dev['text'].apply(lambda x: re.sub('## klue-dp-v1_dev_.*\\t', '', x))\n"
     ]
    }
   ],
   "source": [
    "# KLUE-dp\n",
    "dp_train = pd.read_csv('./KLUE-baseline/data/klue_benchmark/klue-dp-v1.1/klue-dp-v1.1_train.tsv',sep='delimiter', header=None)\n",
    "dp_dev = pd.read_csv('./KLUE-baseline/data/klue_benchmark/klue-dp-v1.1/klue-dp-v1.1_dev.tsv', sep='delimiter', header=None)\n",
    "dp_train.columns = ['text']\n",
    "dp_dev.columns = ['text']\n",
    "df2_train = dp_train['text'].str.contains('## klue-dp')\n",
    "df2_dev = dp_dev['text'].str.contains('## klue-dp')\n",
    "df2_train = dp_train[df2_train]\n",
    "df2_dev = dp_dev[df2_dev]\n",
    "df2_train['text'] = df2_train['text'].apply(lambda x: re.sub('## klue-dp-v1_train_.*\\t', '', x))\n",
    "df2_dev['text'] = df2_dev['text'].apply(lambda x: re.sub('## klue-dp-v1_dev_.*\\t', '', x))\n",
    "df2_train.to_csv('dp_orig_train.tsv', index=False, sep='\\t')\n",
    "df2_dev.to_csv('dp_orig_dev.tsv', index=False, sep='\\t')\n",
    "\n",
    "def load_data(file_path: str) -> Tuple[List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentenced\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentences: List[str] = []\n",
    "    #labels: List[int] = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip()\n",
    "            # if len(splitted) != 2:\n",
    "            #     #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "            #     continue\n",
    "            sentences.append(splitted)\n",
    "     #       labels.append(label_to_index[splitted[1]])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "task = 'dp'\n",
    "train = load_data('dp_orig_train.tsv')\n",
    "dev = load_data('dp_orig_dev.tsv')\n",
    "\n",
    "corpus_list = [train, dev]\n",
    "corpus_name_list = ['train', 'dev']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLUE-nli\n",
    "# json to tsv\n",
    "import json\n",
    "import pandas as pd\n",
    "with open('./KLUE-baseline/data/klue_benchmark/klue-nli-v1.1/klue-nli-v1.1_train.json', 'r') as f:\n",
    "    nli_train = json.load(f)\n",
    "with open('./KLUE-baseline/data/klue_benchmark/klue-nli-v1.1/klue-nli-v1.1_dev.json') as f2:\n",
    "    nli_dev = json.load(f2)\n",
    "\n",
    "\n",
    "nli_train = pd.DataFrame(nli_train)\n",
    "nli_train = nli_train[['premise', 'hypothesis']]\n",
    "nli_train.to_csv('nli_orig_train.tsv', index=False, sep='\\t')\n",
    "nli_dev = pd.DataFrame(nli_dev)\n",
    "nli_dev = nli_dev[['premise', 'hypothesis']]\n",
    "nli_dev.to_csv('nli_orig_dev.tsv', index=False, sep='\\t')\n",
    "\n",
    "def load_data(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence_a\n",
    "    2. sentence_b\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentence_as = []\n",
    "    sentence_bs = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if splitted[0] == \"\" or splitted[1] == \"\":\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            sentence_as.append(splitted[0])\n",
    "            sentence_bs.append(splitted[1])\n",
    "        \n",
    "    return sentence_as, sentence_bs\n",
    "\n",
    "task = 'nli'\n",
    "train = load_data('nli_orig_train.tsv')\n",
    "dev = load_data('nli_orig_dev.tsv')\n",
    "\n",
    "corpus_list = [train, dev]\n",
    "corpus_name_list = ['train', 'dev']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OOV rate, ## rate 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [UNK]의 개수 / 문장의 길이 * 100\n",
    "from typing import List\n",
    "\n",
    "# 문장당 oov rate (OR)\n",
    "def getOOVRatePerSentence(sentence):\n",
    "    # [CLS], [SEP] 제거\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    OOV_rate = sentence.count('[UNK]') / len(sentence) * 100   \n",
    "    \n",
    "    return OOV_rate\n",
    "\n",
    "# count of all OOV tokens (OC)\n",
    "def getCountofAllOOV(sentence):\n",
    "    cnt = 0\n",
    "    cnt += len(re.findall('[UNK]', sentence))\n",
    "    \n",
    "    return cnt\n",
    "\n",
    "# [CLS, SEP] 제거\n",
    "def removeCS(sentence):\n",
    "    sentence = sentence.replace('[CLS]', '')\n",
    "    sentence = sentence.replace('[SEP]', '')\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# [UNK]수 /전체 토큰 수\n",
    "def getOOVdividedbyAllTokens(corpus):\n",
    "    corpus['OC'] = corpus['sentence'].apply(lambda x: getCountofAllOOV(x))\n",
    "    corpus['sentence'].apply(lambda x: removeCS(x))\n",
    "    corpus['token_count'] = corpus['sentence'].apply(lambda x: len(x))\n",
    "    OOV_count = corpus['OC'].sum()\n",
    "    token_count = corpus['token_count'].sum()\n",
    "    \n",
    "    return OOV_count, token_count, OOV_count/token_count*100\n",
    "\n",
    "\n",
    "\n",
    "# \"##\"\" 세기\n",
    "# 문장당 oov rate (SR)\n",
    "def getShopRatePerSentence(sentence):\n",
    "\n",
    "    OOV_rate = sentence.count('##') / len(sentence) * 100   \n",
    "    \n",
    "    return OOV_rate\n",
    "\n",
    "# count of all ## tokens (SC)\n",
    "def getCountofAllShop(sentence):\n",
    "    cnt = 0\n",
    "    cnt += len(re.findall('##', sentence))\n",
    "    \n",
    "    return cnt\n",
    "\n",
    "# [CLS, SEP] 제거\n",
    "def removeCS(sentence):\n",
    "    sentence = sentence.replace('[CLS]', '')\n",
    "    sentence = sentence.replace('[SEP]', '')\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# ##수 /전체 토큰 수\n",
    "def getOOVdividedbyAllTokens(corpus):\n",
    "    corpus['SC'] = corpus['sentence'].apply(lambda x: getCountofAllShop(x))\n",
    "    corpus['sentence'].apply(lambda x: removeCS(x))\n",
    "    corpus['token_count'] = corpus['sentence'].apply(lambda x: len(x))\n",
    "    Shop_count = corpus['SC'].sum()\n",
    "    token_count = corpus['token_count'].sum()\n",
    "    \n",
    "    return Shop_count, token_count, Shop_count/token_count*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "cola_train = pd.read_csv('dataset_analysis/tokenized/cola_train.tsv', sep='\\t')\n",
    "cola_dev = pd.read_csv('dataset_analysis/tokenized/cola_dev.tsv', sep='\\t')\n",
    "cola_test = pd.read_csv('dataset_analysis/tokenized/cola_test.tsv', sep='\\t')\n",
    "cola = pd.concat([cola_train, cola_dev, cola_test])\n",
    "cola['tokenize_result'] = cola['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "cola['source_len'] = cola['source'].apply(lambda x: len(x.split()))\n",
    "cola['tokenized_len'] = cola['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "\n",
    "cola['OOV_per_tokenized_sent'] = cola['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "cola['OOV_count'] = cola['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "cola['OOV_per_source_sent'] = cola['OOV_count'] / cola['source_len'] * 100\n",
    "cola['##_per_tokenized_sent'] = cola['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "cola['##_count'] = cola['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "cola['##_per_source_sent'] = cola['##_count'] / cola['source_len'] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, tokenize_result, source_len, tokenized_len, OOV_per_tokenized_sent, OOV_count, OOV_per_source_sent, ##_per_tokenized_sent, ##_count, ##_per_source_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola[cola['tokenized_len'] > 128].groupby('tokenizer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>8.460269</td>\n",
       "      <td>0.032876</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>10.523272</td>\n",
       "      <td>62072</td>\n",
       "      <td>72.816467</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>7.428540</td>\n",
       "      <td>0.036712</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>8.088648</td>\n",
       "      <td>43505</td>\n",
       "      <td>51.650842</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>7.844354</td>\n",
       "      <td>0.035132</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>5.560999</td>\n",
       "      <td>50988</td>\n",
       "      <td>60.109946</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>7.228717</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>189</td>\n",
       "      <td>0.162933</td>\n",
       "      <td>4.520664</td>\n",
       "      <td>39909</td>\n",
       "      <td>47.571706</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>12.380196</td>\n",
       "      <td>0.019880</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>1.689564</td>\n",
       "      <td>10372</td>\n",
       "      <td>12.154478</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>11.972772</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>0.508337</td>\n",
       "      <td>3040</td>\n",
       "      <td>3.629573</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>12.213770</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>0.871460</td>\n",
       "      <td>7377</td>\n",
       "      <td>8.710197</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>11.917037</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>0.239431</td>\n",
       "      <td>2037</td>\n",
       "      <td>2.420483</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>12.207157</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>0.740500</td>\n",
       "      <td>7258</td>\n",
       "      <td>8.578487</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>5.011058</td>\n",
       "      <td>11.914259</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>216</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>0.202259</td>\n",
       "      <td>1987</td>\n",
       "      <td>2.356606</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_tokenized_sent  \\\n",
       "                            mean          mean                   mean   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k   5.011058      8.460269               0.032876   \n",
       "eojeol_composed_F_64k   5.011058      7.428540               0.036712   \n",
       "eojeol_pure_F_32k       5.011058      7.844354               0.035132   \n",
       "eojeol_pure_F_64k       5.011058      7.228717               0.037975   \n",
       "fixed_composed_F_32k    5.011058     12.380196               0.019880   \n",
       "fixed_composed_F_64k    5.011058     11.972772               0.020558   \n",
       "fixed_lexical_F_32k     5.011058     12.213770               0.020423   \n",
       "fixed_lexical_F_64k     5.011058     11.917037               0.020611   \n",
       "fixed_pure_F_32         5.011058     12.207157               0.020423   \n",
       "fixed_pure_F_64k        5.011058     11.914259               0.020611   \n",
       "\n",
       "                      OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                            sum                mean                  mean   \n",
       "tokenizer                                                                   \n",
       "eojeol_composed_F_32k       189            0.162933             10.523272   \n",
       "eojeol_composed_F_64k       189            0.162933              8.088648   \n",
       "eojeol_pure_F_32k           189            0.162933              5.560999   \n",
       "eojeol_pure_F_64k           189            0.162933              4.520664   \n",
       "fixed_composed_F_32k        216            0.181257              1.689564   \n",
       "fixed_composed_F_64k        216            0.181257              0.508337   \n",
       "fixed_lexical_F_32k         216            0.181257              0.871460   \n",
       "fixed_lexical_F_64k         216            0.181257              0.239431   \n",
       "fixed_pure_F_32             216            0.181257              0.740500   \n",
       "fixed_pure_F_64k            216            0.181257              0.202259   \n",
       "\n",
       "                      ##_count ##_per_source_sent         \n",
       "                           sum               mean   size  \n",
       "tokenizer                                                 \n",
       "eojeol_composed_F_32k    62072          72.816467  17996  \n",
       "eojeol_composed_F_64k    43505          51.650842  17996  \n",
       "eojeol_pure_F_32k        50988          60.109946  17996  \n",
       "eojeol_pure_F_64k        39909          47.571706  17996  \n",
       "fixed_composed_F_32k     10372          12.154478  17996  \n",
       "fixed_composed_F_64k      3040           3.629573  17996  \n",
       "fixed_lexical_F_32k       7377           8.710197  17996  \n",
       "fixed_lexical_F_64k       2037           2.420483  17996  \n",
       "fixed_pure_F_32           7258           8.578487  17996  \n",
       "fixed_pure_F_64k          1987           2.356606  17996  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean', 'size'], \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsmc_train = pd.read_csv('dataset_analysis/tokenized/nsmc_train.tsv', sep='\\t')\n",
    "nsmc_dev = pd.read_csv('dataset_analysis/tokenized/nsmc_dev.tsv', sep='\\t')\n",
    "nsmc_test = pd.read_csv('dataset_analysis/tokenized/nsmc_test.tsv', sep='\\t')\n",
    "nsmc = pd.concat([nsmc_train, nsmc_dev, nsmc_test])\n",
    "nsmc['tokenize_result'] = nsmc['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "nsmc['source_len'] = nsmc['source'].apply(lambda x: len(x.split()))\n",
    "nsmc['tokenized_len'] = nsmc['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "\n",
    "nsmc['OOV_per_tokenized_sent'] = nsmc['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "nsmc['OOV_count'] = nsmc['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "nsmc['OOV_per_source_sent'] = nsmc['OOV_count'] / nsmc['source_len'] * 100\n",
    "nsmc['##_per_tokenized_sent'] = nsmc['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "nsmc['##_count'] = nsmc['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "nsmc['##_per_source_sent'] = nsmc['##_count'] / nsmc['source_len'] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>17.727344</td>\n",
       "      <td>0.346157</td>\n",
       "      <td>8989</td>\n",
       "      <td>1.392632</td>\n",
       "      <td>14.242295</td>\n",
       "      <td>2027121</td>\n",
       "      <td>186.080295</td>\n",
       "      <td>199992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>15.669567</td>\n",
       "      <td>0.356818</td>\n",
       "      <td>8989</td>\n",
       "      <td>1.392632</td>\n",
       "      <td>12.307936</td>\n",
       "      <td>1615582</td>\n",
       "      <td>154.626038</td>\n",
       "      <td>199992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>16.681637</td>\n",
       "      <td>0.455792</td>\n",
       "      <td>9813</td>\n",
       "      <td>1.738694</td>\n",
       "      <td>8.840555</td>\n",
       "      <td>1817988</td>\n",
       "      <td>167.590889</td>\n",
       "      <td>199992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>15.262716</td>\n",
       "      <td>0.464452</td>\n",
       "      <td>9813</td>\n",
       "      <td>1.738694</td>\n",
       "      <td>7.755301</td>\n",
       "      <td>1534215</td>\n",
       "      <td>144.851687</td>\n",
       "      <td>199992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.892581</td>\n",
       "      <td>0.127833</td>\n",
       "      <td>10730</td>\n",
       "      <td>1.883802</td>\n",
       "      <td>2.627349</td>\n",
       "      <td>317082</td>\n",
       "      <td>32.287948</td>\n",
       "      <td>199992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.090634</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>10730</td>\n",
       "      <td>1.883802</td>\n",
       "      <td>1.325125</td>\n",
       "      <td>156699</td>\n",
       "      <td>17.575249</td>\n",
       "      <td>199992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.536411</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>10448</td>\n",
       "      <td>1.797754</td>\n",
       "      <td>1.480511</td>\n",
       "      <td>245851</td>\n",
       "      <td>26.269544</td>\n",
       "      <td>199992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.041962</td>\n",
       "      <td>0.126059</td>\n",
       "      <td>10448</td>\n",
       "      <td>1.797754</td>\n",
       "      <td>0.897720</td>\n",
       "      <td>146965</td>\n",
       "      <td>17.113886</td>\n",
       "      <td>199992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.511515</td>\n",
       "      <td>0.122186</td>\n",
       "      <td>10430</td>\n",
       "      <td>1.795306</td>\n",
       "      <td>1.351018</td>\n",
       "      <td>240872</td>\n",
       "      <td>25.793359</td>\n",
       "      <td>199992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>7.591344</td>\n",
       "      <td>21.033336</td>\n",
       "      <td>0.125915</td>\n",
       "      <td>10430</td>\n",
       "      <td>1.795306</td>\n",
       "      <td>0.826526</td>\n",
       "      <td>145240</td>\n",
       "      <td>16.919890</td>\n",
       "      <td>199992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_tokenized_sent  \\\n",
       "                            mean          mean                   mean   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k   7.591344     17.727344               0.346157   \n",
       "eojeol_composed_F_64k   7.591344     15.669567               0.356818   \n",
       "eojeol_pure_F_32k       7.591344     16.681637               0.455792   \n",
       "eojeol_pure_F_64k       7.591344     15.262716               0.464452   \n",
       "fixed_composed_F_32k    7.591344     21.892581               0.127833   \n",
       "fixed_composed_F_64k    7.591344     21.090634               0.133200   \n",
       "fixed_lexical_F_32k     7.591344     21.536411               0.122378   \n",
       "fixed_lexical_F_64k     7.591344     21.041962               0.126059   \n",
       "fixed_pure_F_32         7.591344     21.511515               0.122186   \n",
       "fixed_pure_F_64k        7.591344     21.033336               0.125915   \n",
       "\n",
       "                      OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                            sum                mean                  mean   \n",
       "tokenizer                                                                   \n",
       "eojeol_composed_F_32k      8989            1.392632             14.242295   \n",
       "eojeol_composed_F_64k      8989            1.392632             12.307936   \n",
       "eojeol_pure_F_32k          9813            1.738694              8.840555   \n",
       "eojeol_pure_F_64k          9813            1.738694              7.755301   \n",
       "fixed_composed_F_32k      10730            1.883802              2.627349   \n",
       "fixed_composed_F_64k      10730            1.883802              1.325125   \n",
       "fixed_lexical_F_32k       10448            1.797754              1.480511   \n",
       "fixed_lexical_F_64k       10448            1.797754              0.897720   \n",
       "fixed_pure_F_32           10430            1.795306              1.351018   \n",
       "fixed_pure_F_64k          10430            1.795306              0.826526   \n",
       "\n",
       "                      ##_count ##_per_source_sent          \n",
       "                           sum               mean    size  \n",
       "tokenizer                                                  \n",
       "eojeol_composed_F_32k  2027121         186.080295  199992  \n",
       "eojeol_composed_F_64k  1615582         154.626038  199992  \n",
       "eojeol_pure_F_32k      1817988         167.590889  199992  \n",
       "eojeol_pure_F_64k      1534215         144.851687  199992  \n",
       "fixed_composed_F_32k    317082          32.287948  199992  \n",
       "fixed_composed_F_64k    156699          17.575249  199992  \n",
       "fixed_lexical_F_32k     245851          26.269544  199992  \n",
       "fixed_lexical_F_64k     146965          17.113886  199992  \n",
       "fixed_pure_F_32         240872          25.793359  199992  \n",
       "fixed_pure_F_64k        145240          16.919890  199992  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean', 'size'], \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166967</th>\n",
       "      <td>재미없다ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ...</td>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>재미없 다 ㅠ ##ㅠ ㅠ ##ㅠ ㅠ ##ㅠ ㅠ ##ㅠ ㅠ ##ㅠ ㅠ ##ㅠ ㅠ #...</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.077803</td>\n",
       "      <td>79</td>\n",
       "      <td>7900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306632</th>\n",
       "      <td>\"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...</td>\n",
       "      <td>fixed_composed_F_64k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니다 . 어떻게 이런...</td>\n",
       "      <td>22</td>\n",
       "      <td>141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.845824</td>\n",
       "      <td>74</td>\n",
       "      <td>336.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306633</th>\n",
       "      <td>\"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...</td>\n",
       "      <td>fixed_lexical_F_64k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니다 ...</td>\n",
       "      <td>22</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.320826</td>\n",
       "      <td>71</td>\n",
       "      <td>322.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306634</th>\n",
       "      <td>\"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...</td>\n",
       "      <td>fixed_pure_F_64k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니...</td>\n",
       "      <td>22</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.701252</td>\n",
       "      <td>71</td>\n",
       "      <td>322.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306637</th>\n",
       "      <td>\"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...</td>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니다 . 어떻게 이런...</td>\n",
       "      <td>22</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.494845</td>\n",
       "      <td>80</td>\n",
       "      <td>363.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306638</th>\n",
       "      <td>\"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...</td>\n",
       "      <td>fixed_lexical_F_32k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니다 ...</td>\n",
       "      <td>22</td>\n",
       "      <td>142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.761468</td>\n",
       "      <td>75</td>\n",
       "      <td>340.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306639</th>\n",
       "      <td>\"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...</td>\n",
       "      <td>fixed_pure_F_32</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니...</td>\n",
       "      <td>22</td>\n",
       "      <td>142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.134851</td>\n",
       "      <td>75</td>\n",
       "      <td>340.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607922</th>\n",
       "      <td>\"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...</td>\n",
       "      <td>fixed_composed_F_64k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 이말년 만화 로 이 드라마 를 설명 하 ...</td>\n",
       "      <td>23</td>\n",
       "      <td>130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.182741</td>\n",
       "      <td>48</td>\n",
       "      <td>208.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607923</th>\n",
       "      <td>\"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...</td>\n",
       "      <td>fixed_lexical_F_64k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 이말년 만화 로 이 드...</td>\n",
       "      <td>23</td>\n",
       "      <td>130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.084034</td>\n",
       "      <td>48</td>\n",
       "      <td>208.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607924</th>\n",
       "      <td>\"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...</td>\n",
       "      <td>fixed_pure_F_64k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 이말년 만화 로 이 ᄃ...</td>\n",
       "      <td>23</td>\n",
       "      <td>130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.561753</td>\n",
       "      <td>48</td>\n",
       "      <td>208.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607927</th>\n",
       "      <td>\"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...</td>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 이 ##말 ##년 만화 로 이 드라마 를...</td>\n",
       "      <td>23</td>\n",
       "      <td>133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.655087</td>\n",
       "      <td>51</td>\n",
       "      <td>221.739130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607928</th>\n",
       "      <td>\"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...</td>\n",
       "      <td>fixed_lexical_F_32k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 이마 ##ᆯ ##년 만화 로...</td>\n",
       "      <td>23</td>\n",
       "      <td>132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.373444</td>\n",
       "      <td>50</td>\n",
       "      <td>217.391304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607929</th>\n",
       "      <td>\"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...</td>\n",
       "      <td>fixed_pure_F_32</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 이마 ##ᆯ ##년 만화 ᄅ...</td>\n",
       "      <td>23</td>\n",
       "      <td>132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.842520</td>\n",
       "      <td>50</td>\n",
       "      <td>217.391304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965557</th>\n",
       "      <td>\"\"\"범인 잡는 추리드라마였나!!....음 처음이내요 이런글 올려보는거...오죽하면...</td>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>\"\" ##\" ##\" ##\" ##\" ##\" 범인 잡 는 추리 드라마 이 었 나 ! ...</td>\n",
       "      <td>19</td>\n",
       "      <td>130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.564103</td>\n",
       "      <td>49</td>\n",
       "      <td>257.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70382</th>\n",
       "      <td>강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...</td>\n",
       "      <td>fixed_composed_F_64k</td>\n",
       "      <td>강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추...</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.533333</td>\n",
       "      <td>47</td>\n",
       "      <td>4700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70383</th>\n",
       "      <td>강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...</td>\n",
       "      <td>fixed_lexical_F_64k</td>\n",
       "      <td>강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ ᄀ...</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.108527</td>\n",
       "      <td>47</td>\n",
       "      <td>4700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70384</th>\n",
       "      <td>강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...</td>\n",
       "      <td>fixed_pure_F_64k</td>\n",
       "      <td>강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ ᄀ...</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.108527</td>\n",
       "      <td>47</td>\n",
       "      <td>4700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70387</th>\n",
       "      <td>강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...</td>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추...</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.533333</td>\n",
       "      <td>47</td>\n",
       "      <td>4700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70388</th>\n",
       "      <td>강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...</td>\n",
       "      <td>fixed_lexical_F_32k</td>\n",
       "      <td>강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ ᄀ...</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.108527</td>\n",
       "      <td>47</td>\n",
       "      <td>4700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70389</th>\n",
       "      <td>강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...</td>\n",
       "      <td>fixed_pure_F_32</td>\n",
       "      <td>강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ ᄀ...</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.108527</td>\n",
       "      <td>47</td>\n",
       "      <td>4700.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   source  \\\n",
       "166967  재미없다ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ...   \n",
       "306632  \"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...   \n",
       "306633  \"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...   \n",
       "306634  \"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...   \n",
       "306637  \"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...   \n",
       "306638  \"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...   \n",
       "306639  \"\"\"너무 충격적입니다. 어떻게 이런걸 영화라고 내놓나요? 보는내내 물음표만 가득했...   \n",
       "607922  \"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...   \n",
       "607923  \"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...   \n",
       "607924  \"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...   \n",
       "607927  \"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...   \n",
       "607928  \"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...   \n",
       "607929  \"\"\"이말년만화로 이 드라마를 설명할수 있다.드라마에 출연한 배우들 \"\"\"\"미친놈아...   \n",
       "965557  \"\"\"범인 잡는 추리드라마였나!!....음 처음이내요 이런글 올려보는거...오죽하면...   \n",
       "70382   강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...   \n",
       "70383   강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...   \n",
       "70384   강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...   \n",
       "70387   강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...   \n",
       "70388   강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...   \n",
       "70389   강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강추@강...   \n",
       "\n",
       "                   tokenizer  \\\n",
       "166967  fixed_composed_F_32k   \n",
       "306632  fixed_composed_F_64k   \n",
       "306633   fixed_lexical_F_64k   \n",
       "306634      fixed_pure_F_64k   \n",
       "306637  fixed_composed_F_32k   \n",
       "306638   fixed_lexical_F_32k   \n",
       "306639       fixed_pure_F_32   \n",
       "607922  fixed_composed_F_64k   \n",
       "607923   fixed_lexical_F_64k   \n",
       "607924      fixed_pure_F_64k   \n",
       "607927  fixed_composed_F_32k   \n",
       "607928   fixed_lexical_F_32k   \n",
       "607929       fixed_pure_F_32   \n",
       "965557  fixed_composed_F_32k   \n",
       "70382   fixed_composed_F_64k   \n",
       "70383    fixed_lexical_F_64k   \n",
       "70384       fixed_pure_F_64k   \n",
       "70387   fixed_composed_F_32k   \n",
       "70388    fixed_lexical_F_32k   \n",
       "70389        fixed_pure_F_32   \n",
       "\n",
       "                                          tokenize_result  source_len  \\\n",
       "166967   재미없 다 ㅠ ##ㅠ ㅠ ##ㅠ ㅠ ##ㅠ ㅠ ##ㅠ ㅠ ##ㅠ ㅠ ##ㅠ ㅠ #...           1   \n",
       "306632   \"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니다 . 어떻게 이런...          22   \n",
       "306633   \"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니다 ...          22   \n",
       "306634   \"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니...          22   \n",
       "306637   \"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니다 . 어떻게 이런...          22   \n",
       "306638   \"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니다 ...          22   \n",
       "306639   \"\" ##\" ##\" ##\" ##\" ##\" 너무 충격 적 이 ᄇ니...          22   \n",
       "607922   \"\" ##\" ##\" ##\" ##\" ##\" 이말년 만화 로 이 드라마 를 설명 하 ...          23   \n",
       "607923   \"\" ##\" ##\" ##\" ##\" ##\" 이말년 만화 로 이 드...          23   \n",
       "607924   \"\" ##\" ##\" ##\" ##\" ##\" 이말년 만화 로 이 ᄃ...          23   \n",
       "607927   \"\" ##\" ##\" ##\" ##\" ##\" 이 ##말 ##년 만화 로 이 드라마 를...          23   \n",
       "607928   \"\" ##\" ##\" ##\" ##\" ##\" 이마 ##ᆯ ##년 만화 로...          23   \n",
       "607929   \"\" ##\" ##\" ##\" ##\" ##\" 이마 ##ᆯ ##년 만화 ᄅ...          23   \n",
       "965557   \"\" ##\" ##\" ##\" ##\" ##\" 범인 잡 는 추리 드라마 이 었 나 ! ...          19   \n",
       "70382    강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추...           1   \n",
       "70383    강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ ᄀ...           1   \n",
       "70384    강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ ᄀ...           1   \n",
       "70387    강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추...           1   \n",
       "70388    강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ ᄀ...           1   \n",
       "70389    강 ##추 @ 강 ##추 @ 강 ##추 @ 강 ##추 @ ᄀ...           1   \n",
       "\n",
       "        tokenized_len  OOV_per_tokenized_sent  OOV_count  OOV_per_source_sent  \\\n",
       "166967            138                     0.0          0                  0.0   \n",
       "306632            141                     0.0          0                  0.0   \n",
       "306633            138                     0.0          0                  0.0   \n",
       "306634            138                     0.0          0                  0.0   \n",
       "306637            147                     0.0          0                  0.0   \n",
       "306638            142                     0.0          0                  0.0   \n",
       "306639            142                     0.0          0                  0.0   \n",
       "607922            130                     0.0          0                  0.0   \n",
       "607923            130                     0.0          0                  0.0   \n",
       "607924            130                     0.0          0                  0.0   \n",
       "607927            133                     0.0          0                  0.0   \n",
       "607928            132                     0.0          0                  0.0   \n",
       "607929            132                     0.0          0                  0.0   \n",
       "965557            130                     0.0          0                  0.0   \n",
       "70382             140                     0.0          0                  0.0   \n",
       "70383             140                     0.0          0                  0.0   \n",
       "70384             140                     0.0          0                  0.0   \n",
       "70387             140                     0.0          0                  0.0   \n",
       "70388             140                     0.0          0                  0.0   \n",
       "70389             140                     0.0          0                  0.0   \n",
       "\n",
       "        ##_per_tokenized_sent  ##_count  ##_per_source_sent  \n",
       "166967              18.077803        79         7900.000000  \n",
       "306632              15.845824        74          336.363636  \n",
       "306633              13.320826        71          322.727273  \n",
       "306634              12.701252        71          322.727273  \n",
       "306637              16.494845        80          363.636364  \n",
       "306638              13.761468        75          340.909091  \n",
       "306639              13.134851        75          340.909091  \n",
       "607922              12.182741        48          208.695652  \n",
       "607923              10.084034        48          208.695652  \n",
       "607924               9.561753        48          208.695652  \n",
       "607927              12.655087        51          221.739130  \n",
       "607928              10.373444        50          217.391304  \n",
       "607929               9.842520        50          217.391304  \n",
       "965557              12.564103        49          257.894737  \n",
       "70382               12.533333        47         4700.000000  \n",
       "70383                9.108527        47         4700.000000  \n",
       "70384                9.108527        47         4700.000000  \n",
       "70387               12.533333        47         4700.000000  \n",
       "70388                9.108527        47         4700.000000  \n",
       "70389                9.108527        47         4700.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc[nsmc['tokenized_len'] > 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>14.261856</td>\n",
       "      <td>31.760239</td>\n",
       "      <td>0.332959</td>\n",
       "      <td>52211</td>\n",
       "      <td>3.706431</td>\n",
       "      <td>13.449360</td>\n",
       "      <td>1856386</td>\n",
       "      <td>125.130972</td>\n",
       "      <td>106089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>14.261856</td>\n",
       "      <td>27.389041</td>\n",
       "      <td>0.378464</td>\n",
       "      <td>52211</td>\n",
       "      <td>3.706431</td>\n",
       "      <td>11.111521</td>\n",
       "      <td>1392650</td>\n",
       "      <td>94.064866</td>\n",
       "      <td>106089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>14.261856</td>\n",
       "      <td>29.551999</td>\n",
       "      <td>0.063106</td>\n",
       "      <td>30829</td>\n",
       "      <td>2.055070</td>\n",
       "      <td>9.001736</td>\n",
       "      <td>1622116</td>\n",
       "      <td>109.734915</td>\n",
       "      <td>106089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>14.261856</td>\n",
       "      <td>26.715211</td>\n",
       "      <td>0.068572</td>\n",
       "      <td>30829</td>\n",
       "      <td>2.055070</td>\n",
       "      <td>7.712552</td>\n",
       "      <td>1321164</td>\n",
       "      <td>89.547106</td>\n",
       "      <td>106089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>14.261856</td>\n",
       "      <td>35.557918</td>\n",
       "      <td>0.282816</td>\n",
       "      <td>52346</td>\n",
       "      <td>3.715277</td>\n",
       "      <td>6.317381</td>\n",
       "      <td>846546</td>\n",
       "      <td>57.006188</td>\n",
       "      <td>106089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>14.261856</td>\n",
       "      <td>32.424436</td>\n",
       "      <td>0.307464</td>\n",
       "      <td>52346</td>\n",
       "      <td>3.715277</td>\n",
       "      <td>4.134036</td>\n",
       "      <td>514118</td>\n",
       "      <td>34.930494</td>\n",
       "      <td>106089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>14.261856</td>\n",
       "      <td>34.030267</td>\n",
       "      <td>0.296267</td>\n",
       "      <td>52346</td>\n",
       "      <td>3.715277</td>\n",
       "      <td>4.323391</td>\n",
       "      <td>684479</td>\n",
       "      <td>46.160720</td>\n",
       "      <td>106089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>14.261856</td>\n",
       "      <td>31.962305</td>\n",
       "      <td>0.311179</td>\n",
       "      <td>52346</td>\n",
       "      <td>3.715277</td>\n",
       "      <td>3.099956</td>\n",
       "      <td>465091</td>\n",
       "      <td>31.684449</td>\n",
       "      <td>106089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>14.261856</td>\n",
       "      <td>34.159102</td>\n",
       "      <td>0.049922</td>\n",
       "      <td>30904</td>\n",
       "      <td>2.059366</td>\n",
       "      <td>4.037302</td>\n",
       "      <td>698147</td>\n",
       "      <td>47.287916</td>\n",
       "      <td>106089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>14.261856</td>\n",
       "      <td>32.073646</td>\n",
       "      <td>0.052145</td>\n",
       "      <td>30904</td>\n",
       "      <td>2.059366</td>\n",
       "      <td>2.902907</td>\n",
       "      <td>476903</td>\n",
       "      <td>32.651603</td>\n",
       "      <td>106089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_tokenized_sent  \\\n",
       "                            mean          mean                   mean   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k  14.261856     31.760239               0.332959   \n",
       "eojeol_composed_F_64k  14.261856     27.389041               0.378464   \n",
       "eojeol_pure_F_32k      14.261856     29.551999               0.063106   \n",
       "eojeol_pure_F_64k      14.261856     26.715211               0.068572   \n",
       "fixed_composed_F_32k   14.261856     35.557918               0.282816   \n",
       "fixed_composed_F_64k   14.261856     32.424436               0.307464   \n",
       "fixed_lexical_F_32k    14.261856     34.030267               0.296267   \n",
       "fixed_lexical_F_64k    14.261856     31.962305               0.311179   \n",
       "fixed_pure_F_32        14.261856     34.159102               0.049922   \n",
       "fixed_pure_F_64k       14.261856     32.073646               0.052145   \n",
       "\n",
       "                      OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                            sum                mean                  mean   \n",
       "tokenizer                                                                   \n",
       "eojeol_composed_F_32k     52211            3.706431             13.449360   \n",
       "eojeol_composed_F_64k     52211            3.706431             11.111521   \n",
       "eojeol_pure_F_32k         30829            2.055070              9.001736   \n",
       "eojeol_pure_F_64k         30829            2.055070              7.712552   \n",
       "fixed_composed_F_32k      52346            3.715277              6.317381   \n",
       "fixed_composed_F_64k      52346            3.715277              4.134036   \n",
       "fixed_lexical_F_32k       52346            3.715277              4.323391   \n",
       "fixed_lexical_F_64k       52346            3.715277              3.099956   \n",
       "fixed_pure_F_32           30904            2.059366              4.037302   \n",
       "fixed_pure_F_64k          30904            2.059366              2.902907   \n",
       "\n",
       "                      ##_count ##_per_source_sent          \n",
       "                           sum               mean    size  \n",
       "tokenizer                                                  \n",
       "eojeol_composed_F_32k  1856386         125.130972  106089  \n",
       "eojeol_composed_F_64k  1392650          94.064866  106089  \n",
       "eojeol_pure_F_32k      1622116         109.734915  106089  \n",
       "eojeol_pure_F_64k      1321164          89.547106  106089  \n",
       "fixed_composed_F_32k    846546          57.006188  106089  \n",
       "fixed_composed_F_64k    514118          34.930494  106089  \n",
       "fixed_lexical_F_32k     684479          46.160720  106089  \n",
       "fixed_lexical_F_64k     465091          31.684449  106089  \n",
       "fixed_pure_F_32         698147          47.287916  106089  \n",
       "fixed_pure_F_64k        476903          32.651603  106089  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws_train = pd.read_csv('dataset_analysis/tokenized/paws_train.tsv', sep='\\t')\n",
    "paws_dev = pd.read_csv('dataset_analysis/tokenized/paws_dev.tsv', sep='\\t')\n",
    "paws_test = pd.read_csv('dataset_analysis/tokenized/paws_test.tsv', sep='\\t')\n",
    "paws = pd.concat([paws_train, paws_dev, paws_test])\n",
    "\n",
    "paws['tokenize_result'] = paws['tokenize_result'].apply(str)\n",
    "paws['tokenize_result'] = paws['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "paws['source_len'] = paws['source'].apply(lambda x: len(x.split()))\n",
    "paws['tokenized_len'] = paws['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "paws['OOV_per_tokenized_sent'] = paws['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "paws['OOV_count'] = paws['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "paws['OOV_per_source_sent'] = paws['OOV_count'] / paws['source_len'] * 100\n",
    "paws['##_per_tokenized_sent'] = paws['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "paws['##_count'] = paws['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "paws['##_per_source_sent'] = paws['##_count'] / paws['source_len'] * 100\n",
    "\n",
    "paws.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean', 'size'], \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, tokenize_result, source_len, tokenized_len, OOV_per_tokenized_sent, OOV_count, OOV_per_source_sent, ##_per_tokenized_sent, ##_count, ##_per_source_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws[paws['tokenized_len'] > 128].groupby('tokenizer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>20.849289</td>\n",
       "      <td>0.270432</td>\n",
       "      <td>803</td>\n",
       "      <td>1.595408</td>\n",
       "      <td>15.587404</td>\n",
       "      <td>104113</td>\n",
       "      <td>169.389726</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>18.462890</td>\n",
       "      <td>0.292663</td>\n",
       "      <td>803</td>\n",
       "      <td>1.595408</td>\n",
       "      <td>13.781274</td>\n",
       "      <td>84146</td>\n",
       "      <td>139.137183</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>19.770288</td>\n",
       "      <td>0.276332</td>\n",
       "      <td>794</td>\n",
       "      <td>1.589262</td>\n",
       "      <td>9.805903</td>\n",
       "      <td>95085</td>\n",
       "      <td>155.923334</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>18.094419</td>\n",
       "      <td>0.296624</td>\n",
       "      <td>794</td>\n",
       "      <td>1.589262</td>\n",
       "      <td>8.724854</td>\n",
       "      <td>81063</td>\n",
       "      <td>134.379660</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>24.718179</td>\n",
       "      <td>0.210914</td>\n",
       "      <td>1031</td>\n",
       "      <td>1.951710</td>\n",
       "      <td>3.394665</td>\n",
       "      <td>18142</td>\n",
       "      <td>30.930292</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>23.516434</td>\n",
       "      <td>0.222857</td>\n",
       "      <td>1031</td>\n",
       "      <td>1.951710</td>\n",
       "      <td>1.607559</td>\n",
       "      <td>8087</td>\n",
       "      <td>14.188006</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>24.185132</td>\n",
       "      <td>0.215083</td>\n",
       "      <td>1028</td>\n",
       "      <td>1.941751</td>\n",
       "      <td>1.827213</td>\n",
       "      <td>13682</td>\n",
       "      <td>23.700267</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>23.412095</td>\n",
       "      <td>0.223857</td>\n",
       "      <td>1028</td>\n",
       "      <td>1.941751</td>\n",
       "      <td>0.997295</td>\n",
       "      <td>7214</td>\n",
       "      <td>12.934445</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>8.406</td>\n",
       "      <td>24.155372</td>\n",
       "      <td>0.214752</td>\n",
       "      <td>1025</td>\n",
       "      <td>1.935775</td>\n",
       "      <td>1.645531</td>\n",
       "      <td>13433</td>\n",
       "      <td>23.295903</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>8.406</td>\n",
       "      <td>23.398948</td>\n",
       "      <td>0.223262</td>\n",
       "      <td>1025</td>\n",
       "      <td>1.935775</td>\n",
       "      <td>0.898466</td>\n",
       "      <td>7104</td>\n",
       "      <td>12.729737</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_tokenized_sent  \\\n",
       "                            mean          mean                   mean   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k      8.406     20.849289               0.270432   \n",
       "eojeol_composed_F_64k      8.406     18.462890               0.292663   \n",
       "eojeol_pure_F_32k          8.406     19.770288               0.276332   \n",
       "eojeol_pure_F_64k          8.406     18.094419               0.296624   \n",
       "fixed_composed_F_32k       8.406     24.718179               0.210914   \n",
       "fixed_composed_F_64k       8.406     23.516434               0.222857   \n",
       "fixed_lexical_F_32k        8.406     24.185132               0.215083   \n",
       "fixed_lexical_F_64k        8.406     23.412095               0.223857   \n",
       "fixed_pure_F_32            8.406     24.155372               0.214752   \n",
       "fixed_pure_F_64k           8.406     23.398948               0.223262   \n",
       "\n",
       "                      OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                            sum                mean                  mean   \n",
       "tokenizer                                                                   \n",
       "eojeol_composed_F_32k       803            1.595408             15.587404   \n",
       "eojeol_composed_F_64k       803            1.595408             13.781274   \n",
       "eojeol_pure_F_32k           794            1.589262              9.805903   \n",
       "eojeol_pure_F_64k           794            1.589262              8.724854   \n",
       "fixed_composed_F_32k       1031            1.951710              3.394665   \n",
       "fixed_composed_F_64k       1031            1.951710              1.607559   \n",
       "fixed_lexical_F_32k        1028            1.941751              1.827213   \n",
       "fixed_lexical_F_64k        1028            1.941751              0.997295   \n",
       "fixed_pure_F_32            1025            1.935775              1.645531   \n",
       "fixed_pure_F_64k           1025            1.935775              0.898466   \n",
       "\n",
       "                      ##_count ##_per_source_sent        \n",
       "                           sum               mean  size  \n",
       "tokenizer                                                \n",
       "eojeol_composed_F_32k   104113         169.389726  8367  \n",
       "eojeol_composed_F_64k    84146         139.137183  8367  \n",
       "eojeol_pure_F_32k        95085         155.923334  8367  \n",
       "eojeol_pure_F_64k        81063         134.379660  8367  \n",
       "fixed_composed_F_32k     18142          30.930292  8367  \n",
       "fixed_composed_F_64k      8087          14.188006  8367  \n",
       "fixed_lexical_F_32k      13682          23.700267  8367  \n",
       "fixed_lexical_F_64k       7214          12.934445  8367  \n",
       "fixed_pure_F_32          13433          23.295903  8367  \n",
       "fixed_pure_F_64k          7104          12.729737  8367  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsd_train = pd.read_csv('dataset_analysis/tokenized/hsd_train.tsv', sep='\\t')\n",
    "hsd_dev = pd.read_csv('dataset_analysis/tokenized/hsd_dev.tsv', sep='\\t')\n",
    "hsd = pd.concat([hsd_train, hsd_dev])\n",
    "\n",
    "hsd['tokenize_result'] = hsd['tokenize_result'].apply(str)\n",
    "hsd['tokenize_result'] = hsd['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "hsd['source_len'] = hsd['source'].apply(lambda x: len(x.split()))\n",
    "hsd['tokenized_len'] = hsd['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "hsd['OOV_per_tokenized_sent'] = hsd['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "hsd['OOV_count'] = hsd['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "hsd['OOV_per_source_sent'] = hsd['OOV_count'] / hsd['source_len'] * 100\n",
    "hsd['##_per_tokenized_sent'] = hsd['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "hsd['##_count'] = hsd['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "hsd['##_per_source_sent'] = hsd['##_count'] / hsd['source_len'] * 100\n",
    "\n",
    "hsd.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean', 'size'], \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, tokenize_result, source_len, tokenized_len, OOV_per_tokenized_sent, OOV_count, OOV_per_source_sent, ##_per_tokenized_sent, ##_count, ##_per_source_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsd[hsd['tokenized_len'] > 128].groupby('tokenizer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>21.536333</td>\n",
       "      <td>0.240620</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>12.104248</td>\n",
       "      <td>121449</td>\n",
       "      <td>91.453097</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>18.498500</td>\n",
       "      <td>0.276842</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>9.435488</td>\n",
       "      <td>84995</td>\n",
       "      <td>64.573298</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>19.841750</td>\n",
       "      <td>0.259793</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>6.472703</td>\n",
       "      <td>101114</td>\n",
       "      <td>76.673673</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>17.945833</td>\n",
       "      <td>0.285277</td>\n",
       "      <td>2881</td>\n",
       "      <td>1.451239</td>\n",
       "      <td>5.216587</td>\n",
       "      <td>78363</td>\n",
       "      <td>59.605604</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>27.813750</td>\n",
       "      <td>0.185928</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>2.118451</td>\n",
       "      <td>21074</td>\n",
       "      <td>14.808530</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>26.661667</td>\n",
       "      <td>0.194508</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>0.742135</td>\n",
       "      <td>7249</td>\n",
       "      <td>5.042086</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>27.215167</td>\n",
       "      <td>0.190371</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>0.945381</td>\n",
       "      <td>13891</td>\n",
       "      <td>9.798267</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>26.539333</td>\n",
       "      <td>0.195732</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>0.395972</td>\n",
       "      <td>5781</td>\n",
       "      <td>4.088706</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>27.185583</td>\n",
       "      <td>0.190566</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>0.829199</td>\n",
       "      <td>13536</td>\n",
       "      <td>9.538848</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>11.415583</td>\n",
       "      <td>26.534000</td>\n",
       "      <td>0.195764</td>\n",
       "      <td>3346</td>\n",
       "      <td>1.665835</td>\n",
       "      <td>0.352165</td>\n",
       "      <td>5717</td>\n",
       "      <td>4.038697</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_tokenized_sent  \\\n",
       "                            mean          mean                   mean   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k  11.415583     21.536333               0.240620   \n",
       "eojeol_composed_F_64k  11.415583     18.498500               0.276842   \n",
       "eojeol_pure_F_32k      11.415583     19.841750               0.259793   \n",
       "eojeol_pure_F_64k      11.415583     17.945833               0.285277   \n",
       "fixed_composed_F_32k   11.415583     27.813750               0.185928   \n",
       "fixed_composed_F_64k   11.415583     26.661667               0.194508   \n",
       "fixed_lexical_F_32k    11.415583     27.215167               0.190371   \n",
       "fixed_lexical_F_64k    11.415583     26.539333               0.195732   \n",
       "fixed_pure_F_32        11.415583     27.185583               0.190566   \n",
       "fixed_pure_F_64k       11.415583     26.534000               0.195764   \n",
       "\n",
       "                      OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                            sum                mean                  mean   \n",
       "tokenizer                                                                   \n",
       "eojeol_composed_F_32k      2881            1.451239             12.104248   \n",
       "eojeol_composed_F_64k      2881            1.451239              9.435488   \n",
       "eojeol_pure_F_32k          2881            1.451239              6.472703   \n",
       "eojeol_pure_F_64k          2881            1.451239              5.216587   \n",
       "fixed_composed_F_32k       3346            1.665835              2.118451   \n",
       "fixed_composed_F_64k       3346            1.665835              0.742135   \n",
       "fixed_lexical_F_32k        3346            1.665835              0.945381   \n",
       "fixed_lexical_F_64k        3346            1.665835              0.395972   \n",
       "fixed_pure_F_32            3346            1.665835              0.829199   \n",
       "fixed_pure_F_64k           3346            1.665835              0.352165   \n",
       "\n",
       "                      ##_count ##_per_source_sent         \n",
       "                           sum               mean   size  \n",
       "tokenizer                                                 \n",
       "eojeol_composed_F_32k   121449          91.453097  12000  \n",
       "eojeol_composed_F_64k    84995          64.573298  12000  \n",
       "eojeol_pure_F_32k       101114          76.673673  12000  \n",
       "eojeol_pure_F_64k        78363          59.605604  12000  \n",
       "fixed_composed_F_32k     21074          14.808530  12000  \n",
       "fixed_composed_F_64k      7249           5.042086  12000  \n",
       "fixed_lexical_F_32k      13891           9.798267  12000  \n",
       "fixed_lexical_F_64k       5781           4.088706  12000  \n",
       "fixed_pure_F_32          13536           9.538848  12000  \n",
       "fixed_pure_F_64k          5717           4.038697  12000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_train = pd.read_csv('dataset_analysis/tokenized/dp_train.tsv', sep='\\t')\n",
    "dp_dev = pd.read_csv('dataset_analysis/tokenized/dp_dev.tsv', sep='\\t')\n",
    "dp = pd.concat([dp_train, dp_dev])\n",
    "\n",
    "dp['tokenize_result'] = dp['tokenize_result'].apply(str)\n",
    "dp['tokenize_result'] = dp['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "dp['source_len'] = dp['source'].apply(lambda x: len(x.split()))\n",
    "dp['tokenized_len'] = dp['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "dp['OOV_per_tokenized_sent'] = dp['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "dp['OOV_count'] = dp['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "dp['OOV_per_source_sent'] = dp['OOV_count'] / dp['source_len'] * 100\n",
    "dp['##_per_tokenized_sent'] = dp['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "dp['##_count'] = dp['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "dp['##_per_source_sent'] = dp['##_count'] / dp['source_len'] * 100\n",
    "\n",
    "dp.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean', 'size'], \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, tokenize_result, source_len, tokenized_len, OOV_per_tokenized_sent, OOV_count, OOV_per_source_sent, ##_per_tokenized_sent, ##_count, ##_per_source_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp[dp['tokenized_len'] > 128].groupby('tokenizer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>15.332113</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>151</td>\n",
       "      <td>0.02578</td>\n",
       "      <td>11.634948</td>\n",
       "      <td>395046</td>\n",
       "      <td>90.252580</td>\n",
       "      <td>55996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>13.176370</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>151</td>\n",
       "      <td>0.02578</td>\n",
       "      <td>8.980216</td>\n",
       "      <td>274333</td>\n",
       "      <td>63.527354</td>\n",
       "      <td>55996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>14.121044</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>151</td>\n",
       "      <td>0.02578</td>\n",
       "      <td>6.063119</td>\n",
       "      <td>327231</td>\n",
       "      <td>75.100799</td>\n",
       "      <td>55996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>12.774127</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>151</td>\n",
       "      <td>0.02578</td>\n",
       "      <td>4.866531</td>\n",
       "      <td>251809</td>\n",
       "      <td>58.397642</td>\n",
       "      <td>55996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>20.357936</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>2.181656</td>\n",
       "      <td>73156</td>\n",
       "      <td>16.203469</td>\n",
       "      <td>55996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.523573</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>0.824155</td>\n",
       "      <td>26435</td>\n",
       "      <td>5.897201</td>\n",
       "      <td>55996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.931638</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>1.003572</td>\n",
       "      <td>49285</td>\n",
       "      <td>11.089676</td>\n",
       "      <td>55996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.424727</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>0.431059</td>\n",
       "      <td>20900</td>\n",
       "      <td>4.699381</td>\n",
       "      <td>55996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.909172</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>0.877533</td>\n",
       "      <td>48027</td>\n",
       "      <td>10.805589</td>\n",
       "      <td>55996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>8.277216</td>\n",
       "      <td>19.422119</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>196</td>\n",
       "      <td>0.03105</td>\n",
       "      <td>0.383633</td>\n",
       "      <td>20754</td>\n",
       "      <td>4.670615</td>\n",
       "      <td>55996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_tokenized_sent  \\\n",
       "                            mean          mean                   mean   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k   8.277216     15.332113               0.003902   \n",
       "eojeol_composed_F_64k   8.277216     13.176370               0.004501   \n",
       "eojeol_pure_F_32k       8.277216     14.121044               0.004100   \n",
       "eojeol_pure_F_64k       8.277216     12.774127               0.004600   \n",
       "fixed_composed_F_32k    8.277216     20.357936               0.003230   \n",
       "fixed_composed_F_64k    8.277216     19.523573               0.003463   \n",
       "fixed_lexical_F_32k     8.277216     19.931638               0.003309   \n",
       "fixed_lexical_F_64k     8.277216     19.424727               0.003484   \n",
       "fixed_pure_F_32         8.277216     19.909172               0.003309   \n",
       "fixed_pure_F_64k        8.277216     19.422119               0.003484   \n",
       "\n",
       "                      OOV_count OOV_per_source_sent ##_per_tokenized_sent  \\\n",
       "                            sum                mean                  mean   \n",
       "tokenizer                                                                   \n",
       "eojeol_composed_F_32k       151             0.02578             11.634948   \n",
       "eojeol_composed_F_64k       151             0.02578              8.980216   \n",
       "eojeol_pure_F_32k           151             0.02578              6.063119   \n",
       "eojeol_pure_F_64k           151             0.02578              4.866531   \n",
       "fixed_composed_F_32k        196             0.03105              2.181656   \n",
       "fixed_composed_F_64k        196             0.03105              0.824155   \n",
       "fixed_lexical_F_32k         196             0.03105              1.003572   \n",
       "fixed_lexical_F_64k         196             0.03105              0.431059   \n",
       "fixed_pure_F_32             196             0.03105              0.877533   \n",
       "fixed_pure_F_64k            196             0.03105              0.383633   \n",
       "\n",
       "                      ##_count ##_per_source_sent         \n",
       "                           sum               mean   size  \n",
       "tokenizer                                                 \n",
       "eojeol_composed_F_32k   395046          90.252580  55996  \n",
       "eojeol_composed_F_64k   274333          63.527354  55996  \n",
       "eojeol_pure_F_32k       327231          75.100799  55996  \n",
       "eojeol_pure_F_64k       251809          58.397642  55996  \n",
       "fixed_composed_F_32k     73156          16.203469  55996  \n",
       "fixed_composed_F_64k     26435           5.897201  55996  \n",
       "fixed_lexical_F_32k      49285          11.089676  55996  \n",
       "fixed_lexical_F_64k      20900           4.699381  55996  \n",
       "fixed_pure_F_32          48027          10.805589  55996  \n",
       "fixed_pure_F_64k         20754           4.670615  55996  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_train = pd.read_csv('dataset_analysis/tokenized/nli_train.tsv', sep='\\t')\n",
    "nli_dev = pd.read_csv('dataset_analysis/tokenized/nli_dev.tsv', sep='\\t')\n",
    "nli = pd.concat([nli_train, nli_dev])\n",
    "\n",
    "nli['tokenize_result'] = nli['tokenize_result'].apply(str)\n",
    "nli['tokenize_result'] = nli['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "nli['source_len'] = nli['source'].apply(lambda x: len(x.split()))\n",
    "nli['tokenized_len'] = nli['tokenize_result'].apply(lambda x: len(x.split()))\n",
    "nli['OOV_per_tokenized_sent'] = nli['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "nli['OOV_count'] = nli['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "nli['OOV_per_source_sent'] = nli['OOV_count'] / nli['source_len'] * 100\n",
    "nli['##_per_tokenized_sent'] = nli['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "nli['##_count'] = nli['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "nli['##_per_source_sent'] = nli['##_count'] / nli['source_len'] * 100\n",
    "\n",
    "nli.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_tokenized_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    'OOV_per_source_sent': ['mean'],\n",
    "    '##_per_tokenized_sent': ['mean'],\n",
    "    '##_count': ['sum'],\n",
    "    '##_per_source_sent': ['mean', 'size'], \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_tokenized_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>OOV_per_source_sent</th>\n",
       "      <th>##_per_tokenized_sent</th>\n",
       "      <th>##_count</th>\n",
       "      <th>##_per_source_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, tokenize_result, source_len, tokenized_len, OOV_per_tokenized_sent, OOV_count, OOV_per_source_sent, ##_per_tokenized_sent, ##_count, ##_per_source_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli[nli['tokenized_len'] > 128].groupby('tokenizer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17996.0 199992.0 106132.0 8367.0 12000.0 55996.0\n"
     ]
    }
   ],
   "source": [
    "# size\n",
    "print(len(cola)/10,\n",
    "len(nsmc)/10,\n",
    "len(paws)/10,\n",
    "len(hsd)/10,\n",
    "len(dp)/10,\n",
    "len(nli)/10\n",
    ")\n",
    "\n",
    "# save results\n",
    "cola.to_csv('dataset_analysis/results/cola_result.tsv', sep='\\t', index=False)\n",
    "nsmc.to_csv('dataset_analysis/results/nsmc_result.tsv', sep='\\t', index=False)\n",
    "paws.to_csv('dataset_analysis/results/paws_result.tsv', sep='\\t', index=False)\n",
    "hsd.to_csv('dataset_analysis/results/hsd_result.tsv', sep='\\t', index=False)\n",
    "dp.to_csv('dataset_analysis/results/dp_result.tsv', sep='\\t', index=False)\n",
    "nli.to_csv('dataset_analysis/results/nli_result.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "453e7ba0431a1f965429b96a03a1d5546248e001661d541f6bea069f7abcbd9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
