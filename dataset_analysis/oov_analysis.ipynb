{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import unicodedata\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "sys.path.append(\"..\")\n",
    "from scripts._mecab import Mecab\n",
    "from typing import Dict, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import (\n",
    "    # CharTokenizer,\n",
    "    # JamoTokenizer,\n",
    "    MeCabSentencePieceTokenizer_orig,\n",
    "    MeCabSentencePieceTokenizer_fixed,\n",
    "    MeCabSentencePieceTokenizer,\n",
    "    MeCabWordPieceTokenizer,\n",
    "    # MeCabTokenizer,\n",
    "    MeCabTokenizer_orig,\n",
    "    MeCabTokenizer_fixed,\n",
    "    MeCabTokenizer_all,\n",
    "    # MeCabSentencePieceTokenizer_kortok,\n",
    "    # MeCabTokenizer_kortok,\n",
    "    SentencePieceTokenizer,\n",
    "    WordPieceTokenizer,\n",
    "    Vocab,\n",
    "    # WordTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 토큰화 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_name: str, resource_dir: str, token_type, tokenizer_type: str , decomposition_type: str, space_symbol: str, dummy_letter: str, nfd: bool, grammatical_symbol: list = [\"\", \"\"], skip_special_tokens: bool = False, lexical_grammatical: bool = False):   # for LG\n",
    "    tokenizer_dir = os.path.join(resource_dir, tokenizer_name)\n",
    "\n",
    "    if tokenizer_name.startswith(\"sp-\"):\n",
    "        tokenizer = SentencePieceTokenizer(os.path.join(tokenizer_dir, \"tok.model\"))\n",
    "\n",
    "    elif tokenizer_name.startswith(\"mecab_\"):\n",
    "\n",
    "        sp = SentencePieceTokenizer(os.path.join(tokenizer_dir, \"tok.model\"))\n",
    "\n",
    "        if \"orig\" in tokenizer_name:\n",
    "            mecab = MeCabTokenizer_orig(tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter)\n",
    "            tokenizer = MeCabSentencePieceTokenizer_orig(mecab, sp, use_fixed=False) # mecab_sp_orig.py\n",
    "\n",
    "        elif \"fixed\" in tokenizer_name:\n",
    "            mecab = MeCabTokenizer_fixed(tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter)\n",
    "            tokenizer = MeCabSentencePieceTokenizer_fixed(mecab, sp, use_fixed=True) # mecab_fixed.py\n",
    "\n",
    "\n",
    "    # elif tokenizer_name.startswith(\"eojeol\") or tokenizer_name.startswith(\"morpheme\"):\n",
    "    elif tokenizer_name.startswith(\"eojeol\") or tokenizer_name.startswith(\"morpheme\") or tokenizer_name.startswith(\"LG\"):   # LG도 처리할 수 있도록\n",
    "        wp = WordPieceTokenizer(os.path.join(tokenizer_dir, \"bert_tokenizer.json\"), skip_special_tokens=False)\n",
    "        # mecab = MeCabTokenizer_all(token_type=token_type, tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter, nfd=nfd, grammatical_symbol=grammatical_symbol)\n",
    "        mecab = MeCabTokenizer_all(token_type=token_type, tokenizer_type=tokenizer_type, decomposition_type=decomposition_type, space_symbol=space_symbol, dummy_letter=dummy_letter, nfd=nfd, grammatical_symbol=grammatical_symbol, lexical_grammatical=lexical_grammatical)   # for LG\n",
    "        tokenizer = MeCabWordPieceTokenizer(mecab=mecab, wp=wp) # mecab_wp.py\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Wrong tokenizer name.\")\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_result(tokenizer, string, nfd: bool = True):\n",
    "    # if nfd == True:\n",
    "    #     string = str_to_nfd(string)\n",
    "\n",
    "    tokenized = tokenizer.tokenize(string)\n",
    "    # print(\" \".join(tokenized))\n",
    "    \n",
    "    return \" \".join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 64k\n",
    "tokenizer_eojeol_composed_F_64k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_composed_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"./resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_eojeol_decomposed_pure_F_64k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"./resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_composed_F_64k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_composed_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"./resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_lexical_F_64k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_lexical_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"./resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_lexical\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_pure_F_64k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-64k\",\n",
    "resource_dir = \"./resources/v6_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "\n",
    "# 32k\n",
    "tokenizer_eojeol_composed_F_32k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_composed_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"./resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_eojeol_decomposed_pure_F_32k = get_tokenizer(tokenizer_name = \"eojeol_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"./resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"eojeol\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_composed_F_32k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_composed_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"./resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"composed\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_lexical_F_32k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_lexical_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"./resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_lexical\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n",
    "tokenizer_fixed_decomposed_pure_F_32k = get_tokenizer(tokenizer_name = \"morpheme_mecab_fixed_decomposed_pure_grammatical_symbol_F_wp-32k\",\n",
    "resource_dir = \"./resources/v7_without_dummy_letter_grammatical_symbol_F\",\n",
    "tokenizer_type = \"mecab_fixed\",\n",
    "token_type= \"morpheme\",\n",
    "decomposition_type= \"decomposed_pure\",\n",
    "space_symbol= \"\",\n",
    "dummy_letter= \"\",\n",
    "nfd= True,\n",
    "grammatical_symbol= [\"\",\"\"],\n",
    "lexical_grammatical= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tokenizations(string):\n",
    "    # grammatical symbol F\n",
    "    eojeol_composed_F_64k = string + '\\t' + 'eojeol_composed_F_64k' + '\\t' + get_tokenized_result(tokenizer_eojeol_composed_F_64k, string)\n",
    "    eojeol_pure_F_64k = string + '\\t' + 'eojeol_pure_F_64k' + '\\t' + get_tokenized_result(tokenizer_eojeol_decomposed_pure_F_64k, string)\n",
    "    fixed_composed_F_64k = string + '\\t' + 'fixed_composed_F_64k' + '\\t' + get_tokenized_result(tokenizer_fixed_composed_F_64k, string)\n",
    "    fixed_lexical_F_64k = string + '\\t' + 'fixed_lexical_F_64k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_lexical_F_64k, string)\n",
    "    fixed_pure_F_64k = string + '\\t' + 'fixed_pure_F_64k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_pure_F_64k, string)\n",
    "\n",
    "    eojeol_composed_F_32k = string + '\\t' + 'eojeol_composed_F_32k' + '\\t' + get_tokenized_result(tokenizer_eojeol_composed_F_32k, string)\n",
    "    eojeol_pure_F_32k = string + '\\t' + 'eojeol_pure_F_32k' + '\\t' + get_tokenized_result(tokenizer_eojeol_decomposed_pure_F_32k, string)\n",
    "    fixed_composed_F_32k = string + '\\t' + 'fixed_composed_F_32k' + '\\t' + get_tokenized_result(tokenizer_fixed_composed_F_32k, string)\n",
    "    fixed_lexical_F_32k = string + '\\t' + 'fixed_lexical_F_32k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_lexical_F_32k, string)\n",
    "    fixed_pure_F_32k = string + '\\t' + 'fixed_pure_F_32k' + '\\t' + get_tokenized_result(tokenizer_fixed_decomposed_pure_F_32k, string)\n",
    "    \n",
    "    return \"\\n\".join(['\\n'+eojeol_composed_F_64k , eojeol_pure_F_64k , fixed_composed_F_64k, fixed_lexical_F_64k, fixed_pure_F_64k,\n",
    "    eojeol_composed_F_32k , eojeol_pure_F_32k , fixed_composed_F_32k, fixed_lexical_F_32k, fixed_pure_F_32k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_composed_F_64k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_pure_F_64k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_composed_F_64k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] [UNK] [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_lexical_F_64k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_pure_F_64k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_composed_F_32k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\teojeol_pure_F_32k\\t[CLS] 갑자기 [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_composed_F_32k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] [UNK] [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_lexical_F_32k\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]\\n갑자기 더워彭肽꿿뜛땭뜎حمق\\tfixed_pure_F_32\\t[CLS] 갑자기 덥 어 [UNK] [UNK] 꿰 ##ᆶ ##ᄄ ##ᅲ ##ᆶ ##ᄄ ##ᅣ ##ᆬ ##ᄄ ##ᅲ ##ᆩ [UNK] [SEP]'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_tokenizations('갑자기 더워彭肽꿿뜛땭뜎حمق')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 저장\n",
    "def analysis(task, corpus_list, corpus_name_list, sent2=True):\n",
    "    for corpus, corpus_name in zip(corpus_list, corpus_name_list):\n",
    "        with open('dataset_analysis/' + task+'_'+corpus_name+'.tsv', 'w', encoding='utf-8') as f:\n",
    "            f.write('source'+'\\t'+'tokenizer'+'\\t'+'tokenize_result')\n",
    "            if sent2:\n",
    "                for sent1, sent2 in zip(corpus[0], corpus[1]):\n",
    "                    f.write(show_tokenizations(string=sent1))\n",
    "                    f.write(show_tokenizations(string=sent2))\n",
    "            else:\n",
    "                for sent1 in (corpus):\n",
    "                    f.write(show_tokenizations(string=sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cola\n",
    "def load_data(file_path: str) -> Tuple[List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentences: List[str] = []\n",
    "\n",
    "    # for test set\n",
    "    if file_path == \"./dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_test_with_answer.tsv\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f.readlines()[1:]):\n",
    "                splitted = line.strip().split(\"\\t\")\n",
    "                sentences.append(splitted[2])\n",
    "\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f.readlines()[1:]):\n",
    "                splitted = line.strip().split(\"\\t\")\n",
    "                sentences.append(splitted[3])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "task = 'cola'\n",
    "train = load_data('./dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_train.tsv')\n",
    "dev = load_data('./dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_dev.tsv')\n",
    "test = load_data('./dataset/nlu_tasks/cola/NIKL_CoLA_in_domain_test_with_answer.tsv')\n",
    "\n",
    "corpus_list = [train, dev, test]\n",
    "corpus_name_list = ['train', 'dev', 'test']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsmc\n",
    "\n",
    "def load_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentences: List[str] = []\n",
    "    labels: List[int] = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if len(splitted) != 2:\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            sentences.append(splitted[0])\n",
    "            \n",
    "#             labels.append(label_to_index[splitted[1]])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "task = 'nsmc'\n",
    "train = load_data('./dataset/nlu_tasks/nsmc/ratings_train.tsv')\n",
    "dev = load_data('./dataset/nlu_tasks/nsmc/ratings_dev.tsv')\n",
    "test = load_data('./dataset/nlu_tasks/nsmc/ratings_test.tsv')\n",
    "\n",
    "corpus_list = [train, dev, test]\n",
    "corpus_name_list = ['train', 'dev', 'test']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hsd\n",
    "\n",
    "def load_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence_a\n",
    "    2. sentence_b\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentence = []\n",
    "    # sentence_bs = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if len(splitted) != 4:\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "\n",
    "            sentence.append(splitted[0])\n",
    "                # sentence_bs.append(splitted[1])\n",
    "            # sentence_as.append(splitted[0])\n",
    "            # sentence_bs.append(splitted[1])\n",
    "\n",
    "#             labels.append(label_to_index[splitted[3]])\n",
    "\n",
    "    return sentence\n",
    "\n",
    "task = 'hsd'\n",
    "train = load_data('./dataset/nlu_tasks/hsd/train.tsv')\n",
    "dev = load_data('./dataset/nlu_tasks/hsd/dev.tsv')\n",
    "\n",
    "corpus_list = [train, dev]\n",
    "corpus_name_list = ['train', 'dev']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paws\n",
    "def load_data(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence_a\n",
    "    2. sentence_b\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentence_as = []\n",
    "    sentence_bs = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if len(splitted) != 4:\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            if splitted[1] == \"\" or splitted[2] == \"\":\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            # 문장이 \"NS\"로만 표기된 라인 제외\n",
    "            if splitted[1] == \"NS\" or splitted[2] == \"NS\":\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            sentence_as.append(splitted[1])\n",
    "            sentence_bs.append(splitted[2])\n",
    "        \n",
    "    return sentence_as, sentence_bs\n",
    "\n",
    "task = 'paws'\n",
    "train = load_data('./dataset/nlu_tasks/paws/translated_train.tsv')\n",
    "dev = load_data('./dataset/nlu_tasks/paws/dev_2k.tsv')\n",
    "test = load_data('./dataset/nlu_tasks/paws/test_2k.tsv')\n",
    "\n",
    "corpus_list = [train, dev, test]\n",
    "corpus_name_list = ['train', 'dev', 'test']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bongseok/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n",
      "/tmp/ipykernel_22291/2797297455.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_train['text'] = df2_train['text'].apply(lambda x: re.sub('## klue-dp-v1_train_.*\\t', '', x))\n",
      "/tmp/ipykernel_22291/2797297455.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_dev['text'] = df2_dev['text'].apply(lambda x: re.sub('## klue-dp-v1_dev_.*\\t', '', x))\n"
     ]
    }
   ],
   "source": [
    "# KLUE-dp\n",
    "dp_train = pd.read_csv('./KLUE-baseline/data/klue_benchmark/klue-dp-v1.1/klue-dp-v1.1_train.tsv',sep='delimiter', header=None)\n",
    "dp_dev = pd.read_csv('./KLUE-baseline/data/klue_benchmark/klue-dp-v1.1/klue-dp-v1.1_dev.tsv', sep='delimiter', header=None)\n",
    "dp_train.columns = ['text']\n",
    "dp_dev.columns = ['text']\n",
    "df2_train = dp_train['text'].str.contains('## klue-dp')\n",
    "df2_dev = dp_dev['text'].str.contains('## klue-dp')\n",
    "df2_train = dp_train[df2_train]\n",
    "df2_dev = dp_dev[df2_dev]\n",
    "df2_train['text'] = df2_train['text'].apply(lambda x: re.sub('## klue-dp-v1_train_.*\\t', '', x))\n",
    "df2_dev['text'] = df2_dev['text'].apply(lambda x: re.sub('## klue-dp-v1_dev_.*\\t', '', x))\n",
    "df2_train.to_csv('dp_orig_train.tsv', index=False, sep='\\t')\n",
    "df2_dev.to_csv('dp_orig_dev.tsv', index=False, sep='\\t')\n",
    "\n",
    "def load_data(file_path: str) -> Tuple[List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentenced\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentences: List[str] = []\n",
    "    #labels: List[int] = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip()\n",
    "            # if len(splitted) != 2:\n",
    "            #     #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "            #     continue\n",
    "            sentences.append(splitted)\n",
    "     #       labels.append(label_to_index[splitted[1]])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "task = 'dp'\n",
    "train = load_data('dp_orig_train.tsv')\n",
    "dev = load_data('dp_orig_dev.tsv')\n",
    "\n",
    "corpus_list = [train, dev]\n",
    "corpus_name_list = ['train', 'dev']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLUE-nli\n",
    "# json to tsv\n",
    "import json\n",
    "import pandas as pd\n",
    "with open('./KLUE-baseline/data/klue_benchmark/klue-nli-v1.1/klue-nli-v1.1_train.json', 'r') as f:\n",
    "    nli_train = json.load(f)\n",
    "with open('./KLUE-baseline/data/klue_benchmark/klue-nli-v1.1/klue-nli-v1.1_dev.json') as f2:\n",
    "    nli_dev = json.load(f2)\n",
    "\n",
    "\n",
    "nli_train = pd.DataFrame(nli_train)\n",
    "nli_train = nli_train[['premise', 'hypothesis']]\n",
    "nli_train.to_csv('nli_orig_train.tsv', index=False, sep='\\t')\n",
    "nli_dev = pd.DataFrame(nli_dev)\n",
    "nli_dev = nli_dev[['premise', 'hypothesis']]\n",
    "nli_dev.to_csv('nli_orig_dev.tsv', index=False, sep='\\t')\n",
    "\n",
    "def load_data(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    file_path에 존재하는 tsv를 읽어서 bert_data.InputIds 형태로 변경해주는 함수입니다.\n",
    "    각각의 row를 bert input으로 바꾸어주기 위한 함수입니다.\n",
    "    각 row는 아래처럼 구성되어야 합니다.\n",
    "    1. sentence_a\n",
    "    2. sentence_b\n",
    "    3. label\n",
    "    \"\"\"\n",
    "    sentence_as = []\n",
    "    sentence_bs = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            splitted = line.strip().split(\"\\t\")\n",
    "            if splitted[0] == \"\" or splitted[1] == \"\":\n",
    "                #print(f\"[ERROR] {repr(line)}, line {i}\")\n",
    "                continue\n",
    "            sentence_as.append(splitted[0])\n",
    "            sentence_bs.append(splitted[1])\n",
    "        \n",
    "    return sentence_as, sentence_bs\n",
    "\n",
    "task = 'nli'\n",
    "train = load_data('nli_orig_train.tsv')\n",
    "dev = load_data('nli_orig_dev.tsv')\n",
    "\n",
    "corpus_list = [train, dev]\n",
    "corpus_name_list = ['train', 'dev']\n",
    "\n",
    "analysis(task, corpus_list, corpus_name_list, sent2=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OOV rate, ## rate 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [UNK]의 개수 / 문장의 길이 * 100\n",
    "from typing import List\n",
    "\n",
    "# 문장당 oov rate (OR)\n",
    "def getOOVRatePerSentence(sentence):\n",
    "    # [CLS], [SEP] 제거\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    OOV_rate = sentence.count('[UNK]') / len(sentence) * 100   \n",
    "    \n",
    "    return OOV_rate\n",
    "\n",
    "# count of all OOV tokens (OC)\n",
    "def getCountofAllOOV(sentence):\n",
    "    cnt = 0\n",
    "    cnt += len(re.findall('[UNK]', sentence))\n",
    "    \n",
    "    return cnt\n",
    "\n",
    "# [CLS, SEP] 제거\n",
    "def removeCS(sentence):\n",
    "    sentence = sentence.replace('[CLS]', '')\n",
    "    sentence = sentence.replace('[SEP]', '')\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# [UNK]수 /전체 토큰 수\n",
    "def getOOVdividedbyAllTokens(corpus):\n",
    "    corpus['OC'] = corpus['sentence'].apply(lambda x: getCountofAllOOV(x))\n",
    "    corpus['sentence'].apply(lambda x: removeCS(x))\n",
    "    corpus['token_count'] = corpus['sentence'].apply(lambda x: len(x))\n",
    "    OOV_count = corpus['OC'].sum()\n",
    "    token_count = corpus['token_count'].sum()\n",
    "    \n",
    "    return OOV_count, token_count, OOV_count/token_count*100\n",
    "\n",
    "\n",
    "\n",
    "# \"##\"\" 세기\n",
    "# 문장당 oov rate (SR)\n",
    "def getShopRatePerSentence(sentence):\n",
    "\n",
    "    OOV_rate = sentence.count('##') / len(sentence) * 100   \n",
    "    \n",
    "    return OOV_rate\n",
    "\n",
    "# count of all ## tokens (SC)\n",
    "def getCountofAllShop(sentence):\n",
    "    cnt = 0\n",
    "    cnt += len(re.findall('##', sentence))\n",
    "    \n",
    "    return cnt\n",
    "\n",
    "# [CLS, SEP] 제거\n",
    "def removeCS(sentence):\n",
    "    sentence = sentence.replace('[CLS]', '')\n",
    "    sentence = sentence.replace('[SEP]', '')\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# ##수 /전체 토큰 수\n",
    "def getOOVdividedbyAllTokens(corpus):\n",
    "    corpus['SC'] = corpus['sentence'].apply(lambda x: getCountofAllShop(x))\n",
    "    corpus['sentence'].apply(lambda x: removeCS(x))\n",
    "    corpus['token_count'] = corpus['sentence'].apply(lambda x: len(x))\n",
    "    Shop_count = corpus['SC'].sum()\n",
    "    token_count = corpus['token_count'].sum()\n",
    "    \n",
    "    return Shop_count, token_count, Shop_count/token_count*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola_train = pd.read_csv('dataset_analysis/cola_train.tsv', sep='\\t')\n",
    "cola_dev = pd.read_csv('dataset_analysis/cola_dev.tsv', sep='\\t')\n",
    "cola_test = pd.read_csv('dataset_analysis/cola_test.tsv', sep='\\t')\n",
    "cola = pd.concat([cola_train, cola_dev, cola_test])\n",
    "cola['tokenize_result'] = cola['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "cola['source_len'] = cola['source'].apply(lambda x: len(x))\n",
    "cola['tokenized_len'] = cola['tokenize_result'].apply(lambda x: len(x))\n",
    "\n",
    "cola['OOV_per_sent'] = cola['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "cola['OOV_count'] = cola['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "\n",
    "cola['##_per_sent'] = cola['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "cola['##_count'] = cola['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>tokenize_result</th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>##_per_sent</th>\n",
       "      <th>##_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>높은 달이 떴다.</td>\n",
       "      <td>eojeol_composed_F_64k</td>\n",
       "      <td>높은 달이 떴다.</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>높은 달이 떴다.</td>\n",
       "      <td>eojeol_pure_F_64k</td>\n",
       "      <td>높은 달이 떴다.</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>높은 달이 떴다.</td>\n",
       "      <td>fixed_composed_F_64k</td>\n",
       "      <td>높 은 달 이 뜨 었 다 .</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>높은 달이 떴다.</td>\n",
       "      <td>fixed_lexical_F_64k</td>\n",
       "      <td>높 은 달 이 뜨 었 다 .</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>높은 달이 떴다.</td>\n",
       "      <td>fixed_pure_F_64k</td>\n",
       "      <td>높 은 달 이 뜨 었 다 .</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10595</th>\n",
       "      <td>나는 할아버지가 제일 무서우시다.</td>\n",
       "      <td>eojeol_composed_F_32k</td>\n",
       "      <td>나는 할아버지 ##가 제일 무서 ##우 ##시 ##다.</td>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10596</th>\n",
       "      <td>나는 할아버지가 제일 무서우시다.</td>\n",
       "      <td>eojeol_pure_F_32k</td>\n",
       "      <td>나는 할아버지가 제일 무서 ##우 ##시다.</td>\n",
       "      <td>18</td>\n",
       "      <td>43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.651163</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10597</th>\n",
       "      <td>나는 할아버지가 제일 무서우시다.</td>\n",
       "      <td>fixed_composed_F_32k</td>\n",
       "      <td>나 는 할아버지 가 제일 무서 ##우 시 다 .</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10598</th>\n",
       "      <td>나는 할아버지가 제일 무서우시다.</td>\n",
       "      <td>fixed_lexical_F_32k</td>\n",
       "      <td>나 는 할아버지 가 제일 무서 ##우 시 다 .</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10599</th>\n",
       "      <td>나는 할아버지가 제일 무서우시다.</td>\n",
       "      <td>fixed_pure_F_32</td>\n",
       "      <td>나 는 할아버지 가 제일 무서 ##우 시 다 .</td>\n",
       "      <td>18</td>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179960 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   source              tokenizer  \\\n",
       "0               높은 달이 떴다.  eojeol_composed_F_64k   \n",
       "1               높은 달이 떴다.      eojeol_pure_F_64k   \n",
       "2               높은 달이 떴다.   fixed_composed_F_64k   \n",
       "3               높은 달이 떴다.    fixed_lexical_F_64k   \n",
       "4               높은 달이 떴다.       fixed_pure_F_64k   \n",
       "...                   ...                    ...   \n",
       "10595  나는 할아버지가 제일 무서우시다.  eojeol_composed_F_32k   \n",
       "10596  나는 할아버지가 제일 무서우시다.      eojeol_pure_F_32k   \n",
       "10597  나는 할아버지가 제일 무서우시다.   fixed_composed_F_32k   \n",
       "10598  나는 할아버지가 제일 무서우시다.    fixed_lexical_F_32k   \n",
       "10599  나는 할아버지가 제일 무서우시다.        fixed_pure_F_32   \n",
       "\n",
       "                                     tokenize_result  source_len  \\\n",
       "0                                         높은 달이 떴다.            9   \n",
       "1                               높은 달이 떴다.            9   \n",
       "2                                   높 은 달 이 뜨 었 다 .            9   \n",
       "3                              높 은 달 이 뜨 었 다 .            9   \n",
       "4                        높 은 달 이 뜨 었 다 .            9   \n",
       "...                                              ...         ...   \n",
       "10595                나는 할아버지 ##가 제일 무서 ##우 ##시 ##다.           18   \n",
       "10596     나는 할아버지가 제일 무서 ##우 ##시다.           18   \n",
       "10597                    나 는 할아버지 가 제일 무서 ##우 시 다 .           18   \n",
       "10598        나 는 할아버지 가 제일 무서 ##우 시 다 .           18   \n",
       "10599   나 는 할아버지 가 제일 무서 ##우 시 다 .           18   \n",
       "\n",
       "       tokenized_len  OOV_per_sent  OOV_count  ##_per_sent  ##_count  \n",
       "0                 11           0.0          0     0.000000         0  \n",
       "1                 21           0.0          0     0.000000         0  \n",
       "2                 17           0.0          0     0.000000         0  \n",
       "3                 22           0.0          0     0.000000         0  \n",
       "4                 28           0.0          0     0.000000         0  \n",
       "...              ...           ...        ...          ...       ...  \n",
       "10595             32           0.0          0    12.500000         4  \n",
       "10596             43           0.0          0     4.651163         2  \n",
       "10597             28           0.0          0     3.571429         1  \n",
       "10598             40           0.0          0     2.500000         1  \n",
       "10599             45           0.0          0     2.222222         1  \n",
       "\n",
       "[179960 rows x 9 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>##_per_sent</th>\n",
       "      <th>##_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>19.115748</td>\n",
       "      <td>31.454434</td>\n",
       "      <td>0.032876</td>\n",
       "      <td>189</td>\n",
       "      <td>10.523272</td>\n",
       "      <td>62072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>19.115748</td>\n",
       "      <td>28.359246</td>\n",
       "      <td>0.036712</td>\n",
       "      <td>189</td>\n",
       "      <td>8.088648</td>\n",
       "      <td>43505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>19.115748</td>\n",
       "      <td>49.622583</td>\n",
       "      <td>0.035132</td>\n",
       "      <td>189</td>\n",
       "      <td>5.560999</td>\n",
       "      <td>50988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>19.115748</td>\n",
       "      <td>47.775672</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>189</td>\n",
       "      <td>4.520664</td>\n",
       "      <td>39909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>19.115748</td>\n",
       "      <td>30.536341</td>\n",
       "      <td>0.019880</td>\n",
       "      <td>216</td>\n",
       "      <td>1.689564</td>\n",
       "      <td>10372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>19.115748</td>\n",
       "      <td>29.314070</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>216</td>\n",
       "      <td>0.508337</td>\n",
       "      <td>3040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>19.115748</td>\n",
       "      <td>42.783563</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>216</td>\n",
       "      <td>0.871460</td>\n",
       "      <td>7377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>19.115748</td>\n",
       "      <td>41.893365</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>216</td>\n",
       "      <td>0.239431</td>\n",
       "      <td>2037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>19.115748</td>\n",
       "      <td>50.271394</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>216</td>\n",
       "      <td>0.740500</td>\n",
       "      <td>7258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>19.115748</td>\n",
       "      <td>49.392698</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>216</td>\n",
       "      <td>0.202259</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_sent OOV_count  \\\n",
       "                            mean          mean         mean       sum   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k  19.115748     31.454434     0.032876       189   \n",
       "eojeol_composed_F_64k  19.115748     28.359246     0.036712       189   \n",
       "eojeol_pure_F_32k      19.115748     49.622583     0.035132       189   \n",
       "eojeol_pure_F_64k      19.115748     47.775672     0.037975       189   \n",
       "fixed_composed_F_32k   19.115748     30.536341     0.019880       216   \n",
       "fixed_composed_F_64k   19.115748     29.314070     0.020558       216   \n",
       "fixed_lexical_F_32k    19.115748     42.783563     0.020423       216   \n",
       "fixed_lexical_F_64k    19.115748     41.893365     0.020611       216   \n",
       "fixed_pure_F_32        19.115748     50.271394     0.020423       216   \n",
       "fixed_pure_F_64k       19.115748     49.392698     0.020611       216   \n",
       "\n",
       "                      ##_per_sent ##_count  \n",
       "                             mean      sum  \n",
       "tokenizer                                   \n",
       "eojeol_composed_F_32k   10.523272    62072  \n",
       "eojeol_composed_F_64k    8.088648    43505  \n",
       "eojeol_pure_F_32k        5.560999    50988  \n",
       "eojeol_pure_F_64k        4.520664    39909  \n",
       "fixed_composed_F_32k     1.689564    10372  \n",
       "fixed_composed_F_64k     0.508337     3040  \n",
       "fixed_lexical_F_32k      0.871460     7377  \n",
       "fixed_lexical_F_64k      0.239431     2037  \n",
       "fixed_pure_F_32          0.740500     7258  \n",
       "fixed_pure_F_64k         0.202259     1987  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    '##_per_sent': ['mean'],\n",
    "    '##_count': ['sum']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsmc_train = pd.read_csv('dataset_analysis/nsmc_train.tsv', sep='\\t')\n",
    "nsmc_dev = pd.read_csv('dataset_analysis/nsmc_dev.tsv', sep='\\t')\n",
    "nsmc_test = pd.read_csv('dataset_analysis/nsmc_test.tsv', sep='\\t')\n",
    "nsmc = pd.concat([nsmc_train, nsmc_dev, nsmc_test])\n",
    "nsmc['tokenize_result'] = nsmc['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "nsmc['source_len'] = nsmc['source'].apply(lambda x: len(x))\n",
    "nsmc['tokenized_len'] = nsmc['tokenize_result'].apply(lambda x: len(x))\n",
    "\n",
    "nsmc['OOV_per_sent'] = nsmc['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "nsmc['OOV_count'] = nsmc['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "\n",
    "nsmc['##_per_sent'] = nsmc['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "nsmc['##_count'] = nsmc['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>##_per_sent</th>\n",
       "      <th>##_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>35.25899</td>\n",
       "      <td>67.630310</td>\n",
       "      <td>0.346157</td>\n",
       "      <td>8989</td>\n",
       "      <td>14.242295</td>\n",
       "      <td>2027121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>35.25899</td>\n",
       "      <td>61.456978</td>\n",
       "      <td>0.356818</td>\n",
       "      <td>8989</td>\n",
       "      <td>12.307936</td>\n",
       "      <td>1615582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>35.25899</td>\n",
       "      <td>100.643131</td>\n",
       "      <td>0.455792</td>\n",
       "      <td>9813</td>\n",
       "      <td>8.840555</td>\n",
       "      <td>1817988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>35.25899</td>\n",
       "      <td>96.386365</td>\n",
       "      <td>0.464452</td>\n",
       "      <td>9813</td>\n",
       "      <td>7.755301</td>\n",
       "      <td>1534215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>35.25899</td>\n",
       "      <td>56.690843</td>\n",
       "      <td>0.127833</td>\n",
       "      <td>10730</td>\n",
       "      <td>2.627349</td>\n",
       "      <td>317082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>35.25899</td>\n",
       "      <td>54.285001</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>10730</td>\n",
       "      <td>1.325125</td>\n",
       "      <td>156699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>35.25899</td>\n",
       "      <td>82.471494</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>10448</td>\n",
       "      <td>1.480511</td>\n",
       "      <td>245851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>35.25899</td>\n",
       "      <td>80.988145</td>\n",
       "      <td>0.126059</td>\n",
       "      <td>10448</td>\n",
       "      <td>0.897720</td>\n",
       "      <td>146965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>35.25899</td>\n",
       "      <td>92.037466</td>\n",
       "      <td>0.122186</td>\n",
       "      <td>10430</td>\n",
       "      <td>1.351018</td>\n",
       "      <td>240872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>35.25899</td>\n",
       "      <td>90.602929</td>\n",
       "      <td>0.125915</td>\n",
       "      <td>10430</td>\n",
       "      <td>0.826526</td>\n",
       "      <td>145240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_sent OOV_count  \\\n",
       "                            mean          mean         mean       sum   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k   35.25899     67.630310     0.346157      8989   \n",
       "eojeol_composed_F_64k   35.25899     61.456978     0.356818      8989   \n",
       "eojeol_pure_F_32k       35.25899    100.643131     0.455792      9813   \n",
       "eojeol_pure_F_64k       35.25899     96.386365     0.464452      9813   \n",
       "fixed_composed_F_32k    35.25899     56.690843     0.127833     10730   \n",
       "fixed_composed_F_64k    35.25899     54.285001     0.133200     10730   \n",
       "fixed_lexical_F_32k     35.25899     82.471494     0.122378     10448   \n",
       "fixed_lexical_F_64k     35.25899     80.988145     0.126059     10448   \n",
       "fixed_pure_F_32         35.25899     92.037466     0.122186     10430   \n",
       "fixed_pure_F_64k        35.25899     90.602929     0.125915     10430   \n",
       "\n",
       "                      ##_per_sent ##_count  \n",
       "                             mean      sum  \n",
       "tokenizer                                   \n",
       "eojeol_composed_F_32k   14.242295  2027121  \n",
       "eojeol_composed_F_64k   12.307936  1615582  \n",
       "eojeol_pure_F_32k        8.840555  1817988  \n",
       "eojeol_pure_F_64k        7.755301  1534215  \n",
       "fixed_composed_F_32k     2.627349   317082  \n",
       "fixed_composed_F_64k     1.325125   156699  \n",
       "fixed_lexical_F_32k      1.480511   245851  \n",
       "fixed_lexical_F_64k      0.897720   146965  \n",
       "fixed_pure_F_32          1.351018   240872  \n",
       "fixed_pure_F_64k         0.826526   145240  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    '##_per_sent': ['mean'],\n",
    "    '##_count': ['sum']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>##_per_sent</th>\n",
       "      <th>##_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>69.43389</td>\n",
       "      <td>123.754225</td>\n",
       "      <td>0.332959</td>\n",
       "      <td>52211</td>\n",
       "      <td>13.449360</td>\n",
       "      <td>1856386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>69.43389</td>\n",
       "      <td>110.640632</td>\n",
       "      <td>0.378464</td>\n",
       "      <td>52211</td>\n",
       "      <td>11.111521</td>\n",
       "      <td>1392650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>69.43389</td>\n",
       "      <td>161.549397</td>\n",
       "      <td>0.063106</td>\n",
       "      <td>30829</td>\n",
       "      <td>9.001736</td>\n",
       "      <td>1622116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>69.43389</td>\n",
       "      <td>153.039033</td>\n",
       "      <td>0.068572</td>\n",
       "      <td>30829</td>\n",
       "      <td>7.712552</td>\n",
       "      <td>1321164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>69.43389</td>\n",
       "      <td>110.192734</td>\n",
       "      <td>0.282816</td>\n",
       "      <td>52346</td>\n",
       "      <td>6.317381</td>\n",
       "      <td>846546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>69.43389</td>\n",
       "      <td>100.792288</td>\n",
       "      <td>0.307464</td>\n",
       "      <td>52346</td>\n",
       "      <td>4.134036</td>\n",
       "      <td>514118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>69.43389</td>\n",
       "      <td>135.968008</td>\n",
       "      <td>0.296267</td>\n",
       "      <td>52346</td>\n",
       "      <td>4.323391</td>\n",
       "      <td>684479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>69.43389</td>\n",
       "      <td>129.764123</td>\n",
       "      <td>0.311179</td>\n",
       "      <td>52346</td>\n",
       "      <td>3.099956</td>\n",
       "      <td>465091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>69.43389</td>\n",
       "      <td>150.212727</td>\n",
       "      <td>0.049922</td>\n",
       "      <td>30904</td>\n",
       "      <td>4.037302</td>\n",
       "      <td>698147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>69.43389</td>\n",
       "      <td>143.956357</td>\n",
       "      <td>0.052145</td>\n",
       "      <td>30904</td>\n",
       "      <td>2.902907</td>\n",
       "      <td>476903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_sent OOV_count  \\\n",
       "                            mean          mean         mean       sum   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k   69.43389    123.754225     0.332959     52211   \n",
       "eojeol_composed_F_64k   69.43389    110.640632     0.378464     52211   \n",
       "eojeol_pure_F_32k       69.43389    161.549397     0.063106     30829   \n",
       "eojeol_pure_F_64k       69.43389    153.039033     0.068572     30829   \n",
       "fixed_composed_F_32k    69.43389    110.192734     0.282816     52346   \n",
       "fixed_composed_F_64k    69.43389    100.792288     0.307464     52346   \n",
       "fixed_lexical_F_32k     69.43389    135.968008     0.296267     52346   \n",
       "fixed_lexical_F_64k     69.43389    129.764123     0.311179     52346   \n",
       "fixed_pure_F_32         69.43389    150.212727     0.049922     30904   \n",
       "fixed_pure_F_64k        69.43389    143.956357     0.052145     30904   \n",
       "\n",
       "                      ##_per_sent ##_count  \n",
       "                             mean      sum  \n",
       "tokenizer                                   \n",
       "eojeol_composed_F_32k   13.449360  1856386  \n",
       "eojeol_composed_F_64k   11.111521  1392650  \n",
       "eojeol_pure_F_32k        9.001736  1622116  \n",
       "eojeol_pure_F_64k        7.712552  1321164  \n",
       "fixed_composed_F_32k     6.317381   846546  \n",
       "fixed_composed_F_64k     4.134036   514118  \n",
       "fixed_lexical_F_32k      4.323391   684479  \n",
       "fixed_lexical_F_64k      3.099956   465091  \n",
       "fixed_pure_F_32          4.037302   698147  \n",
       "fixed_pure_F_64k         2.902907   476903  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws_train = pd.read_csv('dataset_analysis/paws_train.tsv', sep='\\t')\n",
    "paws_dev = pd.read_csv('dataset_analysis/paws_dev.tsv', sep='\\t')\n",
    "paws_test = pd.read_csv('dataset_analysis/paws_test.tsv', sep='\\t')\n",
    "paws = pd.concat([paws_train, paws_dev, paws_test])\n",
    "\n",
    "paws['tokenize_result'] = paws['tokenize_result'].apply(str)\n",
    "paws['tokenize_result'] = paws['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "paws['source_len'] = paws['source'].apply(lambda x: len(x))\n",
    "paws['tokenized_len'] = paws['tokenize_result'].apply(lambda x: len(x))\n",
    "paws['OOV_per_sent'] = paws['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "paws['OOV_count'] = paws['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "\n",
    "paws['##_per_sent'] = paws['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "paws['##_count'] = paws['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "\n",
    "\n",
    "paws.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    '##_per_sent': ['mean'],\n",
    "    '##_count': ['sum']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>##_per_sent</th>\n",
       "      <th>##_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>38.718298</td>\n",
       "      <td>77.993546</td>\n",
       "      <td>0.270432</td>\n",
       "      <td>803</td>\n",
       "      <td>15.587404</td>\n",
       "      <td>104113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>38.718298</td>\n",
       "      <td>70.834349</td>\n",
       "      <td>0.292663</td>\n",
       "      <td>803</td>\n",
       "      <td>13.781274</td>\n",
       "      <td>84146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>38.718298</td>\n",
       "      <td>114.948369</td>\n",
       "      <td>0.276332</td>\n",
       "      <td>794</td>\n",
       "      <td>9.805903</td>\n",
       "      <td>95085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>38.718298</td>\n",
       "      <td>109.920760</td>\n",
       "      <td>0.296624</td>\n",
       "      <td>794</td>\n",
       "      <td>8.724854</td>\n",
       "      <td>81063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>38.718298</td>\n",
       "      <td>63.712920</td>\n",
       "      <td>0.210914</td>\n",
       "      <td>1031</td>\n",
       "      <td>3.394665</td>\n",
       "      <td>18142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>38.718298</td>\n",
       "      <td>60.107685</td>\n",
       "      <td>0.222857</td>\n",
       "      <td>1031</td>\n",
       "      <td>1.607559</td>\n",
       "      <td>8087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>38.718298</td>\n",
       "      <td>91.906179</td>\n",
       "      <td>0.215083</td>\n",
       "      <td>1028</td>\n",
       "      <td>1.827213</td>\n",
       "      <td>13682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>38.718298</td>\n",
       "      <td>89.587068</td>\n",
       "      <td>0.223857</td>\n",
       "      <td>1028</td>\n",
       "      <td>0.997295</td>\n",
       "      <td>7214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>38.718298</td>\n",
       "      <td>102.566989</td>\n",
       "      <td>0.214752</td>\n",
       "      <td>1025</td>\n",
       "      <td>1.645531</td>\n",
       "      <td>13433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>38.718298</td>\n",
       "      <td>100.297717</td>\n",
       "      <td>0.223262</td>\n",
       "      <td>1025</td>\n",
       "      <td>0.898466</td>\n",
       "      <td>7104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_sent OOV_count  \\\n",
       "                            mean          mean         mean       sum   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k  38.718298     77.993546     0.270432       803   \n",
       "eojeol_composed_F_64k  38.718298     70.834349     0.292663       803   \n",
       "eojeol_pure_F_32k      38.718298    114.948369     0.276332       794   \n",
       "eojeol_pure_F_64k      38.718298    109.920760     0.296624       794   \n",
       "fixed_composed_F_32k   38.718298     63.712920     0.210914      1031   \n",
       "fixed_composed_F_64k   38.718298     60.107685     0.222857      1031   \n",
       "fixed_lexical_F_32k    38.718298     91.906179     0.215083      1028   \n",
       "fixed_lexical_F_64k    38.718298     89.587068     0.223857      1028   \n",
       "fixed_pure_F_32        38.718298    102.566989     0.214752      1025   \n",
       "fixed_pure_F_64k       38.718298    100.297717     0.223262      1025   \n",
       "\n",
       "                      ##_per_sent ##_count  \n",
       "                             mean      sum  \n",
       "tokenizer                                   \n",
       "eojeol_composed_F_32k   15.587404   104113  \n",
       "eojeol_composed_F_64k   13.781274    84146  \n",
       "eojeol_pure_F_32k        9.805903    95085  \n",
       "eojeol_pure_F_64k        8.724854    81063  \n",
       "fixed_composed_F_32k     3.394665    18142  \n",
       "fixed_composed_F_64k     1.607559     8087  \n",
       "fixed_lexical_F_32k      1.827213    13682  \n",
       "fixed_lexical_F_64k      0.997295     7214  \n",
       "fixed_pure_F_32          1.645531    13433  \n",
       "fixed_pure_F_64k         0.898466     7104  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsd_train = pd.read_csv('dataset_analysis/hsd_train.tsv', sep='\\t')\n",
    "hsd_dev = pd.read_csv('dataset_analysis/hsd_dev.tsv', sep='\\t')\n",
    "hsd = pd.concat([hsd_train, hsd_dev])\n",
    "\n",
    "hsd['tokenize_result'] = hsd['tokenize_result'].apply(str)\n",
    "hsd['tokenize_result'] = hsd['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "hsd['source_len'] = hsd['source'].apply(lambda x: len(x))\n",
    "hsd['tokenized_len'] = hsd['tokenize_result'].apply(lambda x: len(x))\n",
    "hsd['OOV_per_sent'] = hsd['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "hsd['OOV_count'] = hsd['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "\n",
    "hsd['##_per_sent'] = hsd['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "hsd['##_count'] = hsd['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "\n",
    "\n",
    "hsd.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    '##_per_sent': ['mean'],\n",
    "    '##_count': ['sum']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>##_per_sent</th>\n",
       "      <th>##_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>48.684417</td>\n",
       "      <td>80.909833</td>\n",
       "      <td>0.240620</td>\n",
       "      <td>2881</td>\n",
       "      <td>12.104248</td>\n",
       "      <td>121449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>48.684417</td>\n",
       "      <td>71.796333</td>\n",
       "      <td>0.276842</td>\n",
       "      <td>2881</td>\n",
       "      <td>9.435488</td>\n",
       "      <td>84995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>48.684417</td>\n",
       "      <td>126.518250</td>\n",
       "      <td>0.259793</td>\n",
       "      <td>2881</td>\n",
       "      <td>6.472703</td>\n",
       "      <td>101114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>48.684417</td>\n",
       "      <td>120.830500</td>\n",
       "      <td>0.285277</td>\n",
       "      <td>2881</td>\n",
       "      <td>5.216587</td>\n",
       "      <td>78363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>48.684417</td>\n",
       "      <td>72.965417</td>\n",
       "      <td>0.185928</td>\n",
       "      <td>3346</td>\n",
       "      <td>2.118451</td>\n",
       "      <td>21074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>48.684417</td>\n",
       "      <td>69.509167</td>\n",
       "      <td>0.194508</td>\n",
       "      <td>3346</td>\n",
       "      <td>0.742135</td>\n",
       "      <td>7249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>48.684417</td>\n",
       "      <td>109.196667</td>\n",
       "      <td>0.190371</td>\n",
       "      <td>3346</td>\n",
       "      <td>0.945381</td>\n",
       "      <td>13891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>48.684417</td>\n",
       "      <td>107.169167</td>\n",
       "      <td>0.195732</td>\n",
       "      <td>3346</td>\n",
       "      <td>0.395972</td>\n",
       "      <td>5781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>48.684417</td>\n",
       "      <td>121.919500</td>\n",
       "      <td>0.190566</td>\n",
       "      <td>3346</td>\n",
       "      <td>0.829199</td>\n",
       "      <td>13536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>48.684417</td>\n",
       "      <td>119.964750</td>\n",
       "      <td>0.195764</td>\n",
       "      <td>3346</td>\n",
       "      <td>0.352165</td>\n",
       "      <td>5717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_sent OOV_count  \\\n",
       "                            mean          mean         mean       sum   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k  48.684417     80.909833     0.240620      2881   \n",
       "eojeol_composed_F_64k  48.684417     71.796333     0.276842      2881   \n",
       "eojeol_pure_F_32k      48.684417    126.518250     0.259793      2881   \n",
       "eojeol_pure_F_64k      48.684417    120.830500     0.285277      2881   \n",
       "fixed_composed_F_32k   48.684417     72.965417     0.185928      3346   \n",
       "fixed_composed_F_64k   48.684417     69.509167     0.194508      3346   \n",
       "fixed_lexical_F_32k    48.684417    109.196667     0.190371      3346   \n",
       "fixed_lexical_F_64k    48.684417    107.169167     0.195732      3346   \n",
       "fixed_pure_F_32        48.684417    121.919500     0.190566      3346   \n",
       "fixed_pure_F_64k       48.684417    119.964750     0.195764      3346   \n",
       "\n",
       "                      ##_per_sent ##_count  \n",
       "                             mean      sum  \n",
       "tokenizer                                   \n",
       "eojeol_composed_F_32k   12.104248   121449  \n",
       "eojeol_composed_F_64k    9.435488    84995  \n",
       "eojeol_pure_F_32k        6.472703   101114  \n",
       "eojeol_pure_F_64k        5.216587    78363  \n",
       "fixed_composed_F_32k     2.118451    21074  \n",
       "fixed_composed_F_64k     0.742135     7249  \n",
       "fixed_lexical_F_32k      0.945381    13891  \n",
       "fixed_lexical_F_64k      0.395972     5781  \n",
       "fixed_pure_F_32          0.829199    13536  \n",
       "fixed_pure_F_64k         0.352165     5717  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_train = pd.read_csv('dataset_analysis/dp_train.tsv', sep='\\t')\n",
    "dp_dev = pd.read_csv('dataset_analysis/dp_dev.tsv', sep='\\t')\n",
    "dp = pd.concat([dp_train, dp_dev])\n",
    "\n",
    "dp['tokenize_result'] = dp['tokenize_result'].apply(str)\n",
    "dp['tokenize_result'] = dp['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "dp['source_len'] = dp['source'].apply(lambda x: len(x))\n",
    "dp['tokenized_len'] = dp['tokenize_result'].apply(lambda x: len(x))\n",
    "dp['OOV_per_sent'] = dp['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "dp['OOV_count'] = dp['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "\n",
    "dp['##_per_sent'] = dp['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "dp['##_count'] = dp['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "\n",
    "\n",
    "dp.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    '##_per_sent': ['mean'],\n",
    "    '##_count': ['sum']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>OOV_per_sent</th>\n",
       "      <th>OOV_count</th>\n",
       "      <th>##_per_sent</th>\n",
       "      <th>##_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_32k</th>\n",
       "      <td>35.146314</td>\n",
       "      <td>58.309951</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>151</td>\n",
       "      <td>11.634948</td>\n",
       "      <td>395046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_composed_F_64k</th>\n",
       "      <td>35.146314</td>\n",
       "      <td>51.842721</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>151</td>\n",
       "      <td>8.980216</td>\n",
       "      <td>274333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_32k</th>\n",
       "      <td>35.146314</td>\n",
       "      <td>93.310290</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>151</td>\n",
       "      <td>6.063119</td>\n",
       "      <td>327231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eojeol_pure_F_64k</th>\n",
       "      <td>35.146314</td>\n",
       "      <td>89.269537</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>151</td>\n",
       "      <td>4.866531</td>\n",
       "      <td>251809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_32k</th>\n",
       "      <td>35.146314</td>\n",
       "      <td>53.292289</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>196</td>\n",
       "      <td>2.181656</td>\n",
       "      <td>73156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_composed_F_64k</th>\n",
       "      <td>35.146314</td>\n",
       "      <td>50.789199</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>196</td>\n",
       "      <td>0.824155</td>\n",
       "      <td>26435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_32k</th>\n",
       "      <td>35.146314</td>\n",
       "      <td>80.524662</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>196</td>\n",
       "      <td>1.003572</td>\n",
       "      <td>49285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_lexical_F_64k</th>\n",
       "      <td>35.146314</td>\n",
       "      <td>79.003929</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>196</td>\n",
       "      <td>0.431059</td>\n",
       "      <td>20900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_32</th>\n",
       "      <td>35.146314</td>\n",
       "      <td>90.281431</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>196</td>\n",
       "      <td>0.877533</td>\n",
       "      <td>48027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_pure_F_64k</th>\n",
       "      <td>35.146314</td>\n",
       "      <td>88.820273</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>196</td>\n",
       "      <td>0.383633</td>\n",
       "      <td>20754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_len tokenized_len OOV_per_sent OOV_count  \\\n",
       "                            mean          mean         mean       sum   \n",
       "tokenizer                                                               \n",
       "eojeol_composed_F_32k  35.146314     58.309951     0.003902       151   \n",
       "eojeol_composed_F_64k  35.146314     51.842721     0.004501       151   \n",
       "eojeol_pure_F_32k      35.146314     93.310290     0.004100       151   \n",
       "eojeol_pure_F_64k      35.146314     89.269537     0.004600       151   \n",
       "fixed_composed_F_32k   35.146314     53.292289     0.003230       196   \n",
       "fixed_composed_F_64k   35.146314     50.789199     0.003463       196   \n",
       "fixed_lexical_F_32k    35.146314     80.524662     0.003309       196   \n",
       "fixed_lexical_F_64k    35.146314     79.003929     0.003484       196   \n",
       "fixed_pure_F_32        35.146314     90.281431     0.003309       196   \n",
       "fixed_pure_F_64k       35.146314     88.820273     0.003484       196   \n",
       "\n",
       "                      ##_per_sent ##_count  \n",
       "                             mean      sum  \n",
       "tokenizer                                   \n",
       "eojeol_composed_F_32k   11.634948   395046  \n",
       "eojeol_composed_F_64k    8.980216   274333  \n",
       "eojeol_pure_F_32k        6.063119   327231  \n",
       "eojeol_pure_F_64k        4.866531   251809  \n",
       "fixed_composed_F_32k     2.181656    73156  \n",
       "fixed_composed_F_64k     0.824155    26435  \n",
       "fixed_lexical_F_32k      1.003572    49285  \n",
       "fixed_lexical_F_64k      0.431059    20900  \n",
       "fixed_pure_F_32          0.877533    48027  \n",
       "fixed_pure_F_64k         0.383633    20754  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_train = pd.read_csv('dataset_analysis/nli_train.tsv', sep='\\t')\n",
    "nli_dev = pd.read_csv('dataset_analysis/nli_dev.tsv', sep='\\t')\n",
    "nli = pd.concat([nli_train, nli_dev])\n",
    "\n",
    "nli['tokenize_result'] = nli['tokenize_result'].apply(str)\n",
    "nli['tokenize_result'] = nli['tokenize_result'].apply(lambda x: removeCS(x))\n",
    "nli['source_len'] = nli['source'].apply(lambda x: len(x))\n",
    "nli['tokenized_len'] = nli['tokenize_result'].apply(lambda x: len(x))\n",
    "nli['OOV_per_sent'] = nli['tokenize_result'].apply(lambda x: getOOVRatePerSentence(x))\n",
    "nli['OOV_count'] = nli['tokenize_result'].apply(lambda x: getCountofAllOOV(x))\n",
    "\n",
    "nli['##_per_sent'] = nli['tokenize_result'].apply(lambda x: getShopRatePerSentence(x))\n",
    "nli['##_count'] = nli['tokenize_result'].apply(lambda x: getCountofAllShop(x))\n",
    "\n",
    "\n",
    "nli.groupby('tokenizer').agg({\n",
    "    'source_len': ['mean'],\n",
    "    'tokenized_len': ['mean'],\n",
    "    'OOV_per_sent': ['mean'],\n",
    "    'OOV_count': ['sum'],\n",
    "    '##_per_sent': ['mean'],\n",
    "    '##_count': ['sum']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17996.0 199992.0 106132.0 8367.0 12000.0 55996.0\n"
     ]
    }
   ],
   "source": [
    "# size\n",
    "print(len(cola)/10,\n",
    "len(nsmc)/10,\n",
    "len(paws)/10,\n",
    "len(hsd)/10,\n",
    "len(dp)/10,\n",
    "len(nli)/10\n",
    ")\n",
    "\n",
    "# save results\n",
    "cola.to_csv('dataset_analysis/cola_result.tsv', sep='\\t', index=False)\n",
    "nsmc.to_csv('dataset_analysis/nsmc_result.tsv', sep='\\t', index=False)\n",
    "paws.to_csv('dataset_analysis/paws_result.tsv', sep='\\t', index=False)\n",
    "hsd.to_csv('dataset_analysis/hsd_result.tsv', sep='\\t', index=False)\n",
    "dp.to_csv('dataset_analysis/dp_result.tsv', sep='\\t', index=False)\n",
    "nli.to_csv('dataset_analysis/nli_result.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "453e7ba0431a1f965429b96a03a1d5546248e001661d541f6bea069f7abcbd9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
